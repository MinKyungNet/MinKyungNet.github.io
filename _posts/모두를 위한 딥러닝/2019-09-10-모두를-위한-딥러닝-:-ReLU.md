---
layout: post
title: "모두를 위한 딥러닝 : ReLU"
tag: [ReLU, Optimizer]
categoires: [모두를 위한 딥러닝]
---

# 학습 목표

ReLU 활성화 함수에 대해 알아본다

3 핵심 키워드

ReLU

Sigmoid

Optimizer
# Activiation function
![image](https://t1.daumcdn.net/cfile/tistory/275BAD4F577B669920)     
시그모이드 같은 함수는 인풋 값에 따라 아웃풋 0과 1을 반환해준다는 의미에서 엑티베이션 펑션이라고 부른다.

# Vanishing Gradient
![image](https://postfiles.pstatic.net/MjAxOTA1MTlfMjkx/MDAxNTU4MjQ4MTA1Njg2.sknfAUs2yV4PypSwjWJ5fSkPHWRM59btivhS7Td8CMwg.Aa5wB39WhuFzorOwcDJkmjLdL_Y8xXTT5OjMK-vKKnMg.PNG.kbsdr11/image.png?type=w966)        
우리는 코스트 값을 줄이기 위해(모델의 정확도를 높이기 위해) Gradient Descent방법을 사용해서 Weight에 미분한  값들을 뺐다. 그렇게 반복하다보면 코스트 값이 가장 작아지는 곳으로 Weight가 향하게 된다.

각 노드들은 합성함수의 형태를 띄었고 각 합성함수들을 미분하기 위해 뒤의 항은 이미 계산된 것으로 처리하는 Chain Rule인 BackPropagation을 사용했는데, 문제는 여기서 발생한다.

시그모이드 함수는 일정범위를 지나게 되면 기울기가 0에 아주 가까워지는데

0에 가까운 값들을 계속 곱해나가다보면 코스트의 영향력이 점점 작아져서 레이어가 많이 쌓일 수록 학습이 힘들어진다는 것이다.

당시 사람들은 이 문제를 해결하지 못해서 인공신경망은 2차 암흑기에 빠졌다고 한다.

# ReLU
![image](https://postfiles.pstatic.net/MjAxOTA1MTlfMTg0/MDAxNTU4MjQ4MjUwNzY2.pPv2DN62AbbIcSpd_rL7NsqREll6ZlGsa-eYzX5sa0Yg.YdCA9hGEaZnmOWo5YjVAbRuyzN3_ghA4Thuq__7CZ2Ug.PNG.kbsdr11/image.png?type=w966)     
ReLU함수는 인풋이 음수이면 0을 반환하고

인풋이 양수이면 인풋을 그대로 반환하는 함수이다.

x값을 그대로 받아오기 때문에 미분한 값은 x의 미분값과 같게 되므로

cost의 영향력이 희미해질 가능성이 낮아진다.

즉, Gradient Vanishing 문제를 해결할 수 있다.

# 일도 잘하는 ReLU함수
![image](https://postfiles.pstatic.net/MjAxOTA1MjBfMjUx/MDAxNTU4Mjg1NDEyMDI3.go1m50kKrgVeKrbnReD22TSWh5svqYSdMB2MFGErr54g.Ezk1grEaygPsOkhqUPLWS2ukm3tusWr8yLyYywauhxIg.PNG.kbsdr11/image.png?type=w966)    
 
 # 성능
 ![image](https://postfiles.pstatic.net/MjAxOTA1MjBfMTky/MDAxNTU4Mjg1NTUyMzk2.fe5WQkXlQw5d54JQvXx6H1QjxuWXaiVueGH99b76190g.W8_jRieaX-ZmeFz5KpPPdp6nCMgkcATZli50F9JoYcEg.PNG.kbsdr11/image.png?type=w966)     
 최고의 성능은 아니지만 괜찮은 성능을 보여준다.    
 시그모이드는 이제 잘 사용되지 않는다고 한다.    
 ReLU와 친해지자!
