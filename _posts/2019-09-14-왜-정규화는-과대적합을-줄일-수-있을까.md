---
layout: post
title: "왜 정규화는 과대적합을 줄일 수 있을까요?"
tags: [Regularization, Overfitting]
categories: [Improving deep neural networks]
---

# 학습 목표
정규화가 과대적합 문제를 해결할 수 있음을 안다.

# 핵심 키워드
* 정규화 (Regularization)
* 과대적합 (Overfitting)

# 학습 내용
![image](https://user-images.githubusercontent.com/50114210/64882978-3c35a600-d699-11e9-974f-bf7d1dc2a5fe.png)      

# 인트로
왜 정규화가 과대적합 문제를 해결하고 분산을 줄이는데 도움이 될까요?    
어떻게 작동하는지에 관한 직관을 얻기 위해 몇가지 예제를 함께 살펴봅시다.     

# 예제 1
![image](https://user-images.githubusercontent.com/50114210/64883255-ea415000-d699-11e9-9cb5-6b7e122750ab.png)      
이전 비디오의 높은 편향, 높은 분산, 그리고 딱 맞는 경우의 예시들이 기억나시나요.     
크고 깊은 신경망에 맞추는 경우를 살펴봅시다. 과대적합의 문제가 있는 신경망을 생각해봅시다.   

### 정규화를 포함한 손심함수
![image](https://user-images.githubusercontent.com/50114210/64883342-1a88ee80-d69a-11e9-9797-600e0cffca89.png)       
w와 b에 대한 비용함수 J는 1부터 m까지 손실의 합과 같습니다.   
정규화를 위해 이 추가적인 항을 더해주었습니다. 가중치 행렬이 너무 커지지 않도록 막기 위해서입니다.    
그리고 이것을 프로베니우스 노름이라고 부릅니다.
그렇다면 매개변수의 L2 혹은 프로베니우스 노름을 줄이는 것이 왜 과대적합을 줄일 수 있을까요?     
여기서 알 수 있는 것은 정규화에서 람다를 크게 만들어서 가중치 행렬 W를 0에 상당히 가깝게 설정할 수 있다는 것입니다.      

### 가중치들이 0에 가까워진다면
![image](https://user-images.githubusercontent.com/50114210/64883536-971bcd00-d69a-11e9-8568-18122f3abd60.png)      
가중치 행렬의 값들이 0에 가까워지면 은닉 유닛의 영향력은 줄어들게 됩니다.    
그런 경우에 훨씬 더 간단하고 작은 신경망이 될것입니다.     
다양한 층을 저장하고 있긴하지만 로지스틱 회귀 유닛에 가까워집니다.   
따라서 이런 과적합한 경우를 왼쪽의 높은 편향의 경우와 가깝게 만들어 줄 수 있습니다.    
그러나 중간의 딱 맞는 경우와 가깝게 하는 적절한 람다값을 찾는게 좋습니다.     

### 영향력이 적어지는 노드들
![image](https://user-images.githubusercontent.com/50114210/64883779-1e694080-d69b-11e9-8d53-79bc703818bb.png)     
여기서 말씀드리고 싶은 것은 람다의 값을 아주 크게하면 W는 0에 가깝게 설정된다는 것입니다.     
실제의 경우에서는 은닉 유닛의 영향력을 0에 가깝게 줄이면서 로지스틱 회귀에 가까운 네트워크를 만든다고 생각하면 됩니다.     
한 뭉치의 은닉 유닛을 완전히 0으로 만든다는 것은 아닙니다.    
실제로 모든 은닉 유닛을 사용하지만 각각의 영향력이 훨씬 작아지는 것입니다.     
그러나 더 간단한 네트워크가 되는 것은 맞습니다.    
간단한 네트워크는 과대적합 문제가 덜 일어나게 됩니다.   

# 예제 2
![image](https://user-images.githubusercontent.com/50114210/64884002-9899c500-d69b-11e9-9fae-0c82aa58e981.png)   
Tanh 활성화 함수를 사용한다고 가정해보겠습니다. g(z) = tanh(z)인 경우입니다.      
이런 경우라면 z가 아주 작은 경우에는, 즉 z가 작은 범위의 매개변수를 갖는 경우라면 tanh함수의 선형 영역을 사용하게 됩니다. (중간)    
z의 값이 더 작아지거나 커지면 활성화 함수는 선형을 벗어나게 됩니다.    
여기서 얻어낼 수 있는 직관은 정규화 매개변수인 람다가 커질때 비용함수가 커지지 않으려면 상대적으로 w가 작아질 것입니다.     
가중치 W[L]이 작으면 Z[L] = W[L] * A[L-1] + b[L]과 같기 대문에    
W가 작으면 Z도 상대적으로 작은 값을 가지게 됩니다.     
이 작은 범위에서 Z가 상대적으로 작은 값을 갖게되면 g(Z)는 거의 1차원 함수가 됩니다.    
따라서 모든 층은 선형 회귀처럼 거의 직선의 함수를 갖게 됩니다.    
강의 1에서 봤듯이 모든 층이 선형이면 전체 네트워크도 선형입니다.    
따라서 선형 활성화 함수를 가진 싶은 네트워크의 경우에도 선형 함수만을 계산할 수 있게 됩니다.      
![image](https://user-images.githubusercontent.com/50114210/64884438-b287d780-d69c-11e9-910b-4f9ef9832bac.png)      
따라서 매우 복잡한 결정, 비선형 결정의 경계에 맞추기는 불가능합니다.    
이전 슬라이드의 높은 분산의 경우처럼 과대적합된 데이터 세트까지 맞추기는 어렵습니다.      

### 람다가 클 경우
![image](https://user-images.githubusercontent.com/50114210/64884360-80767580-d69c-11e9-9225-8d3bd3fe3d5a.png)     
정리하자면 정규화 매개변수가 매우 크면 매개변수 W는 매우 작습니다.       
b의 효과를 무시하면 Z의 값은 상대적으로 작습니다.      
Z가 상대적으로 작고 작은 범위의 값을 가지기 때문에 tanh의 경우 활성화 함수는 상대적으로 선형이 됩니다.     
전체 신경망은 선형 함수로부터 그리 멀지 않은 곳에서 계산될 것입니다.    
즉 매우 복잡한 비선형 함수보다 더 간단한 함수입니다.    
따라서 과대적합의 가능성이 줄어듭니다.    

# 정규화 구현 팁
![image](https://user-images.githubusercontent.com/50114210/64884728-5a9da080-d69d-11e9-8d98-521e1873dd4a.png)    
정규화를 구현할 때 비용함수 j의 정의를 살펴보았습니다.    
여기에 가중치가 너무 커지는 것을 막기 위한 추가적인 항을 추가했습니다.    
경사 하강법을 구현할 때 경사하강법을 정의하는 한 가지 단계는     
경사 하강법의 반복의 수에 대한 함수로 비용함수를 설정하는 것입니다.    
비용함수 J가 경사 하강법의 반복마다 단조감소하기를 원할 것입니다.    
정규화를 구현할 때 비용함수 J의 예전 정의, 즉 첫 항만을 그린다면 단조감소를 보지 못할 것입니다.    
따라서 경사 하강법을 디버깅할 때는 두 번째 항을 포함한 새로운 비용함수 J를 잘 확인하세요    
그렇지 않다면 매 반복에서 J가 단조감소하는 것을 보지 못할 것입니다.    

# 아웃트로
여기까지 L2 정규화였습니다. 딥러닝 모델의 훈련 중 제가 가장 많이 사용하는 정규화 기법입니다.    
딥러닝에서 가끔 드롭아웃이라는 정규화 기법을 사용하기도 합니다.     
또 보죠.





