---
layout: post
title: ""
tags:
categories: [Big data analsis]
---

# 윈도우에서 스파크를 사용하기 위해 경로 설정
```
import os
import sys

#home=os.path.expanduser("~") # HOME이 설정되어 있지 않으면 expanduser('~')를 사용한다.
#os.environ["PYSPARK_PYTHON"] = "/usr/bin/python"
os.environ["SPARK_HOME"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')
os.environ["PYLIB"]=os.path.join(os.environ["SPARK_HOME"],'python','lib')
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'py4j-0.10.1-src.zip'))
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'pyspark.zip'))
```
리눅스같은 os에서는 이런 설정이 자동으로 된다고 한다.  

# SparkSession 생성
```
import pyspark
myConf=pyspark.SparkConf()
spark = pyspark.sql.SparkSession.builder\
    .master("local")\
    .appName("myApp")\
    .config(conf=myConf)\
    .getOrCreate()
```
싱글톤 패턴으로 마스터 슬레이브? 관계 생성 잘 모르겠다.

# 데이터 구조

|데이터 구조|설명|
|:----:|:----:|
|RDD|비구조적, schema, low-level|
|Dataframe|구조적, schema를 가진다. Dataset[Row]와 같은 의미로, type를 강제하지 않는다.|
|Dataset|자바의 Generic과 같이 Dataset[T]으로 'type'을 강제하는 형식이다. Scala and Java에서 사용한다. Python loosely-typed이므로 사용하지 않는다.|

# List에서 RDD 생성하기
```
myList=[1,2,3,4,5,6,7]
myRdd1 = spark.sparkContext.parallelize(myList)
myRdd1.take(3)
```

```
# Out
[1, 2, 3]
```
