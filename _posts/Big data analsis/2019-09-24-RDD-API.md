---
layout: post
title: "RDD API"
tags: [Rdd, transformations, actions]
categories: [Big data analsis]
---
### spark를 사용하기위한 기초가 되는 코드
```python
import os
import sys
os.environ["SPARK_HOME"]=os.path.join(os.path.expanduser("~"),"spark-2.4.4-bin-hadoop2.7")
os.environ["PYLIB"]=os.path.join(os.environ["SPARK_HOME"],'python','lib')
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'py4j-0.10.7-src.zip'))
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'pyspark.zip'))
import pyspark
myConf=pyspark.SparkConf()
spark = pyspark.sql.SparkSession.builder\
    .master("local")\
    .appName("myApp")\
    .config(conf=myConf)\
    .getOrCreate()
```

### 파이썬 list로 내부에서 rdd 생성
```python
myList = [1,2,3,4,5,6,7]
myRdd1 = spark.sparkContext.parallelize(myList)
myRdd1.take(3)

>>[1, 2, 3]
```

### rdd 생성을 위한 파일 만들기
```python
%%writefile data/ds_spark_wiki.txt
Wikipedia
Apache Spark is an open source cluster computing framework.
아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.
Apache Spark Apache Spark Apache Spark Apache Spark
아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크
Originally developed at the University of California, Berkeley's AMPLab,
the Spark codebase was later donated to the Apache Software Foundation,
which has maintained it since.
Spark provides an interface for programming entire clusters with
implicit data parallelism and fault-tolerance.
```

### 파일로 rdd 생성하기
```python
myRdd2=spark.sparkContext\
    .textFile(os.path.join("data","ds_spark_wiki.txt"))
myRdd2.first()

>>'Wikipedia'
```

### rdd를 생성하기 위한 csv파일 만들기
```python
%%writefile ./data/ds_spark_2cols.csv
35, 2, 3
40, 27, 4
12, 38, 5
15, 31, 6
21, 1, 1
14, 19
46, 1
10, 34
28, 3
48, 1
16, 2
30, 3
32, 2
48, 1
31, 2
22, 1
12, 3
39, 29
19, 37
25, 2
```

### csv파일불러와서 rdd만들고 출력하기
```python
myRdd4 = spark.sparkContext\
    .textFile(os.path.join("data","ds_spark_2cols.csv"))
print(myRdd4.take(1)[:2])
print(type(myRdd4))
myList=myRdd4.take(5)
print(type(myList))

>> ['35, 2, 3']
>> <class 'pyspark.rdd.RDD'>
>> <class 'list'>
```

### Rdd.map 사용해보기
```python
nRdd = spark.sparkContext.parallelize([1,2,3,4])
squard = nRdd.map(lambda x : x ** 2)
print(squard.collect())

>> [1, 4, 9, 16]
```

### Rdd.reduce 사용해보기
```python
myRdd100 = spark.sparkContext.parallelize(range(1,101))
myRdd100.reduce(lambda x, y: x+y)

>> 5050
```

### Rdd.filter 사용해보기
```python
myRdd_spark = myRdd2.filter(lambda line: "Spark" in line)
print(myRdd_spark.collect())
```
```python
['Apache Spark is an open source cluster computing framework.', 'Apache Spark Apache Spark Apache Spark Apache Spark', 'the Spark codebase was later donated to the Apache Software Foundation,', 'Spark provides an interface for programming entire clusters with']
```


### rdd.foreach 사용해보기
```python
spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(lambda x: x + 1)
spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()

>> [2, 3, 4, 5, 6]
```

### 데이터가 몇 줄 있는지 확인하기
```python
myRdd2=spark.sparkContext\
    .textFile(os.path.join("data","ds_spark_wiki.txt"))
sentences=myRdd2.map(lambda x:x.split(" "))
sentences.count()

>> 10

##########################################################

def mySplit(x):
    return x.split(" ")
sentences2=myRdd2.map(mySplit)
sentences2.count()

>> 10
```

### 데이터의 단어를 한줄 씩 출력하기
```python
for line in sentences.collect():
    for word in line:
        print(word)
    print("\n-----")
```
```python
Wikipedia

-----
Apache
Spark
is
an
open
source
cluster
computing
framework.

-----
아파치
스파크는
오픈
소스
클러스터
컴퓨팅
프레임워크이다.

-----
Apache
Spark
Apache
Spark
Apache
Spark
Apache
Spark

-----
아파치
스파크
아파치
스파크
아파치
스파크
아파치
스파크

-----
Originally
developed
at
the
University
of
California,
Berkeley's
AMPLab,

-----
the
Spark
codebase
was
later
donated
to
the
Apache
Software
Foundation,

-----
which
has
maintained
it
since.

-----
Spark
provides
an
interface
for
programming
entire
clusters
with

-----
implicit
data
parallelism
and
fault-tolerance.

-----
```

### 데이터의 한줄당 길이 확인
```python
myRdd2.map(lambda s: len(s)).collect()
```

### 데이터 조작하기
```python
myList=["this is", "a line"]
_rdd = spark.sparkContext.parallelize(myList)
wordsRdd=_rdd.map(lambda x:x.split())
print(wordsRdd.collect())
repRdd=_rdd.map(lambda x:x.replace("a","AA"))
print(repRdd.collect())

>> [['this', 'is'], ['a', 'line']]
>> ['this is', 'AA line']
>> ['this is', 'AA line']

upperRDD = wordsRdd.map(lambda x: x[0].upper())
print(upperRDD.collect())

>> ['THIS', 'A']

upper2RDD = wordsRdd.map(lambda x: [i.upper() for i in x])
upper2RDD.collect()

>> [['THIS', 'IS'], ['A', 'LINE']]
```

### 데이터를 그룹별로 묶기 1
```python
myRdd_group=myRdd2.groupBy(lambda x:x[:2])
print(myRdd_group.collect())
for (k,v) in myRdd_group.collect():
    for eachValue in v:
        print(k, eachValue)
    print("____")
```
```python
[('Wi', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD20F1D0>), ('Ap', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD20FC50>), ('아파', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD20F198>), ('Or', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD20FB70>), ('th', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD20FC88>), ('wh', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD20F278>), ('Sp', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD20FCF8>), ('im', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD20FBA8>)]
Wi Wikipedia
____
Ap Apache Spark is an open source cluster computing framework.
Ap Apache Spark Apache Spark Apache Spark Apache Spark
____
아파 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.
아파 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크
____
Or Originally developed at the University of California, Berkeley's AMPLab,
____
th the Spark codebase was later donated to the Apache Software Foundation,
____
wh which has maintained it since.
____
Sp Spark provides an interface for programming entire clusters with
____
im implicit data parallelism and fault-tolerance.
____
```

### 데이터를 그룹별로 묶기 2
```python
_testList=[("key1",1),("key1",1),("key1",1),("key2",1),("key2",1),
           ("key1",1),("key2",1),
           ("key1",1),("key1",1),("key2",1),("key2",1)]
_testRdd=spark.sparkContext.parallelize(_testList)
print(_testRdd.groupBy(lambda x:x[0]).collect())
print(_testRdd.groupBy(lambda x:x[0]).mapValues(lambda x: list(x)).collect())
print(_testRdd.groupBy(lambda x:x[0]).mapValues(list).collect())
```

```python
[('key1', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD269588>), ('key2', <pyspark.resultiterable.ResultIterable object at 0x000001C6DD2695C0>)]
[('key1', [('key1', 1), ('key1', 1), ('key1', 1), ('key1', 1), ('key1', 1), ('key1', 1)]), ('key2', [('key2', 1), ('key2', 1), ('key2', 1), ('key2', 1), ('key2', 1)])]
[('key1', [('key1', 1), ('key1', 1), ('key1', 1), ('key1', 1), ('key1', 1), ('key1', 1)]), ('key2', [('key2', 1), ('key2', 1), ('key2', 1), ('key2', 1), ('key2', 1)])]
```







```python
```

```python
```

```python
```

