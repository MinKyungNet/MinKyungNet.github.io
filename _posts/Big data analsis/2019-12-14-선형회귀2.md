---
layout: post
title: "선형회귀2"
tags: [regression, spark]
categories: [Big data analsis]
---

# 문제 : 회귀식 그래프 함수

데이터
```python
import numpy as np

x = np.arange(0,9)
y = [19, 20, 20.5, 21.5, 22, 23, 23, 25.5, 24]
```

numpy 회귀식       
상수를 도출하기 위해 '1' 행을 추가하자.

```python
A = np.array([x, np.ones(len(x))])
print(A)
print(A.T)

>>>

[[0. 1. 2. 3. 4. 5. 6. 7. 8.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1.]]
[[0. 1.]
 [1. 1.]
 [2. 1.]
 [3. 1.]
 [4. 1.]
 [5. 1.]
 [6. 1.]
 [7. 1.]
 [8. 1.]]
```


```python
w = np.linalg.lstsq(A.T, y)[0]
fitted = w[0] * x + w[1]
print(fitted)

>>>

[19.18888889 19.90555556 20.62222222 21.33888889 22.05555556 22.77222222
 23.48888889 24.20555556 24.92222222]
```
그래프       
그래프의 marker는 색, 마커, 선모양으로 설정한다.

```python
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

plt.plot(x,fitted,'r-',x,y,'o',markersize=5)
plt.show()
```
![image](https://user-images.githubusercontent.com/50114210/70845871-294c9080-1e97-11ea-97ad-e3c54d2a947d.png)

함수로 만들기
```python
def drawRegress2d(x,y):
    """
    Parameters
    ----------
        arg1: list or array
        arg2: list or array
    Returns
    -------
        None
    Examples
    --------
        x=np.arange(0,9)
        y=[19, 20, 20.5, 21.5, 22, 23, 23, 25.5, 24] 
        drawRegress2d(x,y)
    """
    import matplotlib.pyplot as plt 
    import numpy as np
    A=np.array([x,np.ones(len(x))])
    w=np.linalg.lstsq(A.T,y)[0]
    fitted=w[0]*x+w[1]
    #plt.plot(x,y,'r-',xi,y,'o',markersize=5)
    plt.plot(x, y, 'o-', label='Original data', markersize=5)
    plt.plot(x, fitted, 'r', label='Fitted line')
    plt.show()

%matplotlib inline

import numpy as np
x=np.arange(0,9)
y=np.array([19, 20, 20.5, 21.5, 22, 23, 23, 25.5, 24])
drawRegress2d(x,y)
```
![image](https://user-images.githubusercontent.com/50114210/70845877-35d0e900-1e97-11ea-9133-c5580c5e5307.png)

# S.7 정규화 회귀모형
정규화는 선형회귀 계수에 대한 제약 조건을 추가함으로써 모형이 과도하게 최적화되는 현상을 막는 방법이다.         
모형이 과도하게 최적화되면 모형 계수의 크기도 과도하게 증가하는 경향이 나타난다. 따라서 정규화 방법에서 추가하는 제약 조건은 일반적으로 계수의 크기를 제한하는 방법이다. 일반적으로 다음과 같은 세가지 방법이 사용된다.       

- Ridge 오류함수에 가중치의 제곱합, 즉 L2를 최소화하는 제약조건을 추가한다. L1과 마찬가지고 가중치가 너무 크지 않은 방향으로 학습하게 된다.

- Lasso 오류함수에 가중치의 절대값, 즉 L1을 더해준다. L2와 마찬가지로 가중치가 너무 크지 않은 방향으로 학습하게 된다.

- Elastic Net L1 + L2, 즉 가중치의 절대값의 합과 제곱합을 동시에 제약 조건으로 가지는 모형이다.

# 문제 UCI Abalone

데이터 읽기        
```python
import pandas as pd
file_name="http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"
column_names = ["sex", "length", "diameter", "height", "whole weight", 
                "shucked weight", "viscera weight", "shell weight", "rings"]
abalone = pd.read_csv(file_name, names=column_names)
abalone.head()
>>>

	sex	length	diameter	height	whole weight	shucked weight	viscera weight	shell weight	rings
0	M	0.455	0.365	0.095	0.5140	0.2245	0.1010	0.150	15
1	M	0.350	0.265	0.090	0.2255	0.0995	0.0485	0.070	7
2	F	0.530	0.420	0.135	0.6770	0.2565	0.1415	0.210	9
3	M	0.440	0.365	0.125	0.5160	0.2155	0.1140	0.155	10
4	I	0.330	0.255	0.080	0.2050	0.0895	0.0395	0.055	7
```
데이터의 기초통계
data의 기초 통계를 살펴보자. 데이터 개수가 4177로 일치하는지, 그렇지 않다면 결측 값이 있다는 것이다. 평균, 표준편차, 최대, 최소를 보면서 outlier를 확인한다. 특이하게 height가 0인 경우가있다.

```python
abalone.describe()

>>>


length	diameter	height	whole weight	shucked weight	viscera weight	shell weight	rings
count	4177.000000	4177.000000	4177.000000	4177.000000	4177.000000	4177.000000	4177.000000	4177.000000
mean	0.523992	0.407881	0.139516	0.828742	0.359367	0.180594	0.238831	9.933684
std	0.120093	0.099240	0.041827	0.490389	0.221963	0.109614	0.139203	3.224169
min	0.075000	0.055000	0.000000	0.002000	0.001000	0.000500	0.001500	1.000000
25%	0.450000	0.350000	0.115000	0.441500	0.186000	0.093500	0.130000	8.000000
50%	0.545000	0.425000	0.140000	0.799500	0.336000	0.171000	0.234000	9.000000
75%	0.615000	0.480000	0.165000	1.153000	0.502000	0.253000	0.329000	11.000000
max	0.815000	0.650000	1.130000	2.825500	1.488000	0.760000	1.005000	29.000000
```
height가 없는 경우를 살펴보자. 이 경우 데이터가 0인 이유를 알 수 없는 경우, (1) 평균으로 대체하거나, (2) 모두 결측으로 처리할 수 있다. 여기서는 모두 제거한다.

```python
abalone = abalone[abalone['height']>0]
abalone.describe()
>>>


length	diameter	height	whole weight	shucked weight	viscera weight	shell weight	rings
count	4175.000000	4175.00000	4175.000000	4175.000000	4175.000000	4175.000000	4175.000000	4175.000000
mean	0.524065	0.40794	0.139583	0.829005	0.359476	0.180653	0.238834	9.935090
std	0.120069	0.09922	0.041725	0.490349	0.221954	0.109605	0.139212	3.224227
min	0.075000	0.05500	0.010000	0.002000	0.001000	0.000500	0.001500	1.000000
25%	0.450000	0.35000	0.115000	0.442250	0.186250	0.093500	0.130000	8.000000
50%	0.545000	0.42500	0.140000	0.800000	0.336000	0.171000	0.234000	9.000000
75%	0.615000	0.48000	0.165000	1.153500	0.502000	0.253000	0.328750	11.000000
max	0.815000	0.65000	1.130000	2.825500	1.488000	0.760000	1.005000	29.000000
```

상관관계        
seaborn에서 제공하는 pariplot은 변수 간 상관관계를 시각화하여 준다. seaborn은 그 자체로는 그래프를 그릴 수 없다. matplotlib을 기반으로 하기 때문에 이를 import하고 plt.show() hue는 변수명으로 선택하여 다른 색으로 그래프를 나타낸다. 이때 diag_kind를 'auto', 'hist', 'kde'가운데서 고르거나, 선택적으로 정의하여 모양을 다르게할 수 있다.

```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.pairplot(abalone, hue='sex', markers='x', diag_kind = "kde")
plt.show()
```
![image](https://user-images.githubusercontent.com/50114210/70845907-7a5c8480-1e97-11ea-99d4-131a931662be.png)


독립변수, 종속변수 준비     
'sex'는 명목변수로 dummpy 변수로 전환해서 사용한다.

```python
# Transform sex int a dummy variable
abalone['Male'] = (abalone['sex']=='M').astype(int)
abalone['Female'] = (abalone['sex']=='F').astype(int)
abalone['Infant'] = (abalone['sex']=='I').astype(int)
abalone['Infant'].head()
>>>

0    0
1    0
2    0
3    0
4    1
Name: Infant, dtype: int32
```

회귀분석

```python
from sklearn.model_selection import train_test_split
train, test = train_test_split(abalone, train_size = 0.5)

from sklearn import linear_model
from sklearn.metrics import r2_score, mean_absolute_error

xtrain = train.drop(['rings', 'sex'], axis=1)
ytrain = train['rings']
xtest = test.drop(['rings', 'sex'], axis=1)
ytest = test['rings']
print(xtrain.head())
>>>

      length  diameter  height  whole weight  shucked weight  viscera weight  \
3381   0.545     0.435   0.165        0.9955          0.3245          0.2665   
3411   0.430     0.320   0.100        0.3855          0.1920          0.0745   
2896   0.545     0.430   0.130        0.7595          0.3580          0.1530   
3873   0.330     0.250   0.075        0.1405          0.0560          0.0350   
3745   0.285     0.205   0.070        0.1060          0.0390          0.0285   

      shell weight  Male  Female  Infant  
3381        0.3250     1       0       0  
3411        0.1000     0       0       1  
2896        0.2055     0       0       1  
3873        0.0500     0       0       1  
3745        0.0340     0       0       1  
```


```python
regression = linear_model.LinearRegression()
regression.fit(xtrain, ytrain)

>>>

LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
         normalize=False)
```


```python
print(regression.intercept_)

>>>

3.218664281488719
```


```python
print(train.columns, regression.coef_)

>>>

Index(['sex', 'length', 'diameter', 'height', 'whole weight', 'shucked weight',
       'viscera weight', 'shell weight', 'rings', 'Male', 'Female', 'Infant'],
      dtype='object') [ -4.1251282   13.10013471  24.13732423  10.33170288 -21.21876545
 -12.56873752   6.29739703   0.27159511   0.19474687  -0.46634198]
```


```python
print(zip(train.columns, regression.coef_))

>>>

<zip object at 0x0000019F1B1C73C8>
```

정확성

```python
accuracy = regression.score(xtest, ytest)
print(accuracy*100,'%')

>>>

50.82638051000988 %
```


```python
import numpy as np
print(np.mean((regression.predict(xtest) - ytest) ** 2))

>>>

5.104298995897346
```
예측값과 실제값을 비교해보자.

```python
ypred = regression.predict(xtest)
r2 = r2_score(ytest, ypred)
mae = mean_absolute_error(ytest, ypred)

print(r2, mae)

>>>

0.5082638051000988 1.609014914543014
```


```python
y_actual_pred = pd.DataFrame({'Actual' : ytest, 'Predicted' : ypred.flatten()})
y_actual_pred.head()

>>>


Actual	Predicted
1032	11	14.914922
1521	12	12.206314
1463	9	9.036459
2099	10	8.901034
3202	17	16.962001
```

아래 그림은 예측값 ypred, 실제값 ytest의 산포도를 보여주고 있다. 실선은 실제값 ytest이 예측한 값과 동일한 경우를 말한다. 즉 100% 정확한 경우를 의미한다. 데이터의 후반부는 예측값이 약간 아래로 분포함을 보여주고 있다.
```python
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(ytest, ypred)
ax.set_ylabel('Predicted Rings')
ax.set_xlabel('Actual Rings')
ax.plot(ytest, ytest)
```

![image](https://user-images.githubusercontent.com/50114210/70845931-b1329a80-1e97-11ea-9307-6936ad5f979a.png)

# S.11 비선형 모델
데이터가 선형적 관계를 가지고 있지 않을 경우, n차원 다항식 nth order polynominal을 표현하며 다음과 같다.      
poly1d는 위 계수를 가지고 다항식을 만들어준다.

```python
import numpy as np
f = np.poly1d([0.2, 0, 3, -7, 5])
print(f)

>>>

     4     2
0.2 x + 3 x - 7 x + 5
```
선형 ployfit
x는 0~19의 정수ㄱ 값을 가지도록 생선한다. y는 기울기 slope 2, 절편 offset으로 하고 약간의 무작위수 np.random.normal()을 더한다.

```python
import numpy as np
npoints = 20
slope = 2
offset = 3
x = np.arange(npoints)
y = slope * x + offset + np.random.normal(size = npoints)
print(x)
print(y)
>>>

[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
[ 2.95520589  5.8054514   5.59201792 10.2684791  11.66672206 13.56322264
 14.40115025 17.78374775 17.70738195 22.66810072 22.52436621 27.46346823
 27.85177925 28.9849642  32.01953047 34.12472597 35.23753251 37.33725032
 38.64995068 41.57383271]
```
polyfit(x, y, deg)함수는 데이터에 대한 회귀식을 구한다. 즉 데이터 x, y를, deg는 차수를 넣어준다. 즉 차수1은 선형 즉 직선으로 y = mx + b 모델링하게 된다.

```python
p = np.polyfit(x, y, 1)
print(p)

>>>


[2.02073893 3.21192422]
```

위 계수는 2개이므로, 1차원 식으로 만들어진다. 당연히 이 식은 예측 값을 출력할 때 사용한다.

```python
f = np.poly1d(p)
print(f)

>>>
 
2.021 x + 3.212
```


```python
import matplotlib.pyplot as plt

f = np.poly1d(p)
fig = plt.figure()
ax = fig.add_subplot(111)
plt.plot(x, y, 'ro', label="Data")
plt.plot(x, f(x), 'b-', label="Polyfit")
plt.show()
```
![image](https://user-images.githubusercontent.com/50114210/70845946-d2938680-1e97-11ea-9606-0a6cbc546676.png)

비선현 ployfit
```python
import numpy as np

x = np.linspace(1, 7, 50)
y = np.sin(x)
x = np.array([10, 20, 30, 40, 50])
y = np.array([20, 40, 30, 10, 40])
%matplotlib inline

import matplotlib.pyplot as plt
plt.scatter(x,y)
```

![image](https://user-images.githubusercontent.com/50114210/70845952-dd4e1b80-1e97-11ea-83d1-8e184c9c57fb.png)


```python
np.linspace(1, 10, 10)

>>>

array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
```


```python
coefficients = np.polyfit(x, y, 3)
polynomial = np.poly1d(coefficients)
print(coefficients)
print(polynomial)

>>>

[ 6.66666667e-03 -5.92857143e-01  1.54047619e+01 -8.20000000e+01]
          3          2
0.006667 x - 0.5929 x + 15.4 x - 82
```


```python
plt.scatter(x, y)
x50 = np.arange(10, 51)
plt.plot(x50, polynomial(x50), 'r-')
plt.show()
```
![image](https://user-images.githubusercontent.com/50114210/70845961-ea6b0a80-1e97-11ea-81fe-3dd3a1414b0f.png)

# 문제 iris

데이터 읽기
```python
import pandas as pd
file_name = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris = pd.read_csv(file_name, names=["sepal_length","sepal_width","petal_length", "petal_width","species"])
iris.info()
>>>

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 150 entries, 0 to 149
Data columns (total 5 columns):
sepal_length    150 non-null float64
sepal_width     150 non-null float64
petal_length    150 non-null float64
petal_width     150 non-null float64
species         150 non-null object
dtypes: float64(4), object(1)
memory usage: 5.9+ KB
```


```python
iris.head()

>>>


sepal_length	sepal_width	petal_length	petal_width	species
0	5.1	3.5	1.4	0.2	Iris-setosa
1	4.9	3.0	1.4	0.2	Iris-setosa
2	4.7	3.2	1.3	0.2	Iris-setosa
3	4.6	3.1	1.5	0.2	Iris-setosa
4	5.0	3.6	1.4	0.2	Iris-setosa
```

describe()는 개수, 평균, 표준편차, 최대, 최소, 분기 값의 기본 통계량을 계산해준다. 데이터 개수는 150개로 서로 동일하다. 서로 다르면 결측 값이 있을 수 있다. 평균에 비해 표준편차가 너무 크지 않은지 살펴본다. 또한 최대, 최소를 비교해서 오류가 있는지, 예를 들면 마이너스 값이나 과도하게 큰 최대 값을 살펴본다. outlier는 회구식을 왜곡할 수 있어서 주의가 필요하다.
```python
iris.describe()

>>>


sepal_length	sepal_width	petal_length	petal_width
count	150.000000	150.000000	150.000000	150.000000
mean	5.843333	3.054000	3.758667	1.198667
std	0.828066	0.433594	1.764420	0.763161
min	4.300000	2.000000	1.000000	0.100000
25%	5.100000	2.800000	1.600000	0.300000
50%	5.800000	3.000000	4.350000	1.300000
75%	6.400000	3.300000	5.100000	1.800000
max	7.900000	4.400000	6.900000	2.500000
```

isnull() 함수로 missing values 결측 값이 있는지 확인한다.
```python
iris.isnull().sum()

>>>

sepal_length    0
sepal_width     0
petal_length    0
petal_width     0
species         0
dtype: int64
```
petal 너비와 길이 그래프       
우선 petal_length와 petal_width 두 변수간 관계와 회귀선을 그래프로 그려보자.    
sns.lmplot()는 입력변수 x, y, DataFrame을 인자로 넣어준다. 여기서 x, y는 DataFrame의 컬럼명을 적는다는 점에 주의한다.

```python
%matplotlib inline
iris.hist()
```
![image](https://user-images.githubusercontent.com/50114210/70845977-14bcc800-1e98-11ea-8fe4-329b2f1d1f2c.png)


```python
import seaborn as sns
sns.lmplot(x="petal_length", y="petal_width", data=iris)
plt.show()
```
![image](https://user-images.githubusercontent.com/50114210/70845979-1c7c6c80-1e98-11ea-8e38-b3dc306017c9.png)


```python
sns.pairplot(iris, hue='species', markers='x', diag_kind="kde")
```
![image](https://user-images.githubusercontent.com/50114210/70845983-24d4a780-1e98-11ea-997c-3578a2dabcad.png)

numpy 회귀분석     
petal_length와 petal_width를 회귀분석해보자. x에는 절편을 도출하기 위해 1을 추갛준다.

```python
import numpy as np
x = iris["petal_length"]
y = np.array(iris["petal_width"])
A = np.array([x, np.ones(len(x))])
print(A.T[:5])
print(y[:5])

>>>

[[1.4 1. ]
 [1.4 1. ]
 [1.3 1. ]
 [1.5 1. ]
 [1.4 1. ]]
[0.2 0.2 0.2 0.2 0.2]
```


```python
print(np.linalg.lstsq(A.T, y))

>>>

(array([ 0.41641913, -0.36651405]), array([6.34349195]), 2, array([52.03171411,  5.06958846]))
```


```python
Av = np.vander(x, 2)
print(Av[:5])
np.linalg.lstsq(Av, y)

>>>

[[1.4 1. ]
 [1.4 1. ]
 [1.3 1. ]
 [1.5 1. ]
 [1.4 1. ]]

(array([ 0.41641913, -0.36651405]),
 array([6.34349195]),
 2,
 array([52.03171411,  5.06958846]))
```
statsmodels 회귀분석    
입력값은 x의 Transpose인 A.T로 입력값을 적어준다. 그 결과는 R의 결과값과 같이 출력된다. result coef의 x1, const의 값이 동일하게 도출된다.

```python
import statsmodels.api as sm

model = sm.OLS(y, A.T)
results = model.fit()
print(results.summary())

>>>

OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.927
Model:                            OLS   Adj. R-squared:                  0.926
Method:                 Least Squares   F-statistic:                     1877.
Date:                Fri, 13 Dec 2019   Prob (F-statistic):           5.78e-86
Time:                        22:45:08   Log-Likelihood:                 24.400
No. Observations:                 150   AIC:                            -44.80
Df Residuals:                     148   BIC:                            -38.78
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
x1             0.4164      0.010     43.320      0.000       0.397       0.435
const         -0.3665      0.040     -9.188      0.000      -0.445      -0.288
==============================================================================
Omnibus:                        5.498   Durbin-Watson:                   1.461
Prob(Omnibus):                  0.064   Jarque-Bera (JB):                5.217
Skew:                           0.353   Prob(JB):                       0.0736
Kurtosis:                       3.579   Cond. No.                         10.3
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
```

sklearn 회귀분석
```python
from sklearn.linear_model import LinearRegression
X = iris["petal_length"]
Y = iris["petal_width"]

# Fit the linear model
model = LinearRegression()
results = model.fit(A.T, y)
print(results.intercept_, results.coef_)

>>>

-0.3665140452167277 [0.41641913 0.        ]
```

모든
```python
x=iris.values[:,0:3]
y=iris.values[:,3]
```


```python
model = LinearRegression()
results = model.fit(x, y)
print(results.intercept_, results.coef_)

>>>

-0.2487235860244541 [-0.21027133  0.22877721  0.52608818]
```


```python
dummies = pd.get_dummies(iris["species"])
iris = pd.concat([iris, dummies], axis=1)
iris.head()

>>>

	sepal_length	sepal_width	petal_length	petal_width	species	Iris-setosa	Iris-versicolor	Iris-virginica	Iris-setosa	Iris-versicolor	Iris-virginica
0	5.1	3.5	1.4	0.2	Iris-setosa	1	0	0	1	0	0
1	4.9	3.0	1.4	0.2	Iris-setosa	1	0	0	1	0	0
2	4.7	3.2	1.3	0.2	Iris-setosa	1	0	0	1	0	0
3	4.6	3.1	1.5	0.2	Iris-setosa	1	0	0	1	0	0
4	5.0	3.6	1.4	0.2	Iris-setosa	1	0	0	1	0	0
```

문제 : 주택가격의 예측             
데이터 
```python
from sklearn import datasets
boston = datasets.load_boston()
print(boston.keys())
>>>

dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])
```
저장되어 있는 데이터 구분을 보자.
- 데이터는 말 그대로 원본 데이터
- feature_names는 속성명
- DESCR는 데이터 관련한 설명을 제공하고 있고
- target은 종속변수 '주택각격'으로, 506개 타운의 주택 가격 중앙값

```python
print(boston.keys())

>>>

dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])
```


```python
print(boston.data.shape)

>>>

(506, 13)
```


```python
print(boston.feature_names)

>>>

['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'
 'B' 'LSTAT']
```

주택가격에 대해 그래프를 그려보자. boston.data[행, 열]로 표기한다. 예를 들어 [:, 1]는 1열의 모든 행을 의미한다.
```python
%matplotlib inline

import matplotlib.pyplot as plt
plt.scatter(boston.data[:,2], boston.target, color = 'r')
plt.show()
```
![image](https://user-images.githubusercontent.com/50114210/70846021-9280d380-1e98-11ea-95de-f46e799d64ef.png)

X, Y    
데이터를 Pandas로 변환해서 일부를 출력해보자.
```python
import pandas as pd
X = pd.DataFrame(boston.data, columns=boston.feature_names)
X.head()

>>>

	CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT
0	0.00632	18.0	2.31	0.0	0.538	6.575	65.2	4.0900	1.0	296.0	15.3	396.90	4.98
1	0.02731	0.0	7.07	0.0	0.469	6.421	78.9	4.9671	2.0	242.0	17.8	396.90	9.14
2	0.02729	0.0	7.07	0.0	0.469	7.185	61.1	4.9671	2.0	242.0	17.8	392.83	4.03
3	0.03237	0.0	2.18	0.0	0.458	6.998	45.8	6.0622	3.0	222.0	18.7	394.63	2.94
4	0.06905	0.0	2.18	0.0	0.458	7.147	54.2	6.0622	3.0	222.0	18.7	396.90	5.33
```
X 데이터프레임에 Y를 추가하려면 간단하게 컬럼명을 정해주고 Y를 할당하면 된다.

```python
Y = pd.DataFrame(boston.target, columns=["MEDV"])
Y.head()
>>>


MEDV
0	24.0
1	21.6
2	34.7
3	33.4
4	36.2
```
탐색

```python
import seaborn as sns
cols = ["RM", "AGE", "RAD"]
sns.pairplot(X[cols])
plt.show()
```

![image](https://user-images.githubusercontent.com/50114210/70846037-a62c3a00-1e98-11ea-97ca-023a21e450af.png)

```python
X.describe()

>>>

	CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT
count	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000	506.000000
mean	3.613524	11.363636	11.136779	0.069170	0.554695	6.284634	68.574901	3.795043	9.549407	408.237154	18.455534	356.674032	12.653063
std	8.601545	23.322453	6.860353	0.253994	0.115878	0.702617	28.148861	2.105710	8.707259	168.537116	2.164946	91.294864	7.141062
min	0.006320	0.000000	0.460000	0.000000	0.385000	3.561000	2.900000	1.129600	1.000000	187.000000	12.600000	0.320000	1.730000
25%	0.082045	0.000000	5.190000	0.000000	0.449000	5.885500	45.025000	2.100175	4.000000	279.000000	17.400000	375.377500	6.950000
50%	0.256510	0.000000	9.690000	0.000000	0.538000	6.208500	77.500000	3.207450	5.000000	330.000000	19.050000	391.440000	11.360000
75%	3.677083	12.500000	18.100000	0.000000	0.624000	6.623500	94.075000	5.188425	24.000000	666.000000	20.200000	396.225000	16.955000
max	88.976200	100.000000	27.740000	1.000000	0.871000	8.780000	100.000000	12.126500	24.000000	711.000000	22.000000	396.900000	37.970000
```

각 속성의 통계량을 살펴보자. 데이터 개수는 506개로 서로 일치한다. 최소 값에 음수 값이 있는지, 평균 및 표준편차가 서로 많이 차이가 나는지 살펴본다.
훈련 데이터 분리
```python
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=0)
X_train.shape, Y_train.shape, X_test.shape, Y_test.shape
>>>

((303, 13), (303, 1), (203, 13), (203, 1))
```

모델링
```python
from sklearn.linear_model import LinearRegression

lm = LinearRegression()
X_train = np.array(X_train)
X_test = np.array(X_test)
Y_train = np.array(Y_train)
Y_test = np.array(Y_test)

lm.fit(X_train, Y_train)

Y_pred = lm.predict(X_test)
Y_pred.shape
>>>

(203, 1)
```

테스트 데이터에 대해 예측된 값과 실제를 비교해보자. 다중회귀분석이므로 2차원으로 그래프를 작성하는 것은 불가능하다. 예측값은 선형회귀선이므로 대각선을 가로지르는 선이 되 고, 분포가 흩어져 있는만큼 오류가 된다.
```python
plt.scatter(Y_test, Y_pred)
plt.xlabel("Prices: $Y_i$")
plt.ylabel("Predicted prices: $What {Y}_i$")
plt.title("Prices vs Pridicted prices: $Y_i$ vs $What{Y}_i$")
```

![image](https://user-images.githubusercontent.com/50114210/70846050-c22fdb80-1e98-11ea-956c-f4927f10f1e5.png)
오류

```python
import sklearn
mse = sklearn.metrics.mean_squared_error(Y_test, Y_pred)
print(mse)

>>>

25.79036215070249
```
numpy로 listsq

```python
import numpy as np
xarr = np.array(X)
yarr = np.array(Y)
print(np.linalg.lstsq(xarr, yarr))

>>>

(array([[-9.28965170e-02],
       [ 4.87149552e-02],
       [-4.05997958e-03],
       [ 2.85399882e+00],
       [-2.86843637e+00],
       [ 5.92814778e+00],
       [-7.26933458e-03],
       [-9.68514157e-01],
       [ 1.71151128e-01],
       [-9.39621540e-03],
       [-3.92190926e-01],
       [ 1.49056102e-02],
       [-4.16304471e-01]]), array([12228.04626104]), 13, array([1.25851816e+04, 3.44597406e+03, 6.45757109e+02, 4.02050461e+02,
       1.58964612e+02, 1.21502936e+02, 9.04652420e+01, 7.79311708e+01,
       6.50828345e+01, 2.46251803e+01, 1.80945451e+01, 5.51505065e+00,
       1.48096916e+00]))
```
# S.12 Spark

```python
import os
import sys
os.environ["SPARK_HOME"]=os.path.join(os.path.expanduser("~"),"spark-2.4.4-bin-hadoop2.7")
os.environ["PYLIB"]=os.path.join(os.environ["SPARK_HOME"],'python','lib')
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'py4j-0.10.7-src.zip'))
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'pyspark.zip'))
import pyspark
myConf=pyspark.SparkConf()
spark = pyspark.sql.SparkSession.builder\
    .master("local")\
    .appName("myApp")\
    .config(conf=myConf)\
    .getOrCreate()
```

데이터 생성      
sklearn 데이터를 Pandas를 경유해 생성         
boston.data와 boston.target 데이터를 어떻게 합치나      
두 데이터를 합치려면 shape를 알아야한다.

```python
print("boston data: ", type(boston.data), boston.data.shape)
print("boston target: ", type(boston.target), boston.target.shape)

>>>

boston data:  <class 'numpy.ndarray'> (506, 13)
boston target:  <class 'numpy.ndarray'> (506,)
```

boston.data와 boston.target 데이터를 합치기

```python
import numpy as np
XY = np.column_stack((boston.data, boston.target))
np.append(boston.feature_names, "MEDV")
>>>

array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',
       'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'], dtype='<U7')
```


```python
import pandas as pd

#X=pd.DataFrame(boston.data, columns=boston.feature_names)
#Y=pd.DataFrame(boston.target, columns=["MEDV"])
Z=pd.DataFrame(XY, columns=np.append(boston.feature_names,"MEDV"))
Z.head()
>>>

	CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT	MEDV
0	0.00632	18.0	2.31	0.0	0.538	6.575	65.2	4.0900	1.0	296.0	15.3	396.90	4.98	24.0
1	0.02731	0.0	7.07	0.0	0.469	6.421	78.9	4.9671	2.0	242.0	17.8	396.90	9.14	21.6
2	0.02729	0.0	7.07	0.0	0.469	7.185	61.1	4.9671	2.0	242.0	17.8	392.83	4.03	34.7
3	0.03237	0.0	2.18	0.0	0.458	6.998	45.8	6.0622	3.0	222.0	18.7	394.63	2.94	33.4
4	0.06905	0.0	2.18	0.0	0.458	7.147	54.2	6.0622	3.0	222.0	18.7	396.90	5.33	36.2
```


```python
from pyspark.sql.types import FloatType
bos = spark.createDataFrame(Z)
bos.head()
>>>

Row(CRIM=0.00632, ZN=18.0, INDUS=2.31, CHAS=0.0, NOX=0.538, RM=6.575, AGE=65.2, DIS=4.09, RAD=1.0, TAX=296.0, PTRATIO=15.3, B=396.9, LSTAT=4.98, MEDV=24.0)
```

"MEDV"는 제외하고 feature를 만든다.
```python
from pyspark.ml.feature import VectorAssembler
vectorAssembler = VectorAssembler(inputCols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',
       'TAX', 'PTRATIO', 'B', 'LSTAT'], outputCol = 'features')
bosFeatures = vectorAssembler.transform(bos)
bosFeatures.show(10)
```


```python
from pyspark.ml.regression import LinearRegression
lr = LinearRegression(featuresCol = 'features', labelCol = 'MEDV', maxIter = 10, regParam = 0.3, elasticNetParam=0.8)
lr_model = lr.fit(bosFeatures)
Y.head()
>>>

	MEDV
0	24.0
1	21.6
2	34.7
3	33.4
4	36.2
```

# 문제 diabetes
데이터
```python
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

diabetes = datasets.load_diabetes()
diabetes.keys()
>>>

dict_keys(['data', 'target', 'DESCR', 'feature_names', 'data_filename', 'target_filename'])
```
x, y       
단순 회귀분석을 하기위해, 3번째 데이터만 추출해서 독립변수 x로 한다.

```python
diabetes_X = diabetes.data[:, np.newaxis, 2]
diabetes_X[:5]
>>>

array([[ 0.06169621],
       [-0.05147406],
       [ 0.04445121],
       [-0.01159501],
       [-0.03638469]])
```


```python
diabetes.target[:5]

>>>

array([151.,  75., 141., 206., 135.])
```

회귀모델
```python
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]

# Split the targets into training/testing sets
diabetes_y_train = diabetes.target[:-20]
diabetes_y_test = diabetes.target[-20:]

# Create linear regression object
regr = linear_model.LinearRegression(fit_intercept=False)

# Train the model using the training sets
regr.fit(diabetes_X_train, diabetes_y_train)

# Make predictions using the testing set
diabetes_y_pred = regr.predict(diabetes_X_test)

# The coefficients
print('Coefficients: \n', regr.coef_)
>>>

Coefficients: 
 [970.16723129]
```


```python
regr.intercept_

>>>

0.0
```


```python
regr.get_params()

>>>

{'copy_X': True, 'fit_intercept': False, 'n_jobs': None, 'normalize': False}
```


```python
np.linalg.lstsq(diabetes_X_train, diabetes_y_train)

>>>

(array([970.16723129]), array([11536167.22048419]), 1, array([0.97763907]))
```
