---
layout: post
title: "선형 회귀"
tags: [regression, gradient, ols, inverse matrix]
categories: [Big data analsis]
---

# S.2 문제의 이해
S.2 문제의 이해
어떤 값을 예측한다고 하고, 그 값에 영향을 미치는 변수들이 있다고 하자. 예측 값을 출력변수 Output variables 또는 목표변수 Target variables라고 하고, 영향변수를 입력변수 Input variables라고 한다. 예를 들어, 영화매출을 예측한다고 하자. 매출에 영향을 미칠 수 있는 변수로 평점, 광고비, 투자비, 개봉일 매출액, 조회수 등을 꼽을 수 있다. 회귀분석은 입력변수로 부터 목표 값을 예측을 하는 문제에 사용한다.

# S.3 회귀식
x : 입력 데이터
y : 종속 변수, 1개의 클래스 값을 가진다.

S.4 OLS
OLS는 실제값과 예측의 차이의 제곱을 합계낸 값으로, 이를 최소화하는 지수를 구하면 최적식을 구할 수 있다. 실제값과 예측의 차이는 음수가 나올 수 있음으로 제곱한다.

# S.4.1 데이터

```python
import numpy as np
x = np.array([1,2,3,4])
y = np.array([6,5,7,10])
```

# S.4.2 그래프

```python
%matplotlib inline
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(x,y)
```
![image](https://user-images.githubusercontent.com/50114210/70706302-fd0b0580-1d18-11ea-845d-2b648435f625.png)

```python
yhat = x*1.4 + 3.5
print(yhat)

>>>

[4.9 6.3 7.7 9.1]
```

```python
yhat-y

>>>

array([-1.1,  1.3,  0.7, -0.9])
```

```python
np.sum(np.power(yhat-y, 2))

>>>

4.199999999999998
```

# S.4.4 Python으로 미분해서 풀어보자

```python
import sympy as sp

f=210 + 4*b0**2 + 30*b1**2 + 20*b0*b1 - 56*b0 - 154*b1
fdiff_b0 = sp.diff(f, b0)
fdiff_b1 = sp.diff(f, b1)
print("fdiff with b0: ", fdiff_b0, "fdiff with b1: ", fdiff_b1)
b0, b1 = sp.symbols('b0 b1')
exp = sp.solve([fdiff_b0, fdiff_b1], [b0, b1])
print("exp={0}".format(exp))

>>>

fdiff with b0:  8*b0 + 20*b1 - 56 fdiff with b1:  20*b0 + 60*b1 - 154
exp={b0: 7/2, b1: 7/5}
```

# S.4.5 R^2
R^2는 종속변수의 분산을 독립변수로 얼마나 예측할 수 있는지 나타낸다.

```python
import numpy as np
x=np.arange(0,9)
y=np.array([19, 20, 20.5, 21.5, 22, 23, 23, 25.5, 24])

A=np.array([x,np.ones(9)])
print(A)

w0, w1 = np.linalg.lstsq(A.T, y)[0]
print(w0, w1)

yhat = w0 * x + w1
print(yhat)

%matplotlib inline
import matplotlib.pyplot as plt
plt.plot(x, yhat, '-r', x, y, 'bo')
plt.show()

>>>

[[0. 1. 2. 3. 4. 5. 6. 7. 8.]
 [1. 1. 1. 1. 1. 1. 1. 1. 1.]]
 
 0.7166666666666672 19.188888888888897
 
 [19.18888889 19.90555556 20.62222222 21.33888889 22.05555556 22.77222222
 23.48888889 24.20555556 24.92222222]
```
![image](https://user-images.githubusercontent.com/50114210/70706530-6854d780-1d19-11ea-8a01-49ebb84f74cb.png)

```python
SSR = sum((y-yhat)**2)
TSS = sum((y-np.mean(y))**2)
r_squared = 1 - (float(SSR))/TSS
print(r_squared)

adjusted_r_squared = 1 - (1-r_squared) * (len(y)-1) / (len(y)-A.shape[0]-1)
print(adjusted_r_squared)

import statsmodels.formula.api as sm
result = sm.OLS(y, A.T).fit()
print(result.summary())
print(result.rsquared, result.rsquared_adj)

>>>

0.9138385502471171
0.8851180669961561

                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.914
Model:                            OLS   Adj. R-squared:                  0.902
Method:                 Least Squares   F-statistic:                     74.24
Date:                Wed, 11 Dec 2019   Prob (F-statistic):           5.66e-05
Time:                        01:00:18   Log-Likelihood:                -7.6827
No. Observations:                   9   AIC:                             19.37
Df Residuals:                       7   BIC:                             19.76
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
x1             0.7167      0.083      8.616      0.000       0.520       0.913
const         19.1889      0.396     48.458      0.000      18.253      20.125
==============================================================================
Omnibus:                        4.624   Durbin-Watson:                   3.078
Prob(Omnibus):                  0.099   Jarque-Bera (JB):                1.154
Skew:                           0.773   Prob(JB):                        0.561
Kurtosis:                       3.830   Cond. No.                         9.06
==============================================================================
```

# S.5 matrix inverse method
```python
import numpy as np
y=[8,9]
x=[[2,-4],[3,6]]

np.set_printoptions(precision=2, suppress=True)

xI = np.linalg.inv(x) 
print("x inv: \n", xI)

v=np.array([[2,3],[4,5]])
vI=np.linalg.inv(v)
print("v={0}\nv.I={1}".format(v,vI))

left=1./(2*5-3*4)
right=np.array([5,-3,-4,2])
print(left*right)

import numpy as np

x=np.array([[2,-4],[3,6]])
y=np.array([8,9])

xI=np.linalg.inv(x)

w1,w2=np.dot(xI,y)
print(w1,w2)

print(np.linalg.solve(x,y))

>>>

x inv: 
 [[ 0.25  0.17]
 [-0.12  0.08]]
 
 v=[[2 3]
 [4 5]]
v.I=[[-2.5  1.5]
 [ 2.  -1. ]]
 
 [-2.5  1.5  2.  -1. ]
 
 3.5 -0.25
 
 [ 3.5  -0.25]
```

# 문제 : Matrix Inverse Method 회귀 모델
```python
import numpy as np
x=np.array([1,2,3,4])
y=np.array([6,5,7,10])

x=np.array([x,np.ones(len(x))])
x=x.T
print(x)

>>>

[[1. 1.]
 [2. 1.]
 [3. 1.]
 [4. 1.]]
```

```python
print(x.shape,x.T.shape)

>>>

(4, 2) (2, 4)
```

```python
np.dot(x,x.T)

>>>

array([[ 2.,  3.,  4.,  5.],
       [ 3.,  5.,  7.,  9.],
       [ 4.,  7., 10., 13.],
       [ 5.,  9., 13., 17.]])
```

```python
xTx=np.dot(x.T,x)
print("xTx=",xTx)

>>>

xTx= [[30. 10.]
 [10.  4.]]
``` 
 
```python
xTxI=np.linalg.inv(xTx)
print("xTxI=",xTxI)

>>>

xTxI= [[ 0.2 -0.5]
 [-0.5  1.5]]
``` 
 
```python
xTyT=np.dot(x.T,y)
print("xTyT=",xTyT)

>>>

xTyT= [77. 28.]
```

```python
print("weights=",np.dot(xTxI,xTyT))

>>>

weights= [1.4 3.5]
```

```python
np.dot(np.linalg.inv(np.dot(x.T, x)), np.dot(x.T, y))

>>>

array([1.4, 3.5])
```

# numpy 최소 자승법
```python
np.warnings.filterwarnings('ignore')   #suppress numpy warnings
print("numpy lstsq: ",np.linalg.lstsq(x,y))

>>>

numpy lstsq:  (array([1.4, 3.5]), array([4.2]), 2, array([5.78, 0.77]))
```

```python
np.set_printoptions(formatter={'float': '{: 0.2f}'.format})
w1,w0=np.linalg.lstsq(x,y)[0]
print("w1=",w1,"\nw0=",w0)

>>>

w1= 1.4000000000000006 
w0= 3.499999999999998
```

```python
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

# compute yhat from the model values of b0 and b1
x=np.array([1,2,3,4])
y=np.array([6,5,7,10])
_x=np.array([x,np.ones(len(x))])
_x=_x.T
b1,b0=np.linalg.lstsq(_x,y)[0]
yhat=b0+b1*x
fig=plt.figure()
ax=fig.add_subplot(111)
ax.scatter(x,y)
ax.plot(x,yhat)
```
![image](https://user-images.githubusercontent.com/50114210/70706987-6fc8b080-1d1a-11ea-8ae3-d94b8d947fb1.png)

# S.6 Gradient 알고리즘
### data
```python
import numpy as np
x=np.array([1,2,3,4])
y=np.array([6,5,7,10])

```


### theta
```python
x=np.array([x,np.ones(len(x))])  # w0 * x0 + w1 * x1
x=x.T
print(x, x.shape)

>>>

[[ 1.00  1.00]
 [ 2.00  1.00]
 [ 3.00  1.00]
 [ 4.00  1.00]] (4, 2)
```


### h(x)
```python
theta=np.array(np.ones([x.shape[1]]))
print(theta)

>>>

[ 1.00  1.00]
```

### error
```python
def h(x,theta):
    return np.dot(x,theta)

h(x,theta)

>>>

array([ 2.00,  3.00,  4.00,  5.00])
```


### Loss
```python
loss = np.sum(y-h(x,theta)) * (-2) / len(x)
print(loss)

>>>

-7.0
```

### gradient
```python
gradient=np.dot(x.T,error)

>>>

[-37.00 -14.00]
```

### update
```python
alpha = 0.01
theta -= alpha * gradient
print(theta)

>>>

[ 1.37  1.14]
```

### gradient descent
```python
alpha = 0.01
numIterations = 4000
theta = np.ones([x.shape[1]])

for i in range(numIterations):
    h = np.dot(x, theta)
    error = h-y
    cost = np.sum(error**2)/(2*len(x))
    gradient = np.dot(x.T, error)
    theta -= alpha * gradient
    if i % 100 == 0:
        print("Iteration {0} | theta {1} Cost {2:.5f}".format(i, theta, cost))

>>>

Iteration 0 | theta [ 1.37  1.14] Cost 6.75000
Iteration 100 | theta [ 1.79  2.34] Cost 0.63779
Iteration 200 | theta [ 1.62  2.87] Cost 0.55893
Iteration 300 | theta [ 1.52  3.15] Cost 0.53521
Iteration 400 | theta [ 1.46  3.31] Cost 0.52807
Iteration 500 | theta [ 1.44  3.40] Cost 0.52592
Iteration 600 | theta [ 1.42  3.44] Cost 0.52528
Iteration 700 | theta [ 1.41  3.47] Cost 0.52508
Iteration 800 | theta [ 1.41  3.48] Cost 0.52503
Iteration 900 | theta [ 1.40  3.49] Cost 0.52501
Iteration 1000 | theta [ 1.40  3.49] Cost 0.52500
Iteration 1100 | theta [ 1.40  3.50] Cost 0.52500
Iteration 1200 | theta [ 1.40  3.50] Cost 0.52500
Iteration 1300 | theta [ 1.40  3.50] Cost 0.52500
Iteration 1400 | theta [ 1.40  3.50] Cost 0.52500
Iteration 1500 | theta [ 1.40  3.50] Cost 0.52500
Iteration 1600 | theta [ 1.40  3.50] Cost 0.52500
Iteration 1700 | theta [ 1.40  3.50] Cost 0.52500
Iteration 1800 | theta [ 1.40  3.50] Cost 0.52500
Iteration 1900 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2000 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2100 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2200 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2300 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2400 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2500 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2600 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2700 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2800 | theta [ 1.40  3.50] Cost 0.52500
Iteration 2900 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3000 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3100 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3200 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3300 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3400 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3500 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3600 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3700 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3800 | theta [ 1.40  3.50] Cost 0.52500
Iteration 3900 | theta [ 1.40  3.50] Cost 0.52500
```

```python
def computeAvgError(a,b,x,y):
    totalError = 0
    for i in range(0, len(x)):
        totalError += (y[i] - (a + b * x[i])) ** 2
    return totalError / float(len(x))

a=1
b=1
alpha=0.01
n=len(x)
iter=1500
for j in range(iter):
    aGradient = 0
    bGradient = 0
    for i in range(n):
        aGradient += (2./n) * (y[i] - ((a + b * x[i])))*(-1)
        bGradient += (2./n) * (y[i] - ((a + b * x[i])))*(-x[i])
    a = a - (alpha * aGradient)
    b = b - (alpha * bGradient)
    if (j%100==0):
        print("iter:{0} a={1:.3f} b={2:.3f} AvgError={3:.3f}".format(j,a[0],b[0],computeAvgError(a,b,x,y)[0]))

>>>

iter:0 a=1.070 b=1.185 AvgError=9.914
iter:100 a=1.935 b=1.932 AvgError=1.459
iter:200 a=2.340 b=1.794 AvgError=1.275
iter:300 a=2.641 b=1.692 AvgError=1.173
iter:400 a=2.863 b=1.617 AvgError=1.118
iter:500 a=3.028 b=1.560 AvgError=1.087
iter:600 a=3.150 b=1.519 AvgError=1.070
iter:700 a=3.241 b=1.488 AvgError=1.061
iter:800 a=3.308 b=1.465 AvgError=1.056
iter:900 a=3.358 b=1.448 AvgError=1.053
iter:1000 a=3.395 b=1.436 AvgError=1.052
iter:1100 a=3.422 b=1.427 AvgError=1.051
iter:1200 a=3.442 b=1.420 AvgError=1.051
iter:1300 a=3.457 b=1.415 AvgError=1.050
iter:1400 a=3.468 b=1.411 AvgError=1.050
```

```python
import random

def computeAvgError(a, b, x, y):
    totalError = 0
    for i in range(0, len(x)):
        totalError += (y[i] - (a + b * x[i])) ** 2
    return totalError / float(len(x))

def GradientDescent(x, y, alpha, iter):
    # x : attribute, 1d float array
    # y : class, 1d int array
    # alpha : learning rate
    
    a = random.random()
    b = random.random()
    alpha = 0.01
    n = len(x)
    
    for j in range(iter):
        aGradient = 0
        bGradient = 0
        for i in range(n):
            aGradient += (2./n) * (y[i] - ((a+b*x[i]))) * (-1)
            bGradient += (2./n) * (y[i] - ((a+b*x[i]))) * (-x[i])
        a = a - (alpha * aGradient)
        b = b - (alpha * bGradient)
        if (j % 100 == 0):
            print("iter: {0} a = {1:.3f} b = {2:.3f} AvgError = {3:.3f}".format(j, a, b, computeAvgError(a, b, x, y)))
    return a, b

x=np.array([1,2,3,4])
y=np.array([6,5,7,10])
a,b=GradientDescent(x,y,alpha,10000)

print("---> a={0}, b={1} after iterations".format(a,b))

yhat=a + b*x
print(yhat)

>>>

iter: 0 a = 0.604 b = 1.119 AvgError = 14.098
iter: 100 a = 1.640 b = 2.032 AvgError = 1.627
iter: 200 a = 2.122 b = 1.869 AvgError = 1.367
iter: 300 a = 2.479 b = 1.747 AvgError = 1.224
iter: 400 a = 2.744 b = 1.657 AvgError = 1.146
iter: 500 a = 2.940 b = 1.591 AvgError = 1.102
iter: 600 a = 3.085 b = 1.541 AvgError = 1.079
iter: 700 a = 3.192 b = 1.505 AvgError = 1.066
iter: 800 a = 3.272 b = 1.478 AvgError = 1.059
iter: 900 a = 3.331 b = 1.457 AvgError = 1.055
iter: 1000 a = 3.375 b = 1.443 AvgError = 1.053
iter: 1100 a = 3.407 b = 1.432 AvgError = 1.051
iter: 1200 a = 3.431 b = 1.423 AvgError = 1.051
iter: 1300 a = 3.449 b = 1.417 AvgError = 1.050
iter: 1400 a = 3.462 b = 1.413 AvgError = 1.050
iter: 1500 a = 3.472 b = 1.410 AvgError = 1.050
iter: 1600 a = 3.479 b = 1.407 AvgError = 1.050
iter: 1700 a = 3.485 b = 1.405 AvgError = 1.050
iter: 1800 a = 3.489 b = 1.404 AvgError = 1.050
iter: 1900 a = 3.492 b = 1.403 AvgError = 1.050
iter: 2000 a = 3.494 b = 1.402 AvgError = 1.050
iter: 2100 a = 3.495 b = 1.402 AvgError = 1.050
iter: 2200 a = 3.497 b = 1.401 AvgError = 1.050
iter: 2300 a = 3.497 b = 1.401 AvgError = 1.050
iter: 2400 a = 3.498 b = 1.401 AvgError = 1.050
iter: 2500 a = 3.499 b = 1.400 AvgError = 1.050
iter: 2600 a = 3.499 b = 1.400 AvgError = 1.050
iter: 2700 a = 3.499 b = 1.400 AvgError = 1.050
iter: 2800 a = 3.499 b = 1.400 AvgError = 1.050
iter: 2900 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3000 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3100 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3200 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3300 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3400 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3500 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3600 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3700 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3800 a = 3.500 b = 1.400 AvgError = 1.050
iter: 3900 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4000 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4100 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4200 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4300 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4400 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4500 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4600 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4700 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4800 a = 3.500 b = 1.400 AvgError = 1.050
iter: 4900 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5000 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5100 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5200 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5300 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5400 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5500 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5600 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5700 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5800 a = 3.500 b = 1.400 AvgError = 1.050
iter: 5900 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6000 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6100 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6200 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6300 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6400 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6500 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6600 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6700 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6800 a = 3.500 b = 1.400 AvgError = 1.050
iter: 6900 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7000 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7100 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7200 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7300 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7400 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7500 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7600 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7700 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7800 a = 3.500 b = 1.400 AvgError = 1.050
iter: 7900 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8000 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8100 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8200 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8300 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8400 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8500 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8600 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8700 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8800 a = 3.500 b = 1.400 AvgError = 1.050
iter: 8900 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9000 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9100 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9200 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9300 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9400 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9500 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9600 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9700 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9800 a = 3.500 b = 1.400 AvgError = 1.050
iter: 9900 a = 3.500 b = 1.400 AvgError = 1.050
---> a=3.499999999999757, b=1.4000000000000834 after iterations
[ 4.90  6.30  7.70  9.10]
```

