---
layout: post
title: "네트워크에 불법적으로 침입하는 사용자의 분석"
tags: [Spark, ]
categories : [Big data analsis]
---


# Spark를 사용하기 위한 준비
```python
import os
import sys
os.environ["SPARK_HOME"]=os.path.join(os.path.expanduser("~"),"spark-2.4.4-bin-hadoop2.7")
os.environ["PYLIB"]=os.path.join(os.environ["SPARK_HOME"],'python','lib')
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'py4j-0.10.7-src.zip'))
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'pyspark.zip'))
import pyspark
myConf=pyspark.SparkConf()
spark = pyspark.sql.SparkSession.builder\
    .master("local")\
    .appName("myApp")\
    .config(conf=myConf)\
    .getOrCreate()
```

# 데이터 불러오기
```python
import os
import urllib
_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'
_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')
if(not os.path.exists(_fname)):
    print("%s data does not exist! retrieving.." % _fname)
    _f=urllib.request.urlretrieve(_url,_fname)
```
url주소를 가져와서 저장할 디렉토리와 파일명 설정해주기     
파일이 없으면 url에서 지정 위치에 정한 이름으로 다운로드하기

# RDD만들기
```python
_rdd = spark.sparkContext.textFile(_fname)
_rdd.count()
_rdd.take(1)

>>

['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.']
```
위에서 불러온 파일로 RDD를 생성하고      
rdd내의 데이터 개수를 확인,     
실제 데이터의 모습을 확인한다.     

# RDD내의 데이터 콤마로 나누기
```python
_allRdd = _rdd.map(lambda x : x.split(','))
print(_allRdd.take(1))

>>

[['0', 'tcp', 'http', 'SF', '181', '5450', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '8', '8', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '9', '9', '1.00', '0.00', '0.11', '0.00', '0.00', '0.00', '0.00', '0.00', 'normal.']]
```

# reduceByKey로 41번째 요소의 종류, 개수세기
```python
_41 = _allRdd.map(lambda x: (x[41], 1))
_41.reduceByKey(lambda x,y: x+y).collect()

>>

[('normal.', 97278),
 ('buffer_overflow.', 30),
 ('loadmodule.', 9),
 ('perl.', 3),
 ('neptune.', 107201),
 ('smurf.', 280790),
 ('guess_passwd.', 53),
 ('pod.', 264),
 ('teardrop.', 979),
 ('portsweep.', 1040),
 ('ipsweep.', 1247),
 ('land.', 21),
 ('ftp_write.', 8),
 ('back.', 2203),
 ('imap.', 12),
 ('satan.', 1589),
 ('phf.', 4),
 ('nmap.', 231),
 ('multihop.', 7),
 ('warezmaster.', 20),
 ('warezclient.', 1020),
 ('spy.', 2),
 ('rootkit.', 10)]
```
41번째 요소가 나쁜 놈인지 아닌지 구분해준다.         
41번째 요소의 종류와 각 종류의 개수를 reduceByKey로 확인한다.

# groupByKey로 41번째 요소의 종류, 개수 세기
```python
_41 = _allRdd.map(lambda x: (x[41], 1))
def f(x): return len(x)
_41.groupByKey().mapValues(f).collect()

>>

[('normal.', 97278),
 ('buffer_overflow.', 30),
 ('loadmodule.', 9),
 ('perl.', 3),
 ('neptune.', 107201),
 ('smurf.', 280790),
 ('guess_passwd.', 53),
 ('pod.', 264),
 ('teardrop.', 979),
 ('portsweep.', 1040),
 ('ipsweep.', 1247),
 ('land.', 21),
 ('ftp_write.', 8),
 ('back.', 2203),
 ('imap.', 12),
 ('satan.', 1589),
 ('phf.', 4),
 ('nmap.', 231),
 ('multihop.', 7),
 ('warezmaster.', 20),
 ('warezclient.', 1020),
 ('spy.', 2),
 ('rootkit.', 10)]
```
이번에는 groupByKey로 요소의 종류와 개수를 셌다.      
reduceByKey와는 달리   
mapValues를 해줘야 값을 읽어올 수 있다.        

# 정상적인 접근과 비정상적인 접근 구분하기
```python
_normalRdd=_allRdd.filter(lambda x: x[41]=="normal.")
_attackRdd=_allRdd.filter(lambda x: x[41]!="normal.")
print(_normalRdd.count())
print(_attackRdd.count())

>>

97278
396743
```
rdd의 filter함수를 사용하여        
41번째 요소가 normal이면 normalRdd에,           
41번째 요소가 normal이 아니면 attackRdd에 넣는다.

# DataFrame으로 만들기 전에 데이터의 구조 잡아주기
```python
from pyspark.sql import Row

_csv = _rdd.map(lambda l: l.split(","))
_csvRdd = _csv.map(lambda p: 
    Row(
        duration=int(p[0]), 
        protocol=p[1],
        service=p[2],
        flag=p[3],
        src_bytes=int(p[4]),
        dst_bytes=int(p[5]),
        attack=p[41]
    )
)

_df=spark.createDataFrame(_csvRdd)

_df.printSchema()
_df.show(5)

>>

root
 |-- attack: string (nullable = true)
 |-- dst_bytes: long (nullable = true)
 |-- duration: long (nullable = true)
 |-- flag: string (nullable = true)
 |-- protocol: string (nullable = true)
 |-- service: string (nullable = true)
 |-- src_bytes: long (nullable = true)

+-------+---------+--------+----+--------+-------+---------+
| attack|dst_bytes|duration|flag|protocol|service|src_bytes|
+-------+---------+--------+----+--------+-------+---------+
|normal.|     5450|       0|  SF|     tcp|   http|      181|
|normal.|      486|       0|  SF|     tcp|   http|      239|
|normal.|     1337|       0|  SF|     tcp|   http|      235|
|normal.|     1337|       0|  SF|     tcp|   http|      219|
|normal.|     2032|       0|  SF|     tcp|   http|      217|
+-------+---------+--------+----+--------+-------+---------+
only showing top 5 rows
```
dataFramd는 스키마를 자동으로 결정해준다.       
여기서는 Row와 람다를 활용해서       
각 데이터들을 우리가 원하는 형태로 변형하고 있다.           

# 유저 정의 함수와 열추가
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
attack_udf = udf(lambda x: "normal" if x =="normal." else "attack", StringType())
myDf=_df.withColumn("attackB", attack_udf(_df.attack))
myDf.printSchema()

>>

root
 |-- attack: string (nullable = true)
 |-- dst_bytes: long (nullable = true)
 |-- duration: long (nullable = true)
 |-- flag: string (nullable = true)
 |-- protocol: string (nullable = true)
 |-- service: string (nullable = true)
 |-- src_bytes: long (nullable = true)
 |-- attackB: string (nullable = true)
```
attack_udf라는 유저 정의함수를 선언한다.      
x가 normal.이면     
normal으로 아니면 attack으로 이름을 만들어주고      
StringType으로 반환한다.
함수를 돌린 후에는 attackB열이 추가 된 것을 확인할 수 있다.

# 다른식으로 유저 정의함수 만들기
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
def classify41(s):
    _5=""
    if s=="normal.":
        _5="normal"
    elif s=="back." or s=="land." or s=="neptune." or s=="pod." or s=="smurf." or s=="teardrop.":
        _5="dos"
    elif s=="ftp_write." or s=="guess_passwd." or s=="imap." or s=="multihop." or s=="phf." or\
        s=="spy." or s=="warezclient." or s=="warezmaster.":
        _5="r2l"
    elif s=="buffer_overflow." or s=="loadmodule." or s=="perl." or s=="rootkit.":
        _5="u2r"
    elif s=="ipsweep." or s=="nmap." or s=="portsweep." or s=="satan.":
        _5="probing"
    return _5

attack5_udf = udf(classify41, StringType())

myDf=myDf.withColumn("attack5", attack5_udf(_df.attack))

myDf.printSchema()

myDf.show(5)

>>

root
 |-- attack: string (nullable = true)
 |-- dst_bytes: long (nullable = true)
 |-- duration: long (nullable = true)
 |-- flag: string (nullable = true)
 |-- protocol: string (nullable = true)
 |-- service: string (nullable = true)
 |-- src_bytes: long (nullable = true)
 |-- attackB: string (nullable = true)
 |-- attack5: string (nullable = true)
 
 +-------+---------+--------+----+--------+-------+---------+-------+-------+
| attack|dst_bytes|duration|flag|protocol|service|src_bytes|attackB|attack5|
+-------+---------+--------+----+--------+-------+---------+-------+-------+
|normal.|     5450|       0|  SF|     tcp|   http|      181| normal| normal|
|normal.|      486|       0|  SF|     tcp|   http|      239| normal| normal|
|normal.|     1337|       0|  SF|     tcp|   http|      235| normal| normal|
|normal.|     1337|       0|  SF|     tcp|   http|      219| normal| normal|
|normal.|     2032|       0|  SF|     tcp|   http|      217| normal| normal|
+-------+---------+--------+----+--------+-------+---------+-------+-------+
only showing top 5 rows
```
파이썬에서 평범하게 함수를 정의한 후에         
그걸 udf로 활용한다.    

# DataFrame의 groupBy사용하기
```python
myDf.groupBy('attack5').count().show()

>>

+-------+------+
|attack5| count|
+-------+------+
|probing|  4107|
|    u2r|    52|
| normal| 97278|
|    r2l|  1126|
|    dos|391458|
+-------+------+
```

```python
myDf.groupBy("protocol").count().show()

>>

+--------+------+
|protocol| count|
+--------+------+
|     tcp|190065|
|     udp| 20354|
|    icmp|283602|
+--------+------+
```

```python
myDf.groupBy('attackB','protocol').count().show()

>>

+-------+--------+------+
|attackB|protocol| count|
+-------+--------+------+
| normal|     udp| 19177|
| normal|    icmp|  1288|
| normal|     tcp| 76813|
| attack|    icmp|282314|
| attack|     tcp|113252|
| attack|     udp|  1177|
+-------+--------+------+
```
두 종류로 나눠서 구분해준다.

# 행과 열로 따로 나눠서 GroupBy해보기
```python
myDf.groupBy('attackB').pivot('protocol').count().show()

>>

+-------+------+------+-----+
|attackB|  icmp|   tcp|  udp|
+-------+------+------+-----+
| normal|  1288| 76813|19177|
| attack|282314|113252| 1177|
+-------+------+------+-----+
```

```python
myDf.groupBy('attack5').pivot('protocol').avg('src_bytes').show()

>>

+-------+------------------+------------------+------------------+
|attack5|              icmp|               tcp|               udp|
+-------+------------------+------------------+------------------+
|probing|10.700793650793651| 261454.6003016591|25.235897435897435|
|    u2r|              null| 960.8979591836735|13.333333333333334|
| normal| 91.47049689440993|1439.3120305156679| 98.01220211711947|
|    r2l|              null|271972.57460035523|              null|
|    dos| 936.2672084368129| 1090.303422435458|              28.0|
+-------+------------------+------------------+------------------+
```
groupBy(행).pivot(열)의 구조로 계산해준다.

```python
myDf.groupBy('attack5').avg('duration').show()

>>

+-------+--------------------+
|attack5|       avg(duration)|
+-------+--------------------+
|probing|   485.0299488677867|
|    u2r|    80.9423076923077|
| normal|  216.65732231336992|
|    r2l|   559.7522202486679|
|    dos|7.254929008986916E-4|
+-------+--------------------+
```

```python
from pyspark.sql import functions as F
myDf.groupBy('attackB').pivot('protocol').agg(F.max('dst_bytes')).show()

>>

+-------+----+-------+---+
|attackB|icmp|    tcp|udp|
+-------+----+-------+---+
| normal|   0|5134218|516|
| attack|   0|5155468| 74|
+-------+----+-------+---+
```

```python
myDf.select("protocol", "duration", "dst_bytes")\
    .filter(_df.duration>1000)\
    .filter(_df.dst_bytes==0)\
    .groupBy("protocol")\
    .count()\
    .show()
    
>>

+--------+-----+
|protocol|count|
+--------+-----+
|     tcp|  139|
+--------+-----+
```

# Spark에서 SQL문 사용해보기
```python
_df.registerTempTable("_tab")

tcp_interactions = spark.sql(
"""
    SELECT duration, dst_bytes FROM _tab
    WHERE protocol = 'tcp' AND duration > 1000 AND dst_bytes = 0
""")

tcp_interactions.show(5)

>>

+--------+---------+
|duration|dst_bytes|
+--------+---------+
|    5057|        0|
|    5059|        0|
|    5051|        0|
|    5056|        0|
|    5051|        0|
+--------+---------+
only showing top 5 rows
```
_tab열을 임시 테이블로 만들고    
SQL문을 적용했다.

# rdd로 다시 변환하여 모양 이쁘게 만들어 요소로 집어넣기
```python
tcp_interactions_out = tcp_interactions.rdd\
    .map(lambda p: "Duration: {}, Dest. bytes: {}".format(p.duration, p.dst_bytes))
    
for i,ti_out in enumerate(tcp_interactions_out.collect()):
    if(i%10==0):
        print(ti_out)    

>>

Duration: 5057, Dest. bytes: 0
Duration: 5043, Dest. bytes: 0
Duration: 5046, Dest. bytes: 0
Duration: 5051, Dest. bytes: 0
Duration: 5057, Dest. bytes: 0
Duration: 5063, Dest. bytes: 0
Duration: 42448, Dest. bytes: 0
Duration: 40121, Dest. bytes: 0
Duration: 31709, Dest. bytes: 0
Duration: 30619, Dest. bytes: 0
Duration: 22616, Dest. bytes: 0
Duration: 21455, Dest. bytes: 0
Duration: 13998, Dest. bytes: 0
Duration: 12933, Dest. bytes: 0
```
