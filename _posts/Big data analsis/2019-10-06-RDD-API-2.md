---
layout: post
title: "RDD API 2"
tags: [Pair RDD, Word count, Stop word]
categories: [Big data analsis]
---

# Pair RDD 만들기
```
_testList=[("key1",1),("key1",1),("key1",1),("key2",1),("key2",1),
           ("key1",1),("key2",1),
           ("key1",1),("key1",1),("key2",1),("key2",1)]
# sparkContext는 RDD를 만들 때 사용한다.
_testRdd=spark.sparkContext.parallelize(_testList)
```

# collect해서 key 보기
```
_testRdd.keys().collect()

>
['key1',
 'key1',
 'key1',
 'key2',
 'key2',
 'key1',
 'key2',
 'key1',
 'key1',
 'key2',
 'key2']
```

# collect해서 value 보기
```
_testRdd.values().collect()

>
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

# reduceByKey 사용해보기
```
_testRdd.reduceByKey(lambda x,y:x+y).collect()

>
[('key1', 6), ('key2', 5)]
```

# groupByKey 사용해보기
```
_testRdd.groupByKey().collect()

>
[('key1', <pyspark.resultiterable.ResultIterable at 0x2ae958f34e0>),
 ('key2', <pyspark.resultiterable.ResultIterable at 0x2ae958f3ef0>)]
```
groupByKey는 값을 볼 수 없다.


# 단어 빈도 세보기
# RDD 생성
```
myRdd2=spark.sparkContext\
    .textFile(os.path.join("data","ds_spark_wiki.txt"))
```
텍스트 파일을 읽어왔다.

# flatMap과 GroupByKey
```
myRdd2\
    .flatMap(lambda x:x.split())\
    .map(lambda x:(x,1))\
    .groupByKey()\
    .take(3)
    
>
[('Wikipedia', <pyspark.resultiterable.ResultIterable at 0x2ae9432a0b8>),
 ('Apache', <pyspark.resultiterable.ResultIterable at 0x2ae9432a438>),
 ('Spark', <pyspark.resultiterable.ResultIterable at 0x2ae9432a2b0>)]
```
groupByKey는 RDD를 리턴한다.

# groupByKey한 값 보기
```
myRdd2\
    .flatMap(lambda x:x.split())\
    .map(lambda x:(x,1))\
    .groupByKey()\
    .mapValues(sum)\
    .take(20)

>
[('Wikipedia', 1),
 ('Apache', 6),
 ('Spark', 7),
 ('is', 1),
 ('an', 2),
 ('open', 1),
 ('source', 1),
 ('cluster', 1),
 ('computing', 1),
 ('framework.', 1),
 ('아파치', 5),
 ('스파크는', 1),
 ('오픈', 1),
 ('소스', 1),
 ('클러스터', 1),
 ('컴퓨팅', 1),
 ('프레임워크이다.', 1),
 ('스파크', 4),
 ('Originally', 1),
 ('developed', 1)]
```
# sortByKey 사용해보기
```
def f(x): return len(x)
myRdd2\
    .flatMap(lambda x:x.split())\
    .map(lambda x:(x,1))\
    .groupByKey()\
    .mapValues(f)\
    .sortByKey(True)\
    .take(10)

>
[('AMPLab,', 1),
 ('Apache', 6),
 ("Berkeley's", 1),
 ('California,', 1),
 ('Foundation,', 1),
 ('Originally', 1),
 ('Software', 1),
 ('Spark', 7),
 ('University', 1),
 ('Wikipedia', 1)]
```

# reduceByKey로 단어 빈도 세기
```
myRdd2\
    .flatMap(lambda x:x.split())\
    .map(lambda x:(x,1))\
    .reduceByKey(lambda x,y:x+y)\
    .take(10)

>
[('Wikipedia', 1),
 ('Apache', 6),
 ('Spark', 7),
 ('is', 1),
 ('an', 2),
 ('open', 1),
 ('source', 1),
 ('cluster', 1),
 ('computing', 1),
 ('framework.', 1)]
```
GroupByKey와 달리 따로 mapValues를 붙여주지 않아도 된다.

# countByKey로 단어 세보기
```
myRdd2\
    .flatMap(lambda x:x.split())\
    .map(lambda x:(x,1))\
    .countByKey() # .items() to be added to get a list
    
>
defaultdict(int,
            {'Wikipedia': 1,
             'Apache': 6,
             'Spark': 7,
             'is': 1,
             'an': 2,
             'open': 1,
             'source': 1,
             'cluster': 1,
             'computing': 1,
             'framework.': 1,
             '아파치': 5,
             '스파크는': 1,
             '오픈': 1,
             '소스': 1,
             '클러스터': 1,
             '컴퓨팅': 1,
             '프레임워크이다.': 1,
             '스파크': 4,
             'Originally': 1,
             'developed': 1,
             'at': 1,
             'the': 3,
             ...
```
countByKey는 딕셔너리를 반환한다.      
rdd와 groupByKey, reduceByKey는 rdd를 리턴한다.           

# textRdd 생성하기
```
txt=spark.sparkContext\
    .textFile(os.path.join("data","ds_bigdata_wiki.txt"))
print(type(txt))

>
<class 'pyspark.rdd.RDD'>
```

# 불용어 설정하기
```
stopwords = ['및', '제', '[편집]', '부터', '지난', '이하', '간', '수', '때']
wordCount = txt.flatMap(lambda x:x.split())\
                .filter(lambda x:x not in stopwords)
```
RDD에 공백이 있으면 일자로 펴주는 동시에 문장의 띄어쓰기마다 전부 떼어낸다.          
그리고 떼어낸 덩어리가 stopwords안에 있다면 빼고 wordCount에 넣어준다.       

# groupByKey를 사용해서 단어 빈도 세기
```
wordCount\
    .flatMap(lambda x:x.split())\
    .map(lambda x:(x,1))\
    .groupByKey()\
    .mapValues(sum)\
    .collect()

>
[('Big', 1),
 ('data', 1),
 ('활용사례', 1),
 ('의의[편집]', 1),
 ('정치', 1),
 ('사회[편집]', 1),
 ('2008년', 2),
 ('미국', 7),
 ('대통령', 3),
 ('선거[편집]', 1),
 ('선거에서', 1),
 ('버락', 1),
 ('오바마', 4),
 ('후보는', 1),
 ('다양한', 2),
 ('형태의', 1),
 ('유권자', 6),
 ...
```
groupByKey는 완전히 RDD를 리턴하기 때문에 mapValues를 사용해준다.

# CountByKey를 사용해서 단어 빈도 세기
```
wordCount\
    .flatMap(lambda x:x.split())\
    .map(lambda x:(x,1))\
    .countByKey()

>
defaultdict(int,
            {'Big': 1,
             'data': 1,
             '활용사례': 1,
             '의의[편집]': 1,
             '정치': 1,
             '사회[편집]': 1,
             '2008년': 2,
             '미국': 7,
             '대통령': 3,
             '선거[편집]': 1,
             '선거에서': 1,
             '버락': 1,
             '오바마': 4,
             '후보는': 1,
             '다양한': 2,
             '형태의': 1,
             '유권자': 6,
             ...
```
countByKey는 딕셔너리를 반환

# ReduceByKey를 사용하여 단어 빈도 세기
```
wordCount\
    .flatMap(lambda x:x.split())\
    .map(lambda x:(x,1))\
    .reduceByKey(lambda x,y:x+y)\
    .collect()
    
>
[('Big', 1),
 ('data', 1),
 ('활용사례', 1),
 ('의의[편집]', 1),
 ('정치', 1),
 ('사회[편집]', 1),
 ('2008년', 2),
 ('미국', 7),
 ('대통령', 3),
 ('선거[편집]', 1),
 ('선거에서', 1),
 ('버락', 1),
 ('오바마', 4),
 ('후보는', 1),
 ('다양한', 2),
 ('형태의', 1),
 ('유권자', 6),
 ('데이터베이스를', 2),
 ('확보하여', 1),
 ('이를', 2),
 ('분석,', 3),
 ('활용한', 5),
 ...
```
reduceByKey는 RDD를 반환하지만        
RDD내부는 리스트로 구성되어 있다.

# RDD생성하기
```
myRdd3=spark.sparkContext\
    .textFile(os.path.join("data","ds_bigdata_wiki.txt"))
```

# 불용어 설정하고, 단어빈도를 센후, 정렬하기
```
stopwords = [u'및',u'등', u'수', 'big','is','am','are','the','for','a', 'an','the']
wc3=myRdd3\
    .flatMap(lambda x:x.split(" "))\
    .filter(lambda x: x.lower() not in stopwords)\
    .map(lambda x:(x,1))\
    .reduceByKey(lambda x,y:x+y)\
    .map(lambda x:(x[1],x[0]))\
    .sortByKey(False)\
    .take(15)
    
for i in wc3:
    print(i[0], i[1])
    
>
23 
21 데이터
18 데이터를
14 빅
9 있다.
8 데이터의
7 미국
7 통해
6 유권자
6 선거
6 대한
6 빅데이터
5 활용한
5 소셜
5 대한민국
```
단어 빈도수로 정렬해주기 위해           
map(lambda x: (x[1], x[0])을 사용하여        
key와 map의 위치를 바꿔주고 sortByKey()를 사용했다.

# 그래프 그리기
```
%matplotlib inline
import matplotlib.pyplot as plt

# 단어 만들기
count = list(map(lambda x: x[0], wc3))
# 빈도 만들기
word = list(map(lambda x: x[1], wc3))
# 
plt.barh(range(len(count)), count, color = 'grey')
plt.yticks(range(len(count)), word)
plt.show()
```
![image](https://user-images.githubusercontent.com/50114210/66257537-7c3e0380-e7d5-11e9-8315-6018ada7c0e1.png)             

# CombineByKey 사용해보기
```
marks = spark.sparkContext.parallelize([('kim',86),('lim',87),('kim',75),
                                      ('kim',91),('lim',78),('lim',92),
                                      ('lim',79),('lee',99)])
marksByKey = marks.combineByKey(lambda value: (value,1),
                             lambda x,value: (x[0]+value, x[1]+1),
                             lambda x,y: (x[0]+y[0], x[1]+y[1]))
marksByKey.collect()
```
combineByKey는 lambda함수 세개를 인자로 받는다.        

# 사람의 점수 합계를 combineByKey로 구해보기
```
marks = spark.sparkContext.parallelize([('kim',86),('lim',87),('kim',75),
                                      ('kim',91),('lim',78),('lim',92),
                                      ('lim',79),('lee',99)])
marksByKey = marks.combineByKey(lambda value: (value,1),
                             lambda x,value: (x[0]+value, x[1]+1),
                             lambda x,y: (x[0]+y[0], x[1]+y[1]))
marksByKey.collect()

>
[('kim', (252, 3)), ('lim', (336, 4)), ('lee', (99, 1))]
```

# 남자 여자의 키 평균을 combineByKey로 구해보기
```
heights = spark.sparkContext.parallelize([
        ('M',182.),('F',164.),('M',180.),('M',185.),('M',171.),('F',162.)
    ])
heightsByKey = heights.combineByKey(lambda value: (value,1),
                             lambda x,value: (x[0]+value, x[1]+1),
                             lambda x,y: (x[0]+y[0], x[1]+y[1]))
heightsByKey.collect()

>
[('M', (718.0, 4)), ('F', (326.0, 2))]
```

이 코드 후에 뒤의 코드를 사용하면 되는데, 나는 에러뜬다.         

```
avgByKey = heightsByKey.map(lambda (label,(valSum,count)):
                                (label,valSum/count))

print avgByKey.collectAsMap()
```

# 문제
1. 이름별로 합계 구하기
2. 과목별로 합계 구하기
3. 이름별로 합계과 빈도수 구하기
4. 이름별로 평균 구학.

# 1. 이름별로 합계 구하기
```
nameSum = testRdd\
    .map(lambda x:(x[0], x[2]))\
    .reduceByKey(lambda x, y: x + y)\
    .map(lambda x: (x[1], x[0]))\
    .sortByKey(True)\
    .map(lambda x: (x[1], x[0]))\
    .collect()

for i in nameSum:
    print('\''+i[0]+'\'', i[1])

>
'임하나' 170
'김하나' 180
'김갑돌' 180.8
```

# 과목별로 합계 구하기
```
subSum = testRdd\
    .map(lambda x:(x[1], x[2]))\
    .reduceByKey(lambda x, y: x + y)\
    .collect()
for i in subSum:
    print('\''+i[0]+'\'', i[1])
    
>
'English' 252.3
'Math' 278.5
```

# 이름별로 합계과 빈도 수 구하기
```
nameSumCount = testRdd.map(lambda x: (x[0], x[2]))
nameSumCount = nameSumCount.combineByKey(lambda value: (value,1),
                             lambda x,value: (x[0]+value, x[1]+1),
                             lambda x,y: (x[0]+y[0], x[1]+y[1]))
result = nameSumCount.collect()
result.sort(key=lambda x: x[1][0])
for i in result:
    print('\''+i[0]+'\'', i[1])

>
'임하나' (170, 2)
'김하나' (180, 2)
'김갑돌' (180.8, 2)
```

# 이름별로 평균 구하기
```
nameSumCount = nameSumCount.map(lambda x : (x[0], x[1][0]/x[1][1]))
result2.sort(key=lambda x:x[1])
for i in result2:
    print('\''+i[0]+'\'', i[1])

>
'임하나' 85.0
'김하나' 90.0
'김갑돌' 90.4
```























