---
layout: post
title: "배치 정규화"
tags: [Batch Normalization]
categories: [Improving deep neural networks]
---

# 학습 목표
배치 정규화를 학습한다.

# 핵심 키워드
배치 정규화(Batch Normalization)

# 학습 내용
* 배치 정규화의 장점은 하이퍼파라미터 탐색을 쉽게 만들어줄 뿐만 아니라, 신경망과 하이퍼파라미터의 상관관계를 줄여줍니다.
* 보통 활성화 함수 이전에 사용되며, 작동원리는 같습니다.
![image](https://user-images.githubusercontent.com/50114210/65894123-2daef480-e3e4-11e9-9698-9256ce4c53db.png)      
* r과 b는 학습 과정에서 학습하는 파라미터입니다. 정규화 이후 다시 선형변환하는 이유는 항상 같은 분포 값을 갖지 않게 하기 위함입니다.

# 인트로
딥러닝이 떠오르면서 가장 중요한 아이디어 중 하나로 배치 정규화라는 알고리즘이 꼽힙니다.            
Sergey Ioffe와 Christian Szegedy가 만들었고요.        
배치 정규화는 하이퍼파라미터 탐색을 쉽게 만들어줄 뿐만 아니라     
신경망과 하이퍼파라미터의 상관관계를 줄여주죠.          
즉 더 많은 하이퍼파라미터가 잘 작동하는 겁니다.         
아주 깊은 심층신경망이라도 쉽게 학습할 수 있도록 도와줍니다.     
어떻게 배치 정규화가 작동하는지 보겠습니다.        

# 정규화 식
![image](https://user-images.githubusercontent.com/50114210/65895175-fccfbf00-e3e5-11e9-8fbd-5cd98a06a991.png)   
여러분이 로지스틱 회귀 등으로 모델을 학습시킬 때        
입력 변수들을 정규화하면 학습이 빨라졌던 것 기억하시나요?        
평균을 계산할 때는 입력 변수의 평균을 뺐고 분산을 계산할 때는 x(i)^2를 사용하죠?       
하나씩 제곱해준다는 것이고요. 이 값을 이용해 정규화해줍니다.           

# 학습 등고선의 변화
![image](https://user-images.githubusercontent.com/50114210/65895195-09ecae00-e3e6-11e9-9483-be1d2e6c9a37.png)     
앞선 영상에서 이 방법이 어떻게 누워있는 학습 등고선을      
경사하강법에 적합토록 더 둥근 형태로 바꾸는지 봤습니다.         
즉 로지스틱 회귀 등 신경망의 입력 변수들을 정규화하면 저렇게 바뀌는 것이죠.         

# 신경망에 적용
![image](https://user-images.githubusercontent.com/50114210/65895239-1a9d2400-e3e6-11e9-85e8-c0f786070cc1.png)     
심층 신경망은 어떨까요? 여기서는 입력 변수 x뿐 아니라      
이 층의 활성값 a^[1] 여기는 활성값 a^[2] 같은 것이 있죠.         
만약 w^[3], b^[3]라는 파라미터를 학습시킨다면      
a^[2]의 평균과 분산을 정규화하는 것이 더 효율적이지 않을까요?       
로지스틱 회귀의 경우 x_1, x_2, x_3를 정규화하는 것이 어떻게        
w, b 학습을 효율적으로 하는지 봤습니다.         

# a를 정규화하냐 z를 정규화하냐
![image](https://user-images.githubusercontent.com/50114210/65895330-43bdb480-e3e6-11e9-9c98-a6065c55379f.png)      
여기서 질문은 은닉층에 대해 w^[3]나 b^[3]를 빠르게 학습시킬 수 있도록        
이 예시의 a^[2]같은 값을 정규화할 수 있냐는 거죠.         
왜냐하면 다음 층 입력값인 a^[2]가 w^[3]와 b^[3] 학습에 영향을 주니까요.         
이 질문이 배치 정규화가 하는 일을 나타내고 있습니다.           
사실 a^[2]가 아니라 z^[2]를 정규화하는 것이죠.         
사실 논쟁이 있습니다. 활성 함수 이전의 값인 z^[2]를 정규화할 건지       
활성 함수 이후의 값인 a^[2]를 정규화할건지 사이에서 말이죠.         
실제로는 z^[2]를 정규화하는 것이 더 자주 쓰입니다.       
이 방법을 계속 쓸거고 여러분께도 추천드립니다.        

# 배치 정규화의 구현
![image](https://user-images.githubusercontent.com/50114210/65895515-913a2180-e3e6-11e9-8aeb-cedd865583d9.png)        
이제 어떻게 배치 정규화를 구현하는지 볼까요?       
신경망에서 사잇값들이 주어졌다고 할 때        
은닉 유닛의 값 z^(1)부터 z^(m)까지 있다고 합시다.     
이 값들은 은닉 유닛의 값들입니다.     
더 정확하게 쓰자면 z^[l](i)라고 써야겠죠.    
i는 1부터 m까지고요. 하지만 간편하게 쓰기 위해서 [l]을 생략합니다.        
이 값들에 대해서 우리는 평균을 이렇게 계산하고요.    
다시 말씀드리지만 어떤 층 l에 관한 내용입니다. [l]을 생략했어요.      
그리고 여러분이 아시는 것처럼 분산을 계산합니다.       
그리고 각 z^(i)에 대해서 정규화를 하여 z^(i)_norm을 얻습니다.        
평균을 뺀 뒤에 표준편차로 나누면 되겠죠.     
수학적 안정성을 위해서 분모에 ε을 추가합니다.       
혹시 표준편차가 0인 경우를 대비해서요.       
이렇게 z 값에 대해서 정규화를 거쳐 모든 z들이 평균이 0이고         
표준편차가 1이 되도록 만들었습니다.          

# 학습되는 변수인 y와 b
![image](https://user-images.githubusercontent.com/50114210/65895570-a31bc480-e3e6-11e9-86b4-f351007b789e.png)   
하지만 은닉 유닛가 항상 평균 0 표준편차 1을 갖는 것이 좋지만은 않습니다.       
말은 될 지 모르지만 은닉 유닛는 다양한 분포를 가져야하니까요.       
그래서 대신 z~를 계산하는데요. 이건 γ*z^(i)_norm + β와 같고요.        
여기서 γ와 β는 모델에서 학습시킬 수 있는 변수입니다.        
즉 여기에서 경사하강법을 이용해서 아니면 모멘텀, RMSprop, Adam을     
이용한 경사하강법 등 다양한 알고리즘을 이용해서 γ와 β를 학습시킬 수 있습니다.        
신경망에서 계수들을 찾았던 것처럼 말이죠.        

# 다양하게 만들 수 있는 y와 b
![image](https://user-images.githubusercontent.com/50114210/65895621-b595fe00-e3e6-11e9-9925-38a246ff3cbe.png)       
γ와 β를 이용하면 z-의 평균을 원하는 대로 설정할 수 있습니다.         
예를 들어 γ=√σ²+ε로써 분모와 같고 β가 μ와 같다면 여기 있는 것처럼 말이죠.    
그러면  γ*z^(i)_norm + β는 이 식을 거꾸로 뒤집은 것과 같은 효과를 낼 겁니다.    
만약 이 식이 성립한다면 z-^(i)는 z^(i)와 같겠죠.        
이렇게 적절히 γ와 β를 설정해서 이 네 개의 식으로 이뤄진       
정규화 과정은 항등함수를 만드는 것과 똑같은 효과를 낸 것이지요.         
하지만 다른 γ와 β 값을 정한다면 은닉 유닛의 값들이 서로 다른
평균이나 분산 값을 만들게 할 수 있습니다.       

# 신경망에 적용
![image](https://user-images.githubusercontent.com/50114210/65895662-c2b2ed00-e3e6-11e9-9e0f-ac2c68a85c4f.png)   
이제 이걸 신경망에 적용시켜 볼까요?       
이전에는 여기 z^(1) 등의 값을 썼다면      
이제는 z~^(i)를 z^(i) 대신 신경망에 사용하는 겁니다.   
그리고 어떤 층인지 알 수 있도록 다시 [l]을 써넣을 수 있겠죠.    
여기서 여러분이 얻었으면 하는 직관은 우리가 X를 정규화하는 것이      
신경망 학습을 도울 수 있는 것을 봤듯이 배치 정규화는 입력층에만         
정규화를 하는 것이 아니라 신경망 안 깊이 있는 은닉층의       
값들까지도 정규화하는 것입니다.       
이런 정규화를 사용해서 은닉 유닛 z의 평균과 분산을 정규화하는 거죠.       

# 다양한 평균과 분포를 갖기를 바람
![image](https://user-images.githubusercontent.com/50114210/65895691-d2cacc80-e3e6-11e9-8863-d4deae1089a9.png)  
하지만 입력층와 은닉 유닛를 학습시킬 때 차이점은      
은닉 유닛 값의 평균과 분산이 0, 1로 고정되기를 원치 않는다는 것입니다.       
예를 들어 시그모이드 활성 함수가 있을 때        
값들이 이쪽에 모여있는 것을 원치는 않죠.      
더 넓은 영역에 걸쳐 퍼져있거나 시그모이드의 비선형성을 살릴 수 있도록       
평균이 0이 아닌 다른 값을 갖게 하는 것이 좋습니다. 한 구간에 뭉쳐있는 것 보다는요.        
이것이 γ와 β를 이용하여 원하는 범위의 값을 만들어내는 이유입니다.       
즉 은닉 유닛가 표준화된 평균과 분산을 갖되        
평균과 분산은 학습 알고리즘에서 설정할 수 있는       
두 변수 γ와 β에 의해 조절되는 것입니다.        
다시 말하자면 은닉 유닛 값 z^(i)의 평균과 표준편차를        
특정한 평균과 분산을 갖도록 정규화하는 것입니다.       
이것은 0이나 1이 될 수도 있고 또다른 값이 될 수도 있죠.       
γ와 β의 조절에 따라서 결정됩니다.       

# 아웃트로
이 글을 통해서 배치 정규화가 적어도 신경망의 한 층에 대해서      
어떻게 작동하고 구현되는지 감을 잡으셨으면 좋겠습니다.        
다음 글에서는 심층 신경망에서 배치 정규화를 사용하는 방법과        
신경망의 서로 다른 층에 대해서 사용하는 방법을 배워볼 겁니다.        
그리고나서 왜 배치 정규화가 신경망 학습에  도움이 되는지 더 살펴보도록 하죠.         
왜 이걸 하는지 아직 아리송한 분들은 계속 영상을 보시면        
두 개 글 안에 명확히 아실 수 있을 겁니다.       
