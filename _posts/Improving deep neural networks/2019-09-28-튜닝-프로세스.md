---
layout: post
title: "튜닝 프로세스"
tags: [Hyperparameter, Tuning, Learning rate]
categories: [Improving deep neural networks]
---

# 학습 목표
하이퍼파라미터의 튜닝 과정을 배운다.

# 핵심 키워드
* 하이퍼파라미터(hyperparameter)
* 튜닝(tuning)
* 학습률(learning rate)

# 학습 내용
* 딥러닝에는 다양한 하이퍼파라미터가 존재합니다. 상황에 따라 다를 수도 있지만, 보통 우선 조정하는 순서로 나열했습니다
  - 학습률
  - 모멘텀 알고리즘의 베타
  - 은닉 유닛의 수
  - 미니배치 크기
  - 학습률 감쇠 정도
  - 아담 알고리즘의 베타1, 베타2, 입실론
* 딥러닝의 하이퍼파라미터 탐색은 무작위 접근 방식이 좋습니다. 이유는 어떤 하이퍼파라미터가 문제 해결에 더 중요한지 미리 알 수 없기 때문입니다.
* 다른 일반적 접근 방식 중 하나는 "정밀화 접근"입니다. 우선 전체 하이퍼파라미터 공간에서 탐색하여 좋은 점을 참은 후, 그 근방에서 더 정밀하게 탐색하는 과정입니다.

# 인트로
지금까지 신경망을 학습시킬 때 여러 하이퍼파라미터들이 관여한다는 걸 배웠는데요.              
그럼 좋은 하이퍼파라미터는 어떻게 찾을 수 있을까요?          
이번 시간에는 체계적으로 하이퍼파라미터를 튜닝할 수 있는 팁을 알려드리겠습니다.             
이 강의를 들으면 좋은 하이퍼파라미터를 효율적으로 정할 수 있을 겁니다.             

# 다양한 하이퍼파라미터들
![image](https://user-images.githubusercontent.com/50114210/65817794-5ac5a080-e246-11e9-920c-731409016118.png)          
심층신경망을 학습시킬 때 가장 어려운 일은 다뤄야 할 하이퍼파라미터가 많다는 것입니다.              
학습률 α나 모멘텀을 나타내는 β,            
Adam 최적화 알고리즘의 하이퍼파라미터인 β1, β2도 있고, ε도 있습니다.                
층의 수도 정해야겠고요. 층에서 은닉 유닛의 숫자를 정할 수도 있습니다.               
학습률 α를 그냥 쓰지 않고 학습률 감쇠를 같이 쓸 수 있겠죠?             
또 미니 배치의 크기를 정할 수도 있습니다.           

# 하이퍼파라미터마다 중요도가 다르다
![image](https://user-images.githubusercontent.com/50114210/65817771-2d78f280-e246-11e9-9eb5-1432dda356e7.png)          
그리고 대부분의 학습에서는 일부 파라미터들이 다른 파라미터보다 중요합니다.            
우선 학습률 α는 튜닝해야 할 가장 중요한 하이퍼파라미터이고요.         
α 이외에 주로 튜닝하는 것들로는 모멘텀이 있겠고요.         
기본값 0.9 정도로 설정할 수 있겠고요 최적화 알고리즘을 효율적으로 돌리기 위해           
미니 배치 크기도 튜닝할 수 있습니다. 은닉 유닛도 자주 튜닝합니다.           
여기 오렌지색 박스는 학습률 α 다음으로 중요한 것들이고요.             
그 다음으로 중요한 것들을 보면요.          
층의 숫자도 큰 차이를 만들 수 있고 학습률 감쇠도 마찬가지입니다.           
그리고 Adam 알고리즘에서는 β1, β2, ε는 튜닝을 하지 않고 0.9, 0.999, 10^(-8)을 항상 사용합니다.                 
물론 원한다면 튜닝을 해도 좋습니다.            
이렇게 무엇이 무엇보다 중요한지에 대해 대략적인 감을 잡아보았습니다.            
당연히 α가 가장 중요하고요. 다음으로 오렌지색 박스         
그 뒤를 이어 보라색 박스 순입니다. 그러나 딱 정해진 것은 아닙니다.          
어떤 전문가들은 동의하지 않거나 다른 직관을 갖고 있을 수도 있지요.           

# 격자탐색
![image](https://user-images.githubusercontent.com/50114210/65817803-7cbf2300-e246-11e9-8f6e-059d325d8da5.png)             
만약 하이퍼파라미터를 튜닝한다면 어떤 값을 탐색할지 어떻게 정할 수 있을까요?            
머신러닝이 만들어진지 얼마 되지 않았을 때는 두 개의 하이퍼파라미터가 있을 때          
각각 Hyperparameter1 Hyperparameter2라고 부를 건데요.           
격자점을 탐색하는 것이 일반적이었습니다. 이렇게 말이죠.               
그리고 체계적으로 여기 있는 값들을 탐색하는 것이지요.              
실제로는 더 크거나 작을 수도 있겠지만 여기서는 5X5 격자에서 25개의 점만 생각하죠.                  
그리고 최고의 하이퍼파라미터를 정하는 겁니다.                  
이 예시는 하이퍼파라미터의 수가 적을 때 쓸 수 있습니다.             
하지만 딥러닝에서는 이런 방식을 추천드립니다.             
무작위로 점들을 선택하는 것이지요. 동일하게 25개의 점만 생각해볼까요?                
그 점들에 대해서 하이퍼파라미터를 정하는 겁니다.            
이렇게 하는 이유는 어떤 하이퍼파라미터가 문제 해결에         
더 중요한지 미리 알 수 없기 때문입니다.             
이전 슬라이드에서 보신 것처럼 하이퍼파라미터의 중요도 순위가 있습니다.          
예를 들어 Hyperparameter1이 학습속도 α이고 극단적인 경우로             
Hyperparameter2를 ε라고 합시다. Adam 알고리즘에서 분모에 있는 값 말이죠.            
이런 경우 α를 고르는 것이 ε을 고르는 것보다 더 중요합니다.              
그래서 격자점으로 다시 돌아가면 5개의 α 값을 확인하게 되는데,             
이때 ε 값이 달라도 결과는 같은 것을 확인할 수 있을 겁니다.             
25개의 모델을 학습시켰지만 가장 중요한 하이퍼파라미터인          
α 5개에 대해서만 학습시킨 것과 다를 게 없죠.                  

# 랜덤탐색
![image](https://user-images.githubusercontent.com/50114210/65817811-8f395c80-e246-11e9-904d-252c3b2b9183.png)           
반대로 무작위로 모델을 고르면 어떨까요?             
그러면 아시다시피 25개의 서로 다른 학습속도 α 값을 이용하여 학습시키게 되고        
더 좋은 하이퍼파라미터를 잘 찾게 될 겁니다.               
여기서는 두 개의 하이퍼파라미터만 써서 예를 들었는데요.                
실제로는 훨씬 많은 하이퍼파라미터를 다루게 될 겁니다.            

# 여러개의 하이퍼파라미터 탐색
![image](https://user-images.githubusercontent.com/50114210/65817831-bdb73780-e246-11e9-851d-c9267b5074c2.png)            
예컨대 세 개의 하이퍼파라미터를 다룬다고 하면               
정사각형을 탐색하는 것이 아니라 정육면체를 탐색하는 것이죠.           
그리고 세 번째 차원은 Hyperparameter3을 가리킬 거고요.              
이 3차원 정육면체 안에서 모델을 고른다면            
각 하이퍼파라미터에 대해서 훨씬 많은 값을 시험해보게 될 겁니다.            
실제로는 3보다 더 많은 하이퍼파라미터를 탐색하곤 합니다.               
그리고 어플리케이션에서 어떤 하이퍼파라미터가              
가장 중요한지 미리 알기 어렵습니다.                
그리고 격자점보다 무작위로 모델을 정하는 것이 
가장 중요한 하이퍼파라미터의 다양한 값을 탐색할 수 있습니다.                
무엇이 중요하건 상관없이 말이죠.              

# 정밀화 접근
![image](https://user-images.githubusercontent.com/50114210/65817846-e2abaa80-e246-11e9-9539-3c3adc7c4470.png)          
다른 일반적 방법 중 하나는 정밀화 접근입니다.           
여기 2차원 예시에서 이 점들을 사용한다고 해보죠.            
그리고 이 점이 최고라는 걸 찾았습니다.             
아마 그 주변에 있는 점들도 좋은 성능을 보이겠죠?             
그러면 정밀화 접근에서는 이렇게 더 작은 영역으로 확대해서             
더 조밀하게 점들을 선택합니다.            
무작위인 것은 그대로이지만 최고의 하이퍼파라미터들이 이 영역에              
있으리라는 믿음 하에 파란색 사각형 안에 초점을 두고 탐색하는 것이죠.            
즉 전체 사각형에서 탐색한 뒤에 더 작은 사각형으로 범위를 좁혀나가는 겁니다.             
그러면 여기에서 더 조밀하게 시험해볼 수 있겠죠.               
이런 정밀화 접근도 자주 쓰이는 방식입니다.          
그리고 이렇게 하이퍼파라미터의 여러 값들을 시험해보며          
학습의 목표나 개발 목표 등에 있어서 최고의 파라미터를 고르는 것이죠.               
그렇게 최적의 하이퍼파라미터를 탐색하는 것입니다.          

# 아웃트로
이번 강의에서는 하이퍼파라미터를 찾는 방법을 체계적으로 정리했습니다.            
여러분이 반드시 알아두셔야 할 두 가지는          
첫째, 격자점이 아니라 무작위이다.             
둘째, 원한다면 정밀화 접근을 이용할 수 있다. 입니다.       
이것보다 더 많은 종류의 탐색이 있습니다.            

