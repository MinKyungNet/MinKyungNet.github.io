---
layout: post
title: "Momentum 최적화 알고리즘"
tags: [Momentm, Optimization]
categories: [Improving deep neural networks]
---

# 학습 목표
Momentum 최적화 알고리즘을 배운다.

# 핵심 키워드
* Momentum
* 최적화(Optimization)

# 학습 내용
* 알고리즘은 아래와 같습니다.
  -

# 인트로
모멘텀 알고리즘 혹은 모멘텀이 있는

경사 하강법은 일반적인 경사 하강법보다

거의 항상 더 빠르게 동작합니다

기본적인 아이디어는

경사에 대한 지수가중평균을 계산하는 것입니다

그 값으로 가중치를 업데이트합니다

이 비디오에서는 한 문장의 설명을 풀어서

어떻게 구현할 수 있을지 알아봅시다

대부분의 예제에서 비용함수를

최적화한다고 가정해봅시다

등고선은 다음과 같습니다

빨간 점은 최솟값의 위치를 나타냅니다

경사 하강법을 여기서 시작해서

경사 하강법 혹은 미니배치 경사 하강법의

한 반복을 취하면 이 쪽으로 향합니다

타원의 반대쪽에서 경사 하강법의

한 단계를 취하면

이런 식으로 오게 됩니다

계속 한 단계씩 갈 때마다

이런 식으로 나아갑니다

많은 단계를 취하면

최솟값으로 나아가면서 천천히 진동합니다

위 아래로 일어나는 이런 진동은

경사 하강법의 속도를 느리게 하고

더 큰 학습률을 사용하는 것을 막습니다

왜냐하면 오버슈팅하게 되어

이런 식으로 발산할 수도 있기 때문입니다

따라서 학습률이 너무 크지 않아야

진동이 커지는 것을 막을 수 있습니다

이 문제를 보는 또 다른 관점은

수직축에서는 진동을 막기 위해

학습이 더 느리게 일어나기를 바라지만

수평축에서는 더 빠른

학습을 원합니다

최솟값을 향해 왼쪽에서 오른쪽으로

이동하는 것을 처리하고 싶기 때문입니다

따라서 모멘텀을 이용한 경사 하강법에서는

구현할 때에는 다음과 같이 합니다

각각의 반복에서 더 정확히 말해서

반복 t에서

보편적인 도함수인 dw와 db를

계산하게 될 것입니다

보편적인 도함수인 dw와 db를

계산하게 될 것입니다

위 첨자로 괄호를 표시하는 것은

생략하겠습니다

그러나 현재의 미니배치에 대한

dw와 db를 계산하게 됩니다

배치 경사 하강법을 사용하는 경우

현재의 미니배치는 전체 배치와 같습니다

현재의 미니배치가 전체 훈련 세트와

같은 경우에도 잘 작동하게 됩니다

그 다음에 하는 것은 V_dw는

βV_dw + (1-β)dw를 계산합니다

이것은 전에 계산했던

V_θ = β*V_θ + (1-β)θ_t 와

비슷합니다

이동평균을 w에 대한

도함수로 계산합니다

v_db도 비슷한 방식으로

βv_db + (1-β)db를 계산합니다

그럼 w를 사용해

가중치를 업데이트합니다

w는 w에서 학습률을 곱하기 v_dw를 뺀

값으로 업데이트됩니다

w는 w에서 학습률을 곱하기 v_dw를 뺀

값으로 업데이트됩니다

b도 비슷하게

b - α*v_db로 업데이트됩니다

b - α*v_db로 업데이트됩니다

이것은 경사 하강법의 단계를

부드럽게 만들어줍니다

지난번에 계산한

몇 가지 도함수가

이런 식이라고 한다면

이 경사의 평균을 구하면

수직 방향의 진동이 0에 가까운

값으로 평균이 만들어집니다

진행을 늦추고 싶은

수직 방향에서는

양수와 음수를 평균하기 때문에

평균이 0이 됩니다

반면에 수평 방향에서 모든 도함수는

오른쪽을 가리키고 있기 때문에

수평 방향의 평균은

꽤 큰 값을 가집니다

따라서 몇 번의 반복이 있는

이 알고리즘에서

경사 하강법은 결국에

수직 방향에서는

훨씬 더 작은 진동이 있고

수평 방향에서는 더 빠르게

움직인다는 것을 찾을 수 있을 겁니다

따라서 이 알고리즘은 더 직선의 길을

가거나 진동을 줄일 수 있게 합니다

이 모멘텀에서 얻을 수 있는 직관은

밥그릇 모양의 함수를

최소화하려고 하면

밥그릇 모양의 함수를

최소화하려고 하면

그럼 이 도함수의 항들은

아래로 내려갈 때

가속을 제공한다고 볼 수 있습니다

그리고 이 모멘텀 항들은

속도를 나타낸다고 볼 수 있습니다

따라서 작은 공이 이 그릇의 경사를 내려갈 때

도함수는 여기에 가속을 부여하고

따라서 작은 공이 이 그릇의 경사를 내려갈 때

도함수는 여기에 가속을 부여하고

더 빠르게 내려가게 만듭니다

가속을 주기 때문입니다

그리고 β의 값은 1보다

조금 작기 때문에

마찰을 제공해서 공이 제한 없이

빨라지는 것을 막습니다

따라서 경사 하강법이 모든 이전 단계를

독립접으로 취하는 대신에

그릇을 내려가는 공에

가속을 주고 모멘텀을 제공할 수 있습니다

그릇을 굴러가는 공에 대한 비유는

물리적인 직관을 좋아하는

사람들에게 잘 통한다고 생각합니다

그러나 그릇을 내려가는 공에 대한 비유가

이해하기 어렵다면 걱정 마세요

이제 어떻게 구현할지에 대한

세부 사항을 살펴봅시다

여기 학습률 α와 지수가중평균을 제어하는

β라는 두 가지 하이퍼파라미터가 있습니다

β의 가장 일반적인 값은 0.9입니다

지난 10일 간의 온도를 평균하는 것이죠

실제로 β가 0.9인 경우

매우 잘 작동합니다

다양한 값을 시도하면서

하이퍼파라미터를 탐색해보세요

그러나 0.9가 꽤 견고한

값을 제공합니다

편향 보정은 어떨까요?

v_dw를 (1-β^t)로 나눠줍니다

많은 사람들이 잘 사용하지 않습니다

그 이유는 10번의 반복 뒤에

이동평균이 충분히 진행 돼서

편향 추정이 더 이상 일어나지 않기 때문입니다

따라서 경사 하강법이나 모멘텀을 구현할 때

편향 보정을 하는 사람들은 거의 없습니다

v_dw를 0으로 초기화하는 과정은

dw와 같은 차원의 0으로

이루어진 행렬입니다

w와도 같은 차원입니다

w와도 같은 차원입니다

v_db도 0에 대한 벡터로

초기화됩니다

db와 b와 같은 차원입니다

모멘텀이 있는 경사 하강법에 대한

논문을 읽어보면

(1-β)에 관한 항이 자주

삭제되어 있는 것을 발견할 것입니다

따라서

따라서 v_dw = β*v_dw + dw입니다

따라서 v_dw = β*v_dw + dw입니다

따라서 v_dw = β*v_dw + dw입니다

보라색으로 표시한 버전의 효과는

v_dw가 1/(1-β)에 대한 계수로

스케일링 되는 것입니다

따라서 경사 하강법의 업데이트를

실행할 때는

α가 1/(1-β)에 대응되는 값으로

바뀔 필요가 있습니다

실제로는 두 가지 모두

잘 작동할 것입니다

학습률 α에 대한 가장 최적의 값에만

영향을 미치게 됩니다

그러나 저는 이 특정한 식이

덜 직관적이라고 생각합니다

이것의 한 가지 효과는 하이퍼파라미터

β의 값을 보정함으로써

v_dw와 v_db의 스케일링에 영향을

주게 되고 학습률도 다시 보정해야 합니다

따라서 제가 왼쪽에 쓴 수식을

개인적으로 더 선호합니다

1-β에 관한 항이 살아있는 수식말입니다

그러나 β를 0.9로 하는 것은 두 설정 모두

하이퍼파라미터의 일반적인 선택입니다

학습률 α가 다르게 보정된다는 것이

이 두 버전의 차이입니다

따라서 모멘텀이 있는

경사 하강법이었습니다

모멘텀이 없는 경사 하강법보다

거의 항상 더 잘 작동합니다

그러나 학습 알고리즘을 빠르게 하기 위한

또 다른 방법이 남아 있습니다

다음 비디오에서 함께 살펴보시죠

