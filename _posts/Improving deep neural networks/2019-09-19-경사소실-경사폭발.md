---
layout: post
title: "경사소실, 경사폭발"
tags: [Vanishing gradients, Exploding gradients]
categories: [Improving deep neural networks]
---

# 학습 목표
경사의 소실 및 폭발 문제를 알 수 있다.

# 핵심 키워드
* 경사의 소실(Vanishing gradients)
* 경사의 폭발(Exploding gradients)

# 학습 내용
* 매우 깊은 신경망을 훈련시킬 때 나타나는 문제점은 경사의 소실과 폭발입니다.
* 예를 들어 g(z) = z, b[L] = 0이라고 가정했을 때 Y = W[L] * W[L-1] * ... * W[2] * W[1] * X가 됩니다. 이 때 모든 가중치 행렬이 W = 1.5E 라고 가정하면, Y^ = 1.5^(L-1)EX가 되고 더 깊은 신경망일수록 Y^의 값은 기하급수적으로 커집니다. 반대로 모든 가중치 행렬 W = 0.5E라고 가정하면 Y^ = 0.5^(L-1)EX가 되고 더 깊은 신경망일수록 Y^의 값은 기하습수적으로 감소합니다. 이를 토대로 생각하면 경사 하강법에서 W의 값이 단위행렬보다 큰 값이라면 경사의 폭발, W의 값이 단위행렬보다 작은 값이라면 경사의 소실 문제점이 생깁니다.
* 경사의 소실과 폭발로 인해 학습 시키는데 많은 시간이 걸리기에 가중치 초기화 값을 신중하게 해야합니다.

# 인트로
신경망을 훈련시키는 것, 특히 매우 깊은 신경망을 훈련시키는 것의 문제는 경사의 소시로가 폭발입니다.    
매우 깊은 신경망을 훈련시킬 때, 미분값 혹은 기울기가 아주 작아지거나 커질 수 있습니다.      
이 글에서는 경사 폭발과 소실의 문제점이 무엇인지 알아보고 그리고 무작위의 가중치 초기화에 대한    
신중한 선택으로 이 문제를 상당히 줄이는 방법도 함께 다뤄보겠습니다.

# 깊은 신경망
![image](https://user-images.githubusercontent.com/50114210/65215332-28020680-dae8-11e9-86f0-2bafed3b1b48.png)
이 그림처럼 매우 깊은 신경망을 훈련시키는 경우를 생각해봅시다.     
위의 그림은 두개의 은닉 유닛만을 가지고있지만 더 많을 수도 있습니다.   
이 신경망은 매개변수 w[1] ,w[2], w[3] ... , w[L]까지 갖습니다.   

### 단순하게 생각해보자
![image](https://user-images.githubusercontent.com/50114210/65215476-9f379a80-dae8-11e9-8196-99b8cea19851.png)
간단함을 위해 활성화 함수 g(z)가 선형 활성화 함수를 사용한다고 해봅시다.   
그리고 b[L]은 0이라고 가정해봅시다.   

### 출력값
![image](https://user-images.githubusercontent.com/50114210/65215537-d27a2980-dae8-11e9-9b19-13377e02bc95.png)   
따라서 이경우에 출력 y는 w[L] * w[L-1] * w[L-2] * ... * w[3] * w[2] * w[1]이 될것입니다.     
수학적으로 확인하고 싶다면 w[1] * x는 z[1]이 될 것이고 a[1]은 g(z[1])과 같습니다.    
선형의 활성화 함수를 사용하기 때문에 이 값은 z[1]과 같습니다.    
이런 식으로 계산하다보면 W[L]부터 W[1]까지 그리고 X를 곱한 값이 Y의 예측값이 됩니다.   

### (1보다 큰 수)E = W
![image](https://user-images.githubusercontent.com/50114210/65215671-4a485400-dae9-11e9-8f22-aa44686ce2da.png)  
각각의 가중치 행렬이 1.5E라고 생각해봅시다.    
마지막행렬은 다른 차원을 갖기 때문에 제외하고     
y^의 예측값을 계산해보면 W[L] * 1.5E^(L-1) * X입니다.    
만약 신경망이 깊게 쌓여져 있다면 Y의 예측값은 매우 클 것입니다.   
1.5에 층의 개수의 제곱만큼 말입니다.   
따라서 매우 깊은 신경망을 갖는다면 y의 값은 폭발할 것입니다.

### (1보다 작은 수)E = W
![image](https://user-images.githubusercontent.com/50114210/65215794-bcb93400-dae9-11e9-9a3c-180d7e1a8b5e.png)        
만약 0.5E를 W으로 가지게 된다면 Y의 예측값은     
W[L] * 0.5E^(L-1) * X가 될 것입니다.    
신경망이 깊어지면 깊어질 수록 예측값은 기하급수적으로 감소할 것입니다.   

### 직관
![image](https://user-images.githubusercontent.com/50114210/65215890-1d487100-daea-11e9-8c97-876c474baa15.png)       
따라서 여기서 얻을 수 있는 직관은     
가중치 W[L]이 단위행렬보다 조금 더 크다면 매우 깊은 네트워크의 경우 활성값은 폭발할 수 있습니다.     
그리고 W[L]이 단위행렬보다 조금 더 작다면 매우 깊은 네트워크의 경우 활성갑은 기하급수적으로 감소할 것입니다.     
비록 이 글에서는 L의 함수로 활성값이 기하급수적으로 증가, 감소한다고 설명했지만,    
비슷한 주장을 미분값 즉, 경사하강법에서 계산하는 경사가 층의 개수에 대한 함수로 기하급수적으로 증가하거나 감소한다는 것을 보여주는데 사용할 수도 있습니다.    

### 아웃트로
현대의 신경망은 보통 L이 150의 값을 갖습니다.    
마이크로소프트는 최근 852개의 층을 가진 신경망을 결과로 내놓았죠.   
그러나 이런 깊은 신경망에서 활성값이나 경사가 L에 대한 함수로 기하급수적으로 증가하거나 감소한다면    
기울기들은 아주 커지거나 작아질 수 있습니다.   
그렇게 된다면 신경망을 훈련시키는 것이 아주 어려워질 수 있습니다.   
특히 경사가 기하급수적으로 작아지는 경우에 말입니다.    
경사 하강법은 한 단계마다 아주 작은 단계만을 진행할 것이고 학습시키는데 아주 오랜 시간이 걸릴 것입니다.    




