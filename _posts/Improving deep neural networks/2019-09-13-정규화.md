---
layout: post
title: "정규화"
tags: [regularization, L1 norm, L2 norm, Frobenius norm]
categories: [Improving deep neural networks]
---

# 학습 목표
정규화를 이용해 과대적합 문제를 해결할 수 있다.

# 핵심 키워드
* 정규화(regularization)
* 정규화 매개 변수 lambda
* L1 노름 (L! norm)
* L2 노름 (L2 norm)
*Frobenius 노름 (Frobenius norm)

# 학습 내용
![image](https://user-images.githubusercontent.com/50114210/64857673-2a83dc80-d660-11e9-9165-1b227566fea4.png)      

# 인트로
높은 분산으로 신경망이 데이터를 과대적합하는 문제가 의심된다면 가장 처음 시도해야할 것은 정규화입니다.     
높은 분산을 해결하는 다른 방법은 더 많은 훈련 데이터를 얻는 것입니다.    
꽤 믿을만한 방법이지만 더 많은 훈련 데이터를 얻는 것은 비용이 많이 들어가게 됩니다.    
그에비해 정규화는 비용이 추가로 들지 않으면서 과대적합을 막고 신경망의 분산을 줄이는데 도움이 됩니다.     

# 로지스틱 회귀
로지스틱 회귀를 사용해 이 아이디어를 발전시켜 보겠습니다.    

### 로지스틱 회귀
![image](https://user-images.githubusercontent.com/50114210/64857980-0d034280-d661-11e9-97dd-9c32a12c3bf7.png)     
로지스틱 회귀는 다음과 같이 정의된 비용 함수 J를 최소화하는 것임을 기억하시나요.    
훈련 샘플의 개별적인 예측의 손실에 관한 함수입니다. 로지스틱 회귀의 w와 b는 매개변수입니다.     

### w와 b
![image](https://user-images.githubusercontent.com/50114210/64858007-260bf380-d661-11e9-8ac5-ba87ccdcef5b.png)     
w는 x차원의 매개변수 벡터이고, b는 실수입니다.    

### 정규화
![image](https://user-images.githubusercontent.com/50114210/64858066-4d62c080-d661-11e9-87f8-e7aa94e83b7d.png)              
![image](https://user-images.githubusercontent.com/50114210/64858081-5a7faf80-d661-11e9-9aed-34013f85e38e.png)          
따라서 로지스틱 회귀에 정규화를 추가하기 위해서 정규화 매개변수라고 부르는 람다를 추가해야 합니다.     
람다를 2 * m으로 나누고 w 제곱의 노름을 곱해줍니다.    
여기서 w 제곱의 노름은 j의 1부터 nx까지 wj^2의 값을 더한 것과 같습니다. w의 전치행렬 곱하기 w와도 같습니다.      
매개변수 벡터 w의 유클리드 노름의 제곱입니다. 이것을 L2 정규화라고 부릅니다.             
매개변수 벡터 w의 유클리드 노름, 즉 L2 노름을 사용하고 있기 때문입니다.    

### 왜 b는 정규화를 안해주지?
![image](https://user-images.githubusercontent.com/50114210/64858341-088b5980-d662-11e9-90fa-232dd6034d9d.png)     
왜 매개변수 w와 정규화할까요? 왜 b에 관한 것은 추가하지 않을까요?    
실제로 가능합니다만 보통 생략합니다. 매개변수 w는 꽤 높은 차원의 매개변수 벡터이기 때문입니다.    
특히 높은 분산을 가질 때 w는 많은 매개변수를 갖습니다. 반면에 b는 하나의 숫자입니다.    
따라서 거의 모든 매개변수는 b가 아닌 w에 있습니다. 이 마지막 항을 넣어도 실질적인 차이는 없을 겁니다.    
왜냐하면 많은 매개변수 중 b는 하나의 매개변수이기 때문입니다.

# L1 정규화와 L2 정규화
L2 정규화는 가장 일반적인 정규화입니다. 하지만 L1 정규화도 존재합니다. 

### L1 정규화
![image](https://user-images.githubusercontent.com/50114210/64858405-3a9cbb80-d662-11e9-83c3-24730203957c.png)    
L1 정규화를 사용한다면 L2 노름 대신의 위의 항을 추가하게 됩니다.   
람다를 m으로 나눈 값에 |w|의 합을 더해줍니다. 매개변수 벡터 w의 L1노름이라고도 불립니다.    
m앞에 곱하기 2는 스케일링 상수 입니다. L1 정규화를 사용하게 되면 w는 희소해지는데 이는 w벡터 안에 0이 많아진다는 의미입니다.    
어떤 사람들은 이것이 모델을 압축하는데 도움이된다고 말합니다.    
왜냐하면 특정 매개변수가 0일 경우 메모리가 적게 필요하기 때문입니다.    
그러나 모델을 희소하게 만들기 위해 L1 정규화를 사용하는 것은 큰 도움이 되지 않습니다.   
모델을 압축하겠다는 목표가 있지 않는 이상 이 정규화를 많이 사용하지 않습니다.    

### L2 정규화
![image](https://user-images.githubusercontent.com/50114210/64858516-98310800-d662-11e9-9552-633a8b6c8d26.png)    
사람들이 네트워크를 훈련할 때는 L2 정규화를 훨씬 더 많이 사용합니다.    

### 정규화 매개변수
위에서 계속 람다라고 불렀던 변수를 정확히는 정규화 매개변수라고 부릅니다.    
개발 세트 혹은 교차 검증 세트를 주로 사용합니다.    
다양한 값을 시도해서 훈련 세트에 잘 맞으면서 두 매개변수의 노름을 잘 설정해 과대적합을 막을 수 있는 최적의 값을 찾습니다.    
따라서 람다는 설정이 필요한 또 다른 하이퍼파라미터입니다.              

# 신경망
신경망은 어떨까요? 

### 비용함수
![image](https://user-images.githubusercontent.com/50114210/64858947-cebb5280-d663-11e9-822f-029a3eb20c32.png)     
신경망에는 비용 함수가 있습니다.   
모든 파라미터 w[1], b[1]부터 w[L], b[L]까지의 매개변수를 갖는 함수입니다.    
L은 신경망에 있는 층의 개수입니다. 따라서 비용함수는 훈련 샘플의 m까지의 손실의 합을 m으로 나눈 값입니다.   

### 정규화 식 더하기
![image](https://user-images.githubusercontent.com/50114210/64858999-f4485c00-d663-11e9-9cba-553e49f698c0.png)     
여기에 정규화를 더하기 위해 람다를 2m으로 나눈 값 곱하기 매개변수 w노름 제곱의 모든 값을 더해줍니다.     

### L2안의 행렬 계산법
![image](https://user-images.githubusercontent.com/50114210/64859340-efd07300-d664-11e9-9251-c73041130eff.png)      
여기 있는 행렬의 노름의 제곱은 i와 j에 해당하는 각각의 행렬의 원소를 제곱한 것을 모두 더해준 값입니다.     
합의 범위를 정의하고 싶다면 i는 1부터 n[L-1]이고 j는 1부터 n[L]까지로 합니다.      
![image](https://user-images.githubusercontent.com/50114210/64859415-20b0a800-d665-11e9-8870-d07f06d3232c.png)            
왜냐하면 w는 (n[L-1], n[L])차원의 행렬이기 때문입니다.    
이것은 해당 층 L-1 과 L의 은닉 유닛의 개수를 나타냅니다.    
따라서 이 행렬의 노름은 프로베니우스 노름이라고 부릅니다. 아래 첨자에 F를 표시해줍니다.         
행렬의 L2 노름이라고 부르는 것이 더 자연스럽지만 관례적인 이유에 따라서 프로베니우스 노름이라고 부릅니다.    
행렬의 원소 제곱의 합이라는 뜻입니다.     

### 경사 하강법 구현
그럼 이것으로 경사 하강법을 어떻게 구현할까요? 전에는 역전파의 dw를 계산했습니다.      
역전파는 w에 대응하는 비용 함수의 편미분 값을 제공했습니다.    
주어진 L에 대한 w입니다. 그리고 W[L] := W[L] - 학습률 * dW[L]으로 업데이트 했습니다.    
이것은 추가적인 정규화 항을 더해주기 이전의 값입니다.    

### 정규화 항을 더한 경사 하강법
![image](https://user-images.githubusercontent.com/50114210/64859440-345c0e80-d665-11e9-9a13-b7b09d3a8e19.png)    
따라서 정규화 항을 더해주게 되면 dw[L]에 람다 나누기 m 곱하기 W[L]을 더해줍니다.    
그리고 이 값을 전과 같은 방식으로 계산합니다.    
이 새로운 dW[L]의 정의는 여전히 비용함수의 미분에 대한 올바른 정의입니다.     
매개변수에 관해 끝에 정규화 항을 더해준 것 뿐입니다.    

### 가중치 감쇠 (Weight decay)
![image](https://user-images.githubusercontent.com/50114210/64859642-cc59f800-d665-11e9-95f9-363dbea53cb3.png)      
이러한 이유 때문에 L2 정규화는 가중치 감쇠라고 불리기도 합니다. 여기 dW[L]의 정의를 아래에 적용시키면     
W[L] := W[L] - 학습률 * ((역전파 값) + (람다 / m * W[L], 정규화 식))이 됩니다.    
이 식을 잘 정리하면
![image](https://user-images.githubusercontent.com/50114210/64859782-20fd7300-d666-11e9-83ff-ed17af9431d4.png)           
이 되기 때문에 W[L]이 어떻 값이든 값이 약간 더 작아진다는 것을 보여줍니다.  
1보다 작은 값을 W[L]에 곱하게 되기 때문이죠.    
따라서 이것이 L2 노름 정규화가 가중치 감쇠라고 불리는 이유입니다.    
역전파의 값을 학습률 정도로 빼서 값을 업데이트 하는 것은 원래의 경사 하강법과 같지만    
W[L]에 1보다 작은 값을 곱해주는 모습이 침식하는 것처럼 느껴지나 봅니다.















