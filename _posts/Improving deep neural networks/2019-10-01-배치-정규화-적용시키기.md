---
layout: post
title: "배치 정규화 적용시키기"
tags: [Batch Normalization]
categories: [Improving deep neural networks]
---

# 학습 목표
신경망에서 배치 정규화의 적용 과정을 상세하게 배운다.

# 핵심 키워드
배치 정규화(Batch Normalization)

# 학습 내용
* 은닉층에서 두 단계로 나뉩니다.
  - 첫째, 선형결합인 z를 계산하고, 이를 배치 정규화 시킵니다.
  - 둘째, 정규화 된 값들을 활성화 함수를 거쳐 활성화 값 a를 얻습니다.
* 선형결합 단계에서 상수항 b는 없어집니다. 그 이유는 배치 정규화 과정에서 z의 평규능ㄹ 빼주면 사라지기 때문입니다.

# 인트로
하나의 은닉층에 배치 정규화를 구현하는 것에 관한 수식을 봤는데요.      
심층 신경망 학습에 어떻게 적용될 수 있는지 봅시다.        

# 신경망의 평범한 계산
![image](https://user-images.githubusercontent.com/50114210/65898055-af565080-e3eb-11e9-96e8-dce21abf7921.png)      
이런 신경망이 있다고 해보죠.     
제가 말씀드렸던 대로 은닉 유닛을 두 가지로 나눠볼 수 있습니다.       
z를 우선 계산하고 활성화 함수를 이용해서 a를 계산하는 거죠.         
즉 원 하나하나가 두 단계의 계산을 나타낸다고 생각하면 됩니다.         
다음 층에서도 비슷하게 z^[2]_1와 a^[2]_1이 되겠죠.      
만약 배치 정규화를 쓰지 않는다면 입력값 x가 첫 번째 은닉층에 주어지겠죠?      
그러면 w^[1]과 b^[1]에 따라 우선 z^[1]을 계산합니다.         
그리고 활성화 함수에 z^[1]이 주어져서 a^[1]을 계산하는 거죠.       

# 배치 정규화를 적용한 계산 1
![image](https://user-images.githubusercontent.com/50114210/65898116-cd23b580-e3eb-11e9-8257-38d1ca3c7db0.png)      
하지만 배치 정규화에서는 z^[1]을 받아서 배치 정규화를 적용합니다.       
BN으로 줄여쓸 수 있고요. 이 과정은 β^[1]과 Y^[1]의 영향을 받겠죠?          
그러면 새로 정규화된 z^[1] 값을 얻게 됩니다.      
그걸 활성화 함수에 줘서 a^[1]을 얻는 것이죠. g1을 z~^[i]에 적용하는 겁니다.       
이렇게 첫 번째 층에 대한 계산을 마쳤습니다.       
배치 정규화가 z와 a를 계산하는 사이에 이뤄졌죠.     

# 배치 정규화를 적용한 계산 2
![image](https://user-images.githubusercontent.com/50114210/65898149-da40a480-e3eb-11e9-9383-fb61b51376f7.png)     
다음으로 a^[1]을 가지고 z^[2]를 계산합니다.    
이 과정 역시 w^[2], b^[2]의 영향을 받고요.      
첫 번째 층에서 했던 것처럼 z^[2]에 대해 BN을 적용합니다.         
이 과정은 배치 정규화의 변수인 β^[2]과 γ^[2]의 영향을 받을 겁니다.        
그러면 z~^[2]를 얻게 되고 이제 활성화 함수를 적용해서 a^[2]를 얻게 됩니다.       
이 과정이 계속 되죠. 마찬가지로 배치 정규화가 z 계산과 a 계산 사이에 이뤄집니다.     
여기서 비정규화된 z 값 대신 정규화된 값 z-를 사용하는 것이죠.      
첫 번째 층에서 그렇게 했고 두 번째 층도 마찬가지입니다.      
비정규화된 z^[2] 대신 평균과 분산으로 정규화된 z-^[2]를 사용합니다.            

# 적용하는데 필요한 파라미터들
![image](https://user-images.githubusercontent.com/50114210/65898230-03613500-e3ec-11e9-811d-09c03d04c6b6.png)     
w^[1], b^[1] 부터 w^[l], b^[l]까지 있겠죠?         
그리고 추가로 β^[1], Y^[1] β^[2], Y^[2] 등이 있을 겁니다.      
각 층에 배치 정규화를 할 때 사용되죠.        
여기 β는 모멘텀이나 기하급수적 평균을 구할 때 쓰는 하이퍼파라미터와 다릅니다.         
Adam 논문에서도 하이퍼파라미터를 나타내기 위해 β를 썼고         
배치 정규화 논문에서도 β를 썼지만 두 β는 완전히 다릅니다.         
물론 논문을 읽을 때 β가 어떻게 쓰이는지 따로 알아둬야겠지만        
배치 정규화에서 사용되는 β^[1], β^[2]는 모멘텀이나       
단일, RMSprop 알고리즘에 쓰이는 하이퍼파라미터 β와 다릅니다.        
이 변수들이 알고리즘에 쓰입니다.       

# 어떤 최적화 알고리즘을 사용하지
![image](https://user-images.githubusercontent.com/50114210/65898298-2ab80200-e3ec-11e9-8608-bfa4377a9978.png)    
이제 변수를 찾기 위해 경사 하강법 등 어떤 최적화를 쓸지 고민할 차례입니다.         
예컨대 어떤 층에서 dβ^[l]을 계산했다고 칩시다.     
그리고 β를 β-(학습 속도)*dβ^[l]로 수정했다 합시다.         
물론 Adam, RMSprop, 모멘텀 등을 써서 β나 γ를 업데이트할 수도 있어요.       
경사하강법만 있는 것이 아닙니다.        

# 프레임워크에서의 배치 정규화
![image](https://user-images.githubusercontent.com/50114210/65898323-3c99a500-e3ec-11e9-941f-ebfe655bca76.png)      
또 이전 영상에서 배치 정규화를 위해 평균과 분산을 빼고 나누는 방식을 설명했지만        
딥러닝 프로그래밍 프레임워크를 사용하면 배치 정규화를 따로 구현할 필요가 없습니다.         
예를 들어 텐서플로 프레임워크에서는 이 함수를 이용해서 배치 정규화를 구현할 수 있습니다.        
실제로 모든 사항을 하나하나 구현할 필요가 없다는 것을 알려드립니다.        
하지만 코드가 뭘 하는지 알기 위해 이 과정이 어떻게 작동하는지 알아두는 것이 좋습니다.        

# 미니배치에 적용한 경사하강법
![image](https://user-images.githubusercontent.com/50114210/65898356-56d38300-e3ec-11e9-80af-54e8b8ba9c72.png)     
지금까지 경사 하강법을 이용해서 배치 정규화로 전체를 학습시켰습니다.       
하지만 실제로는 배치 정규화가 훈련 집합의 미니 배치에 적용됩니다.      
즉 실제로는 첫 번째 미니배치에 대해서 이전 슬라이드에서 했던 것처럼 z^[1]을 계산합니다.    
w^[1]과 b^[1]을 이용해서요. 그리고 이 미니 배치 안에서       
z^[1]의 평균과 분산을 계산한 뒤에 평균을 빼고 표준편차로 나눠 배치 정규화를 진행합니다.       
물론 β^[1]와 γ^[1]을 이용해서 값을 조정해줘야 합니다.       
이 과정을 통해 첫 번째 미니 배치에서 z-^[1]을 얻게 되고      
활성화 함수를 적용해 a^[1]을 얻는 것이죠.         

### 반복
![image](https://user-images.githubusercontent.com/50114210/65898382-694dbc80-e3ec-11e9-9af0-2352c3b58e4b.png)    
그리고 z^[2]를 w^[2]와 b^[2]를 이용해 계산해냅니다. 이 과정의 반복이죠.        
이렇게 첫 번째 미니 배치에 대해 경사하강법을 이용해서 과정을 마쳤으면       
두 번째 미니 배치 X^{2}로 이동합니다.       
그리고 X^{2}에 대해 비슷한 방법으로 z^[1]을 계산하고요.       
그리고 배치 정규화를 써서 z-^[1]을 계산합니다.     
이 정규화에서는 두 번째 미니배치에 있는 데이터만을 이용해서 진행합니다.         
두 번째 미니배치만 이용해서 평균과 분산을 계산하는 것이지요.        
그리고 β와 γ로 보정해서 z-를 얻습니다.       
마찬가지로 세 번째 미니 배치에 대해서도 해 줍니다.        

# 학습에 필요한 파라미터들
![image](https://user-images.githubusercontent.com/50114210/65898441-88e4e500-e3ec-11e9-8b50-83734f811806.png)   
이제 매개변수화 과정에서 제가 한 가지 말씀드리고 싶은 것이 있는데요.         
제가 방금 각 층에 대해 w^[l]과 b^[l]      
그리고 β^[l]와 γ^[l]의 변수가 있다고 말씀드렸죠.        
여기서 z^[l]은 w^[l]*a^[l-1] + b^[l]로 계산됩니다.     
여기서 배치 정규화는 미니 배치를 보고     
z^[l]이 평균 0,  분산 1을 갖도록 정규화한 뒤       
β와 γ를 이용하여 값을 조정해주는 것입니다.       

# 필요없어진 변수 b
![image](https://user-images.githubusercontent.com/50114210/65898484-9ef2a580-e3ec-11e9-844a-6deb6c0bc06f.png)            
여기서 b^[l]은 값이 무엇이든지 간에 없어집니다.         
왜냐하면 배치 정규화의 정규화 과정에서 z의 평균을 계산한 뒤에 빼주기 때문입니다.       
즉 미니 배치의 모든 예시에 상수를 더해줘도    
결국 평균을 빼주면서 사라지기 때문에 아무런 영향을 끼치지 않습니다.      
즉 배치 정규화를 쓴다면 이 변수를 없앨 수 있습니다.          
아니면 저 변수를 항상 0으로 둔다고 생각해도 좋습니다.         
즉 z^[l] = w^[l]*a^[l-1]이 되는 것이죠.    
그리고 z^[l]_norm를 계산하고 z~ = γ^[l]*z^[l]_norm+β^[l]     
여기서 다음 층에 전달되는 z~의 평균을 정하기 위해 β^[l]은 써줘야 합니다.        
다시 말하자면 배치 정규화가 z^[l]의 평균을 0으로 만들기 때문에         
b^[l]이라는 변수가 필요 없습니다. 그걸 없애는 대신        
β^[l]이 그 역할을 차지하는 것이죠. 결과적으로 편향 변수를 결정하니까요.      

# 각 변수들의 차원
![image](https://user-images.githubusercontent.com/50114210/65898602-ebd67c00-e3ec-11e9-89b7-211d73bb0bc8.png)     
끝으로 z^[l]의 차원이 (n^[l], 1)이었던 것 기억하시나요?     
즉 b^[l]은 (n[l], 1) 차원일 겁니다.        
여기서 n^[l]은 층 l에서의 은닉 유닛 숫자고요.     
따라서 β^[l]과 γ^[l]의 차원도 (n[l], 1)이 됩니다.          
왜냐하면 어떤 신경망이든지 간에 n^[l]개의 은닉 유닛을 갖고 있을 때         
β^[l]과 γ^[l]이 각 은닉 유닛의 값을 조정하는 데 쓰이기 때문이죠.          

# 배치정규화 알고리즘
![image](https://user-images.githubusercontent.com/50114210/65898662-0ad50e00-e3ed-11e9-829b-77a06b5713c6.png)        
이제 모든 걸 합쳐서 배치 정규화를 사용하여 경사하강법을 구현하는 법을 살펴봅시다.        
미니 배치 경사하강법을 사용한다고 해보죠. t가 1부터 미니 배치의 숫자까지 바뀐다고 합시다.      
순방향 전파를 사용한다고 하죠. 미니 배치 X^{t}에 대해서요.          
각 은닉층에서 z^[l]을 z-^[l]으로 바꾸기 위해서 말입니다.      
이 과정을 거치면 미니배치의 평균과 표준편차를 이용해서        
z^[l]을 z-^[l]으로 바꾸어주는 것이지요.         
그리고 역방향 전파를 이용해서 층의 모든 변수에 대해 dw, db, dβ, dγ를 계산합니다.         
방금 전에 b는 없앴으니 생각하지 맙시다. 그리고 각 변수들을 업데이트 합니다.       
w는 w-α*dw가 되겠고요. β는 β-α*dβ로 업데이트 됩니다.          
γ도 마찬가지이고요. 이렇게 차이를 계산하면         
적은 것처럼 경사하강법을 이용할 수 있는 것이죠.          
물론 모멘텀, RMSprop, Adam을 사용하는 경사하강법에도 쓸 수 있습니다.       
이렇게 경사하강법을 이용해서 업데이트를 하지 않고          
제가 적은 알고리즘을 이용해서 업데이트를 할 수도 있는 것이지요.     
지난 주의 영상에서 살펴봤던 내용입니다.       
이런 다양한 알고리즘을 이용해서 β나 γ 같이       
배치 정규화에 쓰이는 변수를 업데이트할 수 있습니다.        

# 아웃트로
이 을 통해 배치 정규화의 구현 방법에 대해 감을 좀 잡으셨으면 좋겠고요.        
나중에 다루겠지만 딥러닝 프로그램 프레임워크를 사용하면         
프레임워크에서 누군가가 만들어놓은 코드를 써서 쉽게 배치 정규화를 쓸 수 있습니다.          
만약 아직도 배치 정규화가 알쏭달쏭하다면 또는 왜 이 과정이 학습을            
빠르게 만들어주는지 모르겠다면 다음 글로 가서 왜 배치 정규화가 좋고       
과연 뭘 하는 건지 좀 더 살펴봅시다.     
