---
layout: post
title: "학습률 감쇠"
tags: [Learning Rate Decay]
categories: [Improving deep neural networks]
---

# 학습 목표
학습률 감쇠 기법을 배운다.

# 핵심 키워드
학습률 감쇠(Learning Rate Decay)

# 학습 내용
* 작은 미니배치 일수록 잡읍이 심해서 일정한 학습률이라면 최적값에 수렴하기 어려운 현상을 볼 수 있습니다.
* 학습률 감쇠 기법을 사용하는 이유는 점점 학습률을 작게 줘서 최적값을 더 빨리 찾도록 만드는 것입니다.
* 다양한 학습률 감쇠 기법들이 있습니다.
![image](https://user-images.githubusercontent.com/50114210/65752075-025ba980-e147-11e9-80c4-4874042b9167.png)         

# 인트로
학습 알고리즘의 속도를 높이는 한 가지 방법은 시간에 따라 학습률을 천천히 줄이는 것입니다.            
이것을 학습률 감쇠라고 부르는데 어떻게 구현할 수 있을지 살펴봅시다.          

# 그냥 미니배치 경사하강법의 훈련 양상
![image](https://user-images.githubusercontent.com/50114210/65752571-0805bf00-e148-11e9-949f-9a43b3cfd94e.png)           
왜 학습률 감쇠가 필요한지 예시를 하나 들어보겠습니다.         
상당히 작은 미니배치에 대해 미니배치 경사 하강법을 구현한다고 가정해봅시다.            
64나 128 같은 경우 말입니다. 단계를 거치면서 약간의 노이즈가 있지만              
최솟값으로 향하는 경향을 보일 것입니다.            
그러나 정확하게 수렴하지는 않고 주변을 돌아다니게 될 것입니다.           
왜냐하면 어떤 고정된 값인 α를 사용했고,           
서로 다른 미니배치에 노이즈가 있기 때문입니다.            

# 학습률 감쇠를 적용했을 때의 훈련 양상
![image](https://user-images.githubusercontent.com/50114210/65752618-1c49bc00-e148-11e9-96b2-de2171e2978c.png)            
그러나 천천히 학습률 α를 줄이면            
α가 여전히 큰 초기 단계에서는 여전히 상대적으로 빠른 학습이 가능합니다.              
그러나 α가 작아지면 단계마다 진행 정도가 작아지고,             
최솟값 주변의 밀집된 영역에서 진동하게 될 것입니다.             
훈련이 계속되더라도 최솟값 주변에 배회하는 대신에 말입니다.              
따라서 α를 천천히 줄이는 것의 의미는 학습의 초기 단계에서는           
훨씬 큰 스텝으로 진행하고 학습이 수렴할수록          
학습률이 느려져 작은 스텝으로 진행합니다.              

# 학습률 감쇠 구현
![image](https://user-images.githubusercontent.com/50114210/65752657-31264f80-e148-11e9-84de-5facd95f7f0e.png)           
따라서 여기 학습률 감쇠를 구현하는 방법입니다.             
하나의 에포크는 데이터를 지나는 하나의 패스입니다.             
따라서 훈련 세트가 다음과 같으면 서로 다른 미니배치로 나눠서,             
훈련 세트를 지나는 첫 번째 패스를 첫 번째 에포크라고 부릅니다.             
두 번째 지나는 것을 두 번째 에포크라고 부릅니다.            
따라서 여러분이 할 수 있는 것은 학습률 α를 설정하는데,             
1/ (1+ decay_rate) * epoch_num 여기에 초기 학습률인 α_0를 곱해줍니다.            
여기 있는 감쇠율은 조정이 필요한 또 다른 하이퍼파라미터입니다.             

# 학습률 감쇠 예시
![image](https://user-images.githubusercontent.com/50114210/65752694-426f5c00-e148-11e9-85d0-e8b21cf0f1b0.png)            
여기 구체적인 예시가 있습니다. 여러 번의 에포크를 거치면,            
α_0가 0.2 이고, 감쇠율이 1일 때 첫 번째 에포크를 하는 동안,            
α는 1/(1+1*α_0)입니다. 따라서 감쇠율이 1이고         
에포크가 1일 때 학습률은 0.1이 됩니다.              
두 번째 에포크에서 학습률은 0.67로 떨어지고,            
세 번째 에포크에서는 0.5이고 네 번째는 0.4입니다.           
다음 에포크도 계산해보면서 감을 잡으시기를 바랍니다.       

### 학습률의 변화
![image](https://user-images.githubusercontent.com/50114210/65752713-4ef3b480-e148-11e9-9add-0600cf08fa3c.png)           
에포크 수에 대한 함수에서 학습률은 점차적으로 감소합니다.            
위에 있는 이 식에 의하면 말이죠 학습률 감쇠를 사용하고 싶다면,            
하이퍼파라미터 α_0와 감쇠율에 대해서 다양한 값을 시도하고,              
잘 작동하는 값을 찾으면 됩니다.        

# 여러 방법들
### 지수적 감쇠
![image](https://user-images.githubusercontent.com/50114210/65752760-692d9280-e148-11e9-979b-f3d3a94968f5.png)            
학습률 감쇠에 대한 이 식 말고 사람들이 사용하는 또 다른 방법들이 있습니다.             
예를 들면 지수적 감쇠라고 불리는 것은 α가 1보다 작은 값을 가집니다.             
예를 들면 0.95^epoch_num*α_0입니다.               
따라서 이것은 기하급수적으로 빠르게 학습률을 감소시킵니다.           

### 또다른 방법
![image](https://user-images.githubusercontent.com/50114210/65752778-764a8180-e148-11e9-9ea0-d6be2f3de94b.png)          
사람들이 사용하는 또 다른 식은       
상수 k 나누기 epoch_num의 제곱근에 곱하기 α_0입니다.              
또는 미니배치의 개수 t의 제곱근으로 나눠주는 다음과 같은 식을 사용합니다.             

### 이산적으로 툭툭 떨어지게
![image](https://user-images.githubusercontent.com/50114210/65752793-7ea2bc80-e148-11e9-81bf-af439710e251.png)        
또 어떤 사람들은 이산적 단계로 감소하는 학습률을 사용하기도 합니다.                 
어떤 단계에서는 어떤 학습률 값을 가지고,               
그 뒤에는 학습률이 반으로 줄어들고 일정 시간이 지날 때마다,              
계속 반씩 줄어드는 모습입니다. 이런 것을 이산 계단이라고 부릅니다.               
이런 것을 이산 계단이라고 부릅니다.              

### 수동으로 직접 관리
사람들이 사용하는 또 다른 방법은 직접 조작하는 감쇠입니다.            
따라서 한 번에 하나의 모델을 훈련하는데 몇 시간 혹은 며칠이 걸린다면,            
어떤 사람들은 훈련을 거치면서 모델을 정리해 나갈 것입니다.            
학습률이 느려지고 있는 것처럼 느껴서 데이터의 크기를 줄이는 것입니다.             
이런 식으로 α의 값을 시간이나 날마다 직접 보정하는 것은            
훈련이 작은 수의 모델로만 이루어진 경우에 가능합니다.                
그러나 사람들은 가끔 이렇게도 합니다.               

# 아웃트로
이제 학습률 α를 조작하는 몇 가지 선택이 있습니다.               
만약 하이퍼파라미터의 개수가 너무 많아서         
어떤 선택을 해야 할지 잘 모르겠다면                
다음 글에서 배울 시스템적으로 하이퍼파라미터를               
선택하는 방법에서 알 수 있을 겁니다.                    
저의 경우에 학습률 감쇠는 제가 시도하는것들에서 낮은 우선순위를 갖습니다.              
α의 값만을 바꿔서 큰 영향을 얻을 수 있다는 점에서           
학습률 감쇠는 도움이 되고 훈련 속도를 빠르게 해줄 수 있습니다.            
그러나 제가 시도하는 목록에서는 낮은 우선순위를 갖습니다.            
다음 주에 다룰 시스템적인 하이퍼파라미터            
보정 방법을 통해 효율적으로 하이퍼파라미터를            
찾는 방법을 배울 수 있을 겁니다.           
학습률 감쇠는 여기까지입니다.              
신경망의 지역적 최적화와 안장점에 대해서도 이야기하고 싶습니다.             
최적화 문제의 종류에 대해 더 나은 직관을 얻게 될 것입니다.              
신경망을 훈련시킬 때 최적화 알고리즘이 풀려는 문제에 대해 말입니다.                 
