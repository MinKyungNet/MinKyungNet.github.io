---
layout: post
title: "미니배치 경사하강법 이해하기"
tags: [Mini Batch]
categories: [Improving deep neural networks]
---

# 학습 목표
미니배치 경사 하강법을 알 수 있다.

# 핵심 키워드
미니배치(Mini Batch)

# 학습 내용
* 배치 경사하강법에서는 한 번의 반복을 돌 때마다 비용 함수의 값은 계속 작아져야 합니다.
* 미니배치 경사 하강법에서는 전체적으로 봤을때는 비용함수가 감소하는 경향을 보이지만 많은 노이즈가 발생합니다.
* 미니배치 사이즈를 어떻게 선택하는지에 따라 학습 속도의 차이가 나기에 최적의 값을 찾아내는 것이 중요합니다.
* 만약 훈련 세트가 작다면 (2,000개 이하) 모든 훈련세트를 한 번에 학습시키는 배치 경사 하강을 진행합니다.
* 훈련 세트가 2,000개보다 클 경우 전형적으로 선택하는 미니배치 사이즈는 64, 128, 256, 512와 같은 2의 제곱수입니다.

# 인트로
지난 비디오에서 처음 훈련 세트를 훈련하는 도중에라도 경사 하강의 단계를       
진행시키기 위해 미니배치 경사 하강법을 어떻게 사용하는지에 대해 배웠습니다.     
이번 비디오에서는 경사 하강법에 대해 더 자세히 배우고,        
이것이 무엇이고 왜 잘 작동되는지에 대한 이해를 도울 것입니다.          

# 배치 경사 하강법
![image](https://user-images.githubusercontent.com/50114210/65479270-bd760f80-dec7-11e9-9d50-2fd347366376.png)     
배치 경사 하강법에서는 모든 반복에서 전체 훈련 세트를 진행하고      
각각의 반복마다 비용이 감소하기를 기대합니다.          
비용함수 J를 서로 다른 반복에 대한 함수로 그렸을 때 모든 반복마다 감소해야 합니다.       
하나라도 올라간다면 무언가 잘못된 것입니다. 학습률이 너무 크다든지 말입니다.      

# 미니배치 경사 하강법
![image](https://user-images.githubusercontent.com/50114210/65479292-cf57b280-dec7-11e9-90d7-c0892772788c.png)     
반면에 미니배치 경사 하강법에서 비용 함수에 대한 진행을 그려본다면 모든 반복마다 감소하지는 않습니다.            
특히 모든 반복에서 어떤 X^{t}와 Y^{t}를 진행시키는데 비용함수 J^{t}를 그려본다면,        
X^{t}와 Y^{t}만을 이용해 계산된 값입니다.      
그럼 모든 반복에서 다른 훈련 세트, 즉 다른 미니배치에서 훈련하는 것입니다.        
따라서 비용함수 J를 그리면 이와 같은 모습이 됩니다.        
전체적인 흐름은 감소하나 약간의 노이즈가 발생됩니다.       
따라서 미니배치 경사 하강법을 훈련하기 위한 J^{t}를 그리면       
여러 에포크를 거쳐서 이런 곡선을 보게 될 것입니다.        
모든 반복에서 아래로 내려가지는 않습니다. 그러나 큰 흐름은 아래로 내려갑니다.       
약간의 노이즈가 발생하는 이유는 아마 X^{1}과 Y^{1}이 상대적으로 쉬운 미니배치라서     
비용이 약간 낮은데 우연적으로 X^{2}와 Y^{2}가 더 어려운 미니배치라서     
아마 잘못 표시된 샘플이 있다든지의 이유로 비용이 약간 더 높아질 수 있습니다.     
미니배치 경사 하강법을 실행시킬 때 이러한 진동이 일어나는 이유입니다.       

# 미니 배치의 크기
![image](https://user-images.githubusercontent.com/50114210/65479378-1ba2f280-dec8-11e9-8ca3-5fcd85d14bba.png)    
여러분이 선택해야 하는 매개변수 중 하나는 미니배치의 크기입니다.       
m이 훈련 세트의 크기일 때 극단적인 경우는 미니배치 크기가 m과 같은 경우입니다.        
그럼 이것은 배치 경사 하강법이 됩니다. 따라서 이런 극단적인 경우에는 하나의 미니배치만을 갖게 됩니다.        
X^{1}과 Y^{1}입니다 그리고 이 미니배치의 크기는 전체 훈련 세트와 같습니다.       
따라서 미니배치 크기를 m으로 설정하는 것은 일반적인 경사 하강법과 같습니다.        

# 미니배치의 크기가 1인 경우
![image](https://user-images.githubusercontent.com/50114210/65479407-32494980-dec8-11e9-8b22-e1c1ceee8c63.png)      
다른 극단적인 경우는 미니배치 크기가 1과 같은 경우입니다.        
이것은 확률적 경사 하강법이라고 불리는 알고리즘을 제공합니다.         
그리고 여기서 각각의 샘플은 하나의 미니배치입니다.        
이런 경우에 첫 번째 미니배치인 X^{1}과 Y^{1}을 살펴보면 미니배치 크기가 1일 때          
이것은 첫 번째 훈련 샘플과 같습니다. 첫 번째 훈련 샘플로 경사 하강법을 하는 것입니다.        
그 다음에 두 번째 미니배치를 살펴보면 이것은 두 번째 훈련 샘플과 같고       
이것에 대한 경사 하강 단계를 취합니다 이런 식으로 한 번에 하나의 훈련 샘플만을 살펴보며 계속 진행합니다.       

# 그림으로 봐보자
![image](https://user-images.githubusercontent.com/50114210/65479437-4c832780-dec8-11e9-8886-09c79b3a511f.png)      
이제 이 두 가지 극단적인 경우가 비용함수를 최적화할 때 무엇을 하는지 살펴봅시다.        
만약 이것이 여러분이 최소화하려는 비용함수의 등고선이라면 최솟값은 여기에 위치합니다.         
그럼 배치 경사 하강법(파랑)은 어딘가에서 시작해 상대적으로 노이즈가 적고          
상대적으로 큰 단계를 취합니다. 그러면서 계속 최솟값으로 나아갑니다.    

### 확률적 경사 하강법
그와 반대로 확률적 경사 하강법(보라)에서 어딘가에서 시작하면     
모든 반복에서 하나의 훈련 샘플로 경사 하강법을 실행하게 됩니다.      
대부분의 경우 전역 최솟값으로 가게 되지만 어떤 경우는 잘못된 방향을 가르켜 잘못된 곳으로 가기도 합니다.       
따라서 확률적 경사 하강법은 극단적으로 노이즈가 많을 수 있지만,       
평균적으로는 좋은 방향으로 가게 됩니다 잘못된 방향일수도 있지만요.        
따라서 확률적 경사 하강법은 절대 수렴하지 않을 것입니다.         
진동하면서 최솟값의 주변을 돌아다니게 되지만 최솟값으로 곧장 가서 머물지는 않을 것입니다.        

# 적당한 크기의 미니배치
![image](https://user-images.githubusercontent.com/50114210/65479507-a126a280-dec8-11e9-8d15-5d1ea3226ee7.png)       
실제로 여러분이 사용하는 미니배치 크기는 1과 m 사이일 것입니다.        
1은 상대적으로 너무 작고, m은 상대적으로 너무 큰 값입니다.

### 배치 경사 하강법
![image](https://user-images.githubusercontent.com/50114210/65479497-923ff000-dec8-11e9-99a9-f33d1540be10.png)    
배치 경사 하강법을 사용한다면 미니 배치의 크기는 m과 같습니다.     
그럼 매우 큰 훈련 세트를 모든 반복에서 진행하게 됩니다.      
이것의 주된 단점은 한 반복에서 너무 오랜 시간이 걸린다는 것입니다.       
작은 훈련 세트에서는 괜찮지만 큰 훈련 세트에서는 오랜 시간이 필요합니다.        

### 확률적 경사 하강법
![image](https://user-images.githubusercontent.com/50114210/65479528-bc91ad80-dec8-11e9-8d72-4fa592fe693c.png)       
그와 반대로 확률적 경사 하강법을 사용한다면 하나의 샘플만 처리한 뒤에 계속 진행할 수 있습니다.       
하나의 샘플만 처리한 뒤에 계속 진행할 수 있어 매우 간단합니다.      
노이즈도 작은 학습률을 사용해 줄일 수 있습니다.     
그러나 확률적 경사 하강법의 큰 단점은 벡터화에서 얻을 수 있는 속도 향상을 잃게된다는 것입니다.       
한 번에 하나의 훈련 세트를 진행하기 때문에 각 샘플을 진행하는 방식이 매우 비효율적입니다.       

### 미니배치의 크기가 적당할 때
![image](https://user-images.githubusercontent.com/50114210/65479539-c74c4280-dec8-11e9-9179-8c7a4665e389.png)     
따라서 가장 잘 작동하는 것은 이 사이에 있는 값입니다.         
미니배치 크기가 너무 크거나 작지 않을 때입니다.      
그리고 실제로 이것은 가장 빠른 학습을 제공합니다.          
이를 통한 두 가지 장점이 있습니다. 

#### 첫번째 장점
하나는 많은 벡터화를 얻는다는 것입니다.       
따라서 이전 비디오에서 사용했던 예시처럼 미니배치 크기가 1,000개의 샘플이라면      
1,000개의 샘플에 벡터화를 하게 될 것입니다.       
그럼 한 번에 샘플을 진행하는 속도가 더 빨라지게 됩니다.        

#### 두번째 장점
두 번째로 전체 훈련 세트가 진행되기를 기다리지 않고 진행을 할 수 있습니다.        
이전 비디오의 예를 들면 각각의 훈련 세트의 에포크는 5,000번의 경사 하강 단계를 허용합니다.      
따라서 실제로 사이에 있는 미니배치 크기가 가장 잘 작동합니다.        
여기서 시작한 미니배치라면 한 번의 반복, 두 번의 반복마다 이런 식으로 진행하게 됩니다.      
항상 최솟값으로 수렴한다고 보장할 수는 없지만 더 일관되게 전역의 최솟값으로 향하는 경향이 있습니다.      
그리고 매우 작은 영역에서 항상 정확하게 수렴하거나 진동하게 됩니다.      
그게 문제라면 학습률을 낮추는 방법이 있습니다.       

# 훈련세트가 작을때
![image](https://user-images.githubusercontent.com/50114210/65479573-ecd94c00-dec8-11e9-969a-d99143fe9c4a.png)        
그렇다면 미니배치 크기가 m이나 1이 아닌 그 사이의 값이어야 한다면 이 값을 어떻게 선택할까요?       
첫 번째로 작은 훈련 세트라면 그냥 배치 경사 하강법을 사용하세요.       
훈련 세트가 작다면 미니배치 경사 하강법을 사용할 필요 없이 전체 훈련 세트를 빠르게 진행할 수 있습니다.            
여기서 작은 훈련 세트는 샘플이 2000개보다 적은 경우를 말합니다.       
그런 경우라면 배치 경사 하강법을 쓰는 것이 더 괜찮습니다.       

# 훈련세트가 클 때
![image](https://user-images.githubusercontent.com/50114210/65479588-ff538580-dec8-11e9-8196-597bfd8efc0f.png)        
이와 달리 더 큰 훈련 세트라면 전형적인 미니배치 크기는 64에서 512 사이가 가장 일반적입니다.       
왜냐하면 컴퓨터 메모리의 접근 방식을 생각해보면         
미니배치 크기가 2의 제곱인 것이 코드를 빠르게 실행시켜줍니다.        
따라서 64는 2^6, 128은 2^7, 256은 2^8, 512는 2^9입니다.         
저는 미니배치의 크기를 2의 제곱수로 구현할 것입니다      
이전 비디오에서는 미니배치 크기를 1,000으로 설정했는데       
그 값 대신 2^10인 1024를 사용하는 것을 추천합니다.        
미니배치 크기로 1,024는 조금 드문 경우입니다.       
이 범위의 미니배치 크기가 더 일반적입니다.       

# 한번의 병렬처리로 계산이 가능하도록
![image](https://user-images.githubusercontent.com/50114210/65479592-08dced80-dec9-11e9-98e4-6d9f4ecc8f5a.png)        
마지막 팁은 미니배치에서 모든 X^{t}와 Y^{t}가 CPU와 GPU 메모리에 맞는지 확인하세요.       
이것은 여러분의 애플리케이션과 하나의 훈련 샘플의 크기에 달려 있습니다.        
그러나 CPU나 GPU 메모리에 맞지 않는 미니배치를 진행시키면,      
성능이 갑자기 떨어지고 훨씬 나빠지게 됩니다.       

# 아웃트로
사람들이 사용하는 전형적인 범위의 미니매치 크기에 대한 감을 잡으셨기를 바랍니다.      
실제로 미니배치 크기는 빠른 탐색을 통해 찾아내야 하는 또 다른 하이퍼파라미터입니다.      
가장 효율적이면서 비용함수 J 를 줄이는 값을 찾아내야 합니다.       
제가 하는 방법은 몇 가지 다른 2의 제곱수를 시도해보고,       
경사 하강법 최적화 알고리즘을 가능한 가장 효율적이게 만드는 값을 선택하는 것입니다.        
말씀드린 방법이 값을 찾는 것을 시작하는데 도움이 되었으면 좋겠습니다.        
미니배치 경사 하강법의 구현과 알고리즘의 수행 속도를 높이는 방법을 배웠습니다.      
특히 큰 훈련 세트에서 훈련할 때 말이죠.     
그러나 경사 하강법이나 미니배치 경사 하강법보다 더 효율적인 알고리즘이 있습니다.       
다음 비디오에서 함께 살펴봅시다.        

