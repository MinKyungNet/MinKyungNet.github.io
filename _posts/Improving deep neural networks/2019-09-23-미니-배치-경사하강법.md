---
layout: post
title: "미니 배치 경사하강법"
tags: [Mini Batch]
categories: [Improving deep neural networks]
---

# 학습 목표
미니배치 경사 하강법을 알 수 있다.

# 핵심 키워드
미니 배치(Mini Batch)

# 학습 내용
* 배치 경사 하강법
* 전체 훈련 샘플에 대해 훈련 후 경사 하강 진행
* 미니배치 경사 하강법
* 전체 훈련 샘플은 작은 훈련 세트인 미니배치로 나눈 후, 미니배치 훈련 후 경사 하강 진행
* 배치 경사 하강법은 큰 데이터 세트를 훈련하는데 많은 시간이 들기에 결과적으로 경사 하강을 진행하기까지 오랜 시간이 얼립니다. 따라서 작은 훈련 세트인 미니배치로 나누어 훈련 후 경사 하강을 진행합니다.
* 예를 들어, 전체 훈련 세트 크기가 5,000,000이라고 할 때 이를 사이즈가 1,000인 미니배치 5,000개로 나누어 훈련 및 경사 하강법을 진행합니다.
* 표기법   
  - i 번째 훈련 세트 : x(i)      
  - L 번째 신경망의 z 값 : z[L]
  - t 번째의 미니배치 : X{t}, Y{t}
  
# 인트로  
이번 주에는 신경망을 더 빠르게 학습하도록하는 최적화 알고리즘에 대해 배우겠습니다.    
머신러닝을 적용하는 것은 매우 실험적인 과정이라는 것을 들어봤을 겁니다.   
잘 작동되는 모델을 찾기 위해 많은 훈련을 거쳐야하는 반복적인 과정입니다.    
따라서 모델을 빠르게 훈련시키는 것이 매우 중요합니다.    
딥러닝이 빅데이터에서 가장 잘 작동된다는 것도 훈련을 어렵게 만듭니다.    
큰 데이터 세트에서 신경망을 훈련시킬 수 있을 때 말입니다.    
큰 데이터 세트에서 훈련하는 것은 매우 느린 과정입니다.    
따라서 좋은 최적화 알고리즘을 찾는 것은 여러분의 효율성을 좋게 만들어 줄 것입니다.    
미니배치 경사 하강법에 대해 함께 배워봅시다.    

# 아주 거대한 훈련 
![image](https://user-images.githubusercontent.com/50114210/65427743-f70a3480-de4d-11e9-9409-e981a2cf8429.png)  
전에 벡터화가 m개의 샘플에 대한 계산을 효율적으로 만들어준다는 것을 배웠습니다.    
명시적인 반복문 없이도 훈련 세트를 진행할 수 있도록 합니다. 따라서 훈련 샘플을 받아서 큰 행렬에 저장합니다.   
X = [x(1), x(2)...x(m)]이런 식으로 x(m)까지 가게 됩니다. m개의 훈련 샘플이 있는 경우에요.    
Y도 비슷하게 [y(1), y(2) ... y(m)]입니다.    
따라서 X의 차원은 (nx, m)이고, Y의 차원은 (1, m)입니다.     
벡터화는 m개의 샘플을 상대적으로 빠르게 훈련시킬 수 있지만 m이 매우 크다면 여전히 느릴 수 있습니다.      
예를 들어 만약 m이 5,000,000이거나 그보다 크다면 어떨까요?    
전체 훈련 세트에 대한 경사 하강법을 구현하면 경사 하강법의 작은 한 단계를 밟기 전에 모든 훈련 세트를 처리해야 합니다.       
또 경사 하강법의 다음 단계를 밟기 전에 다시 오백만개의 전체 훈련 샘플을 처리해야합니다.     
따라서 오백만개의 거대한 훈련샘플을 모두 처리하기 전에 경사 하강법이 진행되도록 하면    
더 빠른 알고리즘을 얻을 수 있습니다.    

# 미니배치 
![image](https://user-images.githubusercontent.com/50114210/65427914-55cfae00-de4e-11e9-8713-1031153c762e.png)  
![image](https://user-images.githubusercontent.com/50114210/65428044-96c7c280-de4e-11e9-9aba-0b8f73578944.png)    
다음과 같이 해 볼 수 있습니다.    
훈련 세트를 더 작은 훈련세트들로 나누었다고 해봅시다. 이러한 작은 훈련세트는 미니배치라고 부릅니다.    
각각의 미니배치가 1,000개의 샘플을 갖는다고 해봅시다. 따라서 x(1)부터 x(1000)까지 얻고    
첫번째 작은 훈련 세트, 즉 미니배치라고 부릅니다.    
그리고 다음 1,000개의 샘플인 x(1001)부터 x(2000)까지 얻고 이런 식으로 계속 합니다.   


# 미니배치를 나타내는 표기법
![image](https://user-images.githubusercontent.com/50114210/65428228-fd4ce080-de4e-11e9-9833-669df41d9183.png)   
새로운 표기법을 소개해드리겠습니다.    
한 미니배치를 {}에 표시하면서 X{1}, X{2}와 같이 나타냅니다.    
총 오백만 개의 훈련 샘플이 있고 각각의 미니배치는 천 개의 샘플이라면      
즉 오백만 개를 천 개로 나눈 오천 개의 미니배치가 있다는 뜻입니다.    
따라서 X{5000}으로 끝나게 됩니다. Y에 대해서도 비슷한 방식으로 훈련 데이터를 나누고    
첫번째를 Y{1}이라고 부르고 y(1)부터 y(1000)까지를 나타내게 됩니다.    
이런식으로 Y{5000}까지 계속됩니다. 미니배치의 개수 t는 X{t}와 Y{t}로 구성됩니다.    
그리고 이것은 입력, 출력 쌍에 대응하는 천 개의 훈련 샘플입니다.   

# 표기법 정리
![image](https://user-images.githubusercontent.com/50114210/65428291-181f5500-de4f-11e9-9d60-89519b5afc38.png)   
다음으로 넘어가기 전에 표기법을 정리하겠습니다.    
훈련 세트를 색인하기 위해 (i)를 사용했습니다. x(i)는 i번째 훈련 샘플입니다.    
[L]은 신경망의 서로 다른 층을 색인하기 위해 사용했습니다.     
따라서 z[L]은 L번째 신경망의 z값을 나타냅니다.    
그리고 여기서 소개하는 {t}는 서로 다른 미니배치를 색인하기 위해 사용됩니다. X{t}, Y{t}처럼 말이죠.     
그렇다면 X{t}와 Y{t}의 차원은 무엇일까요? X는 (nx, m)차원입니다.    
따라서 X{1}이 천개의 훈련 샘플이라면 차원은 (nx, 1000)차원이 되어야합니다.    
Y{1}의 차원은 (1, 1000)이 되어야합니다.      

# 미니배치
![image](https://user-images.githubusercontent.com/50114210/65428822-0b4f3100-de50-11e9-957b-9117f22fc85e.png)    
배치 경사 하강법은 우리가 전에 이야기하던 경사 하강법을 말합니다.   
모든 훈련 세트를 동시에 진행시키는 방법입니다.   
이 이름은 동시에 훈련 샘플의 모든 배치를 진행시킨다는 관점에서 나왔습니다.     
이와 반대로 미니배치 경사 하강법은 전체 훈련 세트 X, Y를 한번에 진행시키지 않고,    
하나의 미니배치 X{t}, Y{t}를 동시에 진행시키는 알고리즘을 말합니다.    
미니배치 경사 하강법이 어떻게 작동되는지 살펴봅시다.    

# 알고리즘
![image](https://user-images.githubusercontent.com/50114210/65428957-481b2800-de50-11e9-8ba7-60f6f711ec53.png)    
여러분의 훈련 세트에서 미니배치 경사 하강법을 실행하기 위해서는 t = 1 ... 5000인 반복문을 돌립니다.   
각각의 크기가 1,000개인 미니배치가 5,000개 있기때문입니다.    
반복문 안에서 하려고 하는 것은 기본적으로 한 단계의 경사 하강법을 구현하는 것입니다.   
X{t}와 Y{t}를 사용해서 한 단계의 경사 하강법을 구현하는 것입니다.     
1,000개의 샘플이 있는 훈련 세트는 이미 익숙한 알고리즘을 구현하는 것이지만    
단지 m이 1,000인 작은 훈련 세트에서 구현하는 것 뿐입니다.    
모든 1,000개의 샘플에 대해 명시적인 반복문을 갖는 것보다    
벡터화를 사용해 모든 1,000개의 샘플을 동시에 진행시키게됩니다.   

# 정방향 전파
![image](https://user-images.githubusercontent.com/50114210/65429073-83b5f200-de50-11e9-839d-296430f8ae1c.png)     
첫번째로 입력 X{t}에 대해 정방향 전파를 구현하게 됩니다. 이를 위해 z[1] = W[1]X{t}를 구현합니다.    
전에는 X를 곱해주었지만 전체 훈련 세트를 진행하는 것이 아닌 첫번째 미니배치를 진행하기 때문에   
X{t}를 곱해주게 됩니다. 그리고 A[1]은 g[1](Z[1])입니다.      
벡터화된 구현이기 때문에 Z를 대문자로 써줬습니다. 이런 식으로 A[L] = g[L](Z[L])까지 진행합니다.     
이것이 여러분의 예측입니다. 여기에 벡터화된 구현을 사용해야 한다는 것을 알겠죠?    
이 벡터화된 구현이 동시에 오백만개의 샘플 대신에 1,000개의 샘플을 진행하기 때문입니다.   


# 비용함수
![image](https://user-images.githubusercontent.com/50114210/65429185-be1f8f00-de50-11e9-932a-f19917572bec.png)     
다음은 비용함수 J를 계산하겠습니다. 작은 훈련 샘플의 크기가 1,000이므로 1/1000으로 쓰겠습니다.   
i가 1부터 L까지 y^(i)의 예측값과 y(i)를 매개변수로 하는 손실함수의 합을 곱해줍니다.   
명확성을 위해 이 표기법은 미니배치 X{t}, Y{t}에서 온 샘플을 말합니다.   
정규화를 사용한다면 이 정규화 항을 사용할 수도 있습니다.    
분모에 2를 곱하고 L에 대한 프로베니우스 노름의 제곱의 합을 곱해줍니다.    
하나의 미니배치에 대한 비용이기 때문에 J에 위 첨자 {t}를 색인하도록 하겠습니다.   
모든 것은 전에 했던 경사 하강법을 구현하는 것과 정확히 같지만 
X, Y대신에 X{t}, Y{t}에 한다는 점만 다릅니다.     

# 역전파
![image](https://user-images.githubusercontent.com/50114210/65433590-49e8e980-de58-11e9-82ef-3938496f2fa8.png)     
다음은 역전파를 구하겠습니다. J{t}에 대응하는 경사를 계산하기 위한 것입니다.     
여전히 X{t}와 Y{t}를 사용합니다. 그리고 가중치를 업데이트합니다.   
모든 W[L]은 W[L] - 알파dW[L]로 업데이트 됩니다. b도 비슷하게 업데이트 됩니다.       
따라서 이것은 미니배치 경사 하강법을 사용한 훈련 세트를 지나는 한번 반복입니다.     

### 1 에폭
![image](https://user-images.githubusercontent.com/50114210/65433635-5a995f80-de58-11e9-938f-dafdcbdeb303.png)   
제가 여기에 작성한 코드는 훈련의 한 에포크를 거친다고도 말할 수 있습니다.   
여기서 에포크는 훈련 세트를 거치는 한 반복을 의미합니다.     
따라서 배치 경사 하강법에서 훈련 세트를 거치는 한 반복은      
오직 하나의 경사 하강 단계만을 할 수 있게 합니다.     
5,000개의 경사 하강 단계를 거치도록 합니다.     

# 아웃트로
또 다른 반복문을 사용해서 훈련 세트를 여러번 거치면      
원하는 만큼 계속 거의 수렴할 대까지 훈련 세트를 계속 반복시키게 됩니다.     
훈련 세트가 많다면 미니배치 경사 하강법이 훨씬 더 빠르게 실행됩니다.        
배치 경사 하강법보다 말이죠. 많은 데이터 세트를 훈련시킬 때 거의 모든 사람들이 사용하는 방법입니다.    






