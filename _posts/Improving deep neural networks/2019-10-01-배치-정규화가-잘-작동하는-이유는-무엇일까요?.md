---
layout: post
title: "배치 정규화가 잘 작동하는 이유는 무엇일까요?"
tags: [Batch Normalization, Regularization, Dropdout]
categories: [Improving deep neural networks]
---

# 학습 목표
배치 정규화가 잘 작동하는 원인을 살핀다.

# 핵심 키워드
* 배치 정규화(Batch Normalization)
* 정규화(Regularization)
* 드롭아웃(Dropout)

# 학습 내용
* 배치 정규화는 입력특성 X의 평균을 0, 분산을 1로 만듦으로써 학습 속도를 빠르게 합니다.
* 배치 정규화가 잘 되는 이유중 하나는 이전 층의 가중치 영향을 덜 받게 하는데 있습니다. 은닉층 값의 분포 변화를 줄여줘서, 입력 값의 분포르 ㄹ제한하기 대문입니다. 즉, 배치 정규하ㅗ는 입력값이 바뀌엇 ㅓ발생하는 문제를 안정화 시킵니다. 앞층과 뒷층의 매개변수의 상관관를 줄여주기 때문에, 학습속도를 향상시킬 수 있습니다.
* 배치 정규화의 또 다른 효과는 파라미터의 정규화(Regularization)입니다. 미니배치로 계산한 평균과 분산은 전체 데이터의 일부로 추정한 것이기 때문에 잡음이 많이 끼어있습니다.
* 드롭아웃의 경우 은닉유닛에 확률에 따라 0혹은 1을 곱하기 때문에 곱셈 잡음이 있습니다. 배치 정규화의 경우 곱셉 잡음과 덧셈 잡음이 동시에 있습니다. 따라서 약간의 정규화 효과가 있습니다.
* 은닉층에 잡음을 추가한다는 것은 이후 은닉층이 하나의 은닉 유닛에 너무 의존하지 않도록 만듭니다.
* 큰 미니배치를 사용시 이 정규화 효과는 상대적으로 약해집니다.

# 인트로
어떻게 배치 정규화가 작동하는 걸까요?         
첫 번째 이유입니다. 입력 특성 X를 평균 0, 분산 1로 정규화하는 것이        
학습 속도를 올리는 것을 보았습니다.      
어떤 특성은 0에서 1까지 어떤 특성은 1에서 1,000까지인 것을          
입력 특성 X에 대해 비슷한 범위를 갖도록 정규화하여 학습 속도를 높이는 것이지요.         
그래서 우리는 배치 정규화가 작동하는 이유가      
비슷한 일을 하기 때문이라는 직관을 얻었습니다.         
은닉 유닛과 입력층 모두에서 말이죠.           
하지만 이건 부분적인 그림에 불과합니다.          
이것 외에도 배치 정규화가 뭘 하는 것인지          
더욱 깊이 이해할 수 있는 몇 가지 직관이 더 있습니다.    
이 영상에서 그 내용을 살펴보겠습니다.         

# 검은 고양이를 구분하는 신경망
![image](https://user-images.githubusercontent.com/50114210/65932936-8e711800-e44a-11e9-9ce6-ba6d806871c0.png)    
배치정규화가 작동하는 두 번째 이유는         
신경망에서 깊은, 예컨대 10번 층의 가중치가         
1번 층처럼 앞쪽 층의 가중치의 변화에 영향을 덜 받는다는 것입니다.         
제 말을 쉽게 이해할 수 있도록 예시를 준비했습니다.        
고양이 분류를 위해 신경망을 학습한다고 합시다.            
로지스틱 회귀처럼 얕거나 깊을 수도 있겠죠.         
그리고 검정 고양이의 이미지만 써서 학습시켰다고 합시다.     

# 다른 데이터 셋에 사용한다면
![image](https://user-images.githubusercontent.com/50114210/65932961-a779c900-e44a-11e9-93c2-15925024db9a.png)      
그리고 이 신경망을 다양한 색깔의 고양이에 적용해봅시다.        
왼쪽처럼 검정 고양이가 아니라 오른쪽에 있는 유색 고양이가 정답인 경우         
신경망이 좋은 성능을 못 낼 겁니다.         

# 정답의 분포
![image](https://user-images.githubusercontent.com/50114210/65932994-cd06d280-e44a-11e9-9117-02720f2959ec.png)     
정답과 오답이 이렇게 분포되어있다고 합시다.           
그리고 이걸 일반화해서 이런 정답, 오답 분포에도 적용하는 걸 생각해보면          
왼쪽 데이터로 학습시킨 모델이 오른쪽 데이터에서 좋은 성능을         
못 낼 것이라고 추측할 수 있습니다.         
사실 두 함수는 이렇게 동일하지만 왼쪽 데이터에만         
학습 알고리즘을 적용해서 얻은 초록색 결정 기준에 좋은 성능을 기대하기는 어렵습니다.          

# 공변량 변화(Coveriate Shift)
![image](https://user-images.githubusercontent.com/50114210/65933017-e445c000-e44a-11e9-8287-df2d6d79633b.png)       
이처럼 데이터 분포가 변화하는 것을 좀 어려운 말로 공변량 변화라고 부릅니다.           
이 아이디어는 X, Y 간의 대응을 학습시킬 때 X의 분포가 바뀐다면        
학습 알고리즘을 다시 학습해야한다는 겁니다.        
이 예시의 고양이 구분하기처럼 X에서 Y로 대응하는 관측 함수가 바뀌지 않더라도 말이죠.         
여러분이 새롭게 함수를 학습시킨다면 더 정확해지거나        
관측 함수가 함께 움직여서 더 나빠질 수도 있습니다.      
그럼 공변량 변화가 신경망에 어떻게 적용될 수 있을까요?       

# 신경망을 학습시키자
![image](https://user-images.githubusercontent.com/50114210/65933101-3e468580-e44b-11e9-8eda-4b40d17ef3d6.png)    
이런 심층신경망을 생각해봅시다.        
그리고 이 세 번째 은닉층의 관점에서 학습 과정을 살펴봅시다.          
이 신경망은 w^[3]와 b^[3]를 학습시키고 있습니다
세 번째 은닉층은 앞선 층에서 값들을 받아오고요.            
그리고 관측값인 Y와 가까운 y^과 관련 있는 일을 할 겁니다.         

# 왼쪽 부분을 가린다고 생각해보자
![image](https://user-images.githubusercontent.com/50114210/65933145-561e0980-e44b-11e9-8083-c599ffcd8b64.png)          
잠시 왼쪽 부분을 가려볼게요. 세 번째 층이 값들을 받아오겠죠?         
그것들을 a^[2]_1, a^[2]_2, a^[2]_3, a^[2]_4라고 합시다.         
특성 값인 x_1, x_2, x_3, x_4가 될 수도 있겠죠.           
이 세 번째 층이 할 일은 값을 받아와서 ŷ으로 대응시키는 것입니다.         
경사하강법을 써서 w^[3]와 b^[3]나 w^[4]와 b^[4]           
나아가 w^[5]와 b^[5]를 신경망이 좋은 성능을 내도록 학습시키는 거죠.       
왼쪽에 가려져 있는 값들로부터 출력값인 ŷ을 만들어내는 겁니다.          
이제 신경망의 왼쪽을 다시 벗깁시다. 신경망은 매개변수 w^[2], b^[2]와         
w^[1], b^[1]도 학습시키고 있습니다.          
이 매개변수의 값이 바뀌면 여기 a^[2] 값들도 바뀌겠죠.           
따라서 세 번째 은닉층의 관점에서 이 은닉층의 값들이 계속 바뀌고 있습니다.        
따라서 앞서 살펴봤던 공변량 변화의 문제를 계속 겪게 되는 것이죠.         

# 공변량 변화를 줄여주는 배치 정규화
![image](https://user-images.githubusercontent.com/50114210/65933314-ff64ff80-e44b-11e9-907d-436d39f6fa3e.png)       
여기에서 배치 정규화는 은닉층 값들의 분포가 변화하는 양을 줄여줍니다.          
만약 이 은닉층들의 값의 분포를 그린다면        
여기서는 재정규화된 z인 z^[2]_1과 z^[2]_2를 씁시다.         
값 네 개 대신 두 개를 사용해서 2차원에 나타낼 수 있겠죠.          
여기서 배치 정규화가 말하고자하는 것은           
z^[2]_1과 z^[2]_2가 바뀔 수 있을 거고          
신경망이 앞선 층의 매개변수를 새로 고치면서 아마 바뀔 겁니다.       
여기서 배치 정규화는 얼마나 바뀌든지 간에         
z^[2]_1과 z^[2]_2의 평균과 분산이 동일하게 유지될 거라는 것을 말합니다.           
즉 z^[2]_1과 z^[2]_2의 값이 바뀌더라도         
적어도 평균과 분산은 0과 1처럼 유지될 거라는 거죠.        

# b, y를 써서 다양한 평균과 분산을 가지게하기
![image](https://user-images.githubusercontent.com/50114210/65933296-f5db9780-e44b-11e9-831d-7e164f8e8897.png)     
굳이 0과 1이 아니더라도 β^[2]나 γ^[2]와 같은 값도 가능합니다.           
신경망이 평균과 분산으로 0과 1을 가질 수도 있지만        
충분히 다른 값도 가능하다는 거죠.           
즉 배치 정규화가 하는 일은 앞선 층에서의 매개변수가 바뀌었을 때        
세 번째 층의 값이 받아들여서 학습하게 될 값의 분포를 제한하는 것입니다.            
즉 배치 정규화는 입력값이 바뀌어서 발생하는 문제를 더욱 안정화시키는 것이지요.         
뒤쪽 층은 당연히 더 쉽게 학습할 수 있을 겁니다.       
입력 값의 분포가 조금 바뀌더라도 조금만 바뀌는 것이죠.        
즉 앞쪽 층이 계속 학습하면서 값을 바꾸더라도         
뒤쪽 층이 그것 때문에 겪는 부담을 줄이는 것입니다.         
그리고 이것은 또 앞쪽 층의 매개변수와 뒤쪽 층의 매개변수 간의 관계를 약화시킵니다.          
따라서 신경망의 각 층이 스스로 배울 수 있게 되죠. 다른 층과 상관 없이 말이죠.         

# 배치 정규화는 학습을 쉽게 해준다.
이것을 통해서 전체 신경망의 학습 속도를 상승시킬 수 있습니다.         
여러분에게 좋은 직관이 되었기를 바랍니다.        
반드시 알아두어야 할 것은 배치 정규화에 의해 신경망에서 뒤쪽에 있는 층의 관점에서       
앞선 층이 너무 많이 변화하지 않는다는 것입니다.         
왜냐하면 평균과 분산이 일정하도록 제한받았으니까요.         
이를 통해 뒤쪽 층의 학습이 더욱 용이합니다.         

# 각 배치마다 다른 값이 적용된다.
![image](https://user-images.githubusercontent.com/50114210/65933586-05a7ab80-e44d-11e9-81cb-39a50734a247.png)                   
그리고 배치 정규화의 두 번째 효과가 있습니다. 규제 효과인데요.         
배치 정규화에서 다소 비직관적이죠.          
각각의 미니 배치 X^{t}가 가진 z^[l]에 대해서       
그 미니 배치의 평균과 분산에 따라 값을 조정할 것입니다.         
여기서 미니 배치에서 계산한 평균과 분산은 전체 데이터로부터 계산한 것에 비해
다소 잡음을 갖고 있습니다.          
왜냐하면 64, 128, 256 내지는 더 큰 크기의 훈련 예시를 지닌 미니 배치에 대해          
상대적으로 작은 데이터에 대해서 추정한 것이니까요.         

# 각 배치 정규화 과정마다 잡음이 생긴다.
![image](https://user-images.githubusercontent.com/50114210/65933639-469fc000-e44d-11e9-847c-5d3cf9c91f1f.png)            
여기서 z^[l]에서 z~[l]으로 조정하는 과정 역시 잡음이 끼어있겠죠.           
왜냐하면 잡음이 끼어있는 평균과 분산으로 계산하니까요.          
즉 드롭아웃처럼 은닉층의 활성 함수에 잡음이 끼어 있습니다.           
드롭아웃에서는 은닉층을 가져와서 확률에 따라 0을 곱하거나 확률에 따라 1을 곱했지요?             
따라서 드롭아웃은 곱셈 잡음을 갖고 있습니다. 0이나 1을 곱하니까요.         
반면에 배치 정규화는 표준편차로 나누니 곱셈 잡음도 있고        
평균을 빼니 덧셈 잡음도 있습니다.       
여기서 평균과 표준편차의 추정치에는 잡음이 다소 끼어있고요.          

# 약간의 일반화 효과를 가진 배치 정규화
![image](https://user-images.githubusercontent.com/50114210/65933708-9a120e00-e44d-11e9-8d32-a698bd3a3216.png)        
따라서 드롭아웃처럼 배치 정규화는 약간의 일반화 효과를 갖고 있습니다.        
왜냐하면 은닉층에 잡음을 추가하는 것은        
이후의 은닉층이 하나의 은닉층에 너무 의존하지 않도록 하거든요.         
따라서 드롭아웃처럼 은닉층에 잡음을 추가해서 아주 약간의 일반화 효과를 보여줍니다.         
여기서 잡음이 아주 작다보니 일반화 효과가 그리 크지는 않습니다.        
그래서 배치 정규화와 드롭아웃을 함께 사용할 수도 있겠죠.           
더욱 강력한 일반화를 원한다면 말이죠. 

# 큰 미니배치를 사용하면 일반화 효과가 줄어듬
그리고 다른 비직관적인 효과를 봅시다. 여러분이 큰 미니 배치를 사용한다고 해보죠.          
64 대신 512 크기를 사용한다고 해봅시다. 큰 미니배치를 사용하면 잡음이 줄어들고         
따라서 일반화 효과도 줄어들 것입니다. 드롭아웃의 이상한 특징인데요.           
큰 미니배치를 사용하면 일반화 효과가 줄어드는 것이죠.           
따라서 저는 배치 정규화를 일반화를 목적으로 사용하지는 않습니다.         
배치 정규화는 그걸 목적으로 만들어진게 아닙니다.
하지만 의도하지 않거나, 의도한 것보다     
더 큰 효과를 학습 알고리즘에 가져올 수도 있습니다.           
하지만 배치 정규화를 일반화를 목적으로 사용하진 마세요.          
은닉층의 활성 함수를 정규화해서 학습 속도를 올리는 용도로 사용하세요.        
일반화는 여러분이 의도치 않은 부수효과에 가깝습니다.       

# 아웃트로
이 글을 통해 배치 정규화가 뭘 하는 건지 직관을 얻었기를 바랍니다.        
배치 정규화에 대한 논의를 맺기 전에 한 가지 더 알아두어야 할 게 있습니다.       
배치 정규화는 한 번에 미니배치 하나의 데이터를 다룹니다.         
미니 배치의 평균과 분산을 계산하죠.      
테스트 과정에서는 예측을 해서 신경망을 평가합니다 .          
예시의 미니 배치가 없지요. 한 번에 예시 하나씩을 처리하는 겁니다.               
따라서 테스트 과정에서는 예측이 잘 맞도록 조금 다른 접근을 해야 합니다.         
다음 글이자 배치 정규화에 관한 마지막 글에서         
예측을 할 수 있도록 배치 정규화를 이용해         
신경망을 학습시키는 방법을 다루도록 하겠습니다.         

