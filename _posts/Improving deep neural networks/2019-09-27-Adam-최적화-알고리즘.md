---
layout: post
title: "Adam 최적화 알고리즘"
tags: [Adam, Optimization]
categories: [Improving deep neural networks]
---

# 학습 목표
Adam 최적화 알고리즘을 배운다.

# 핵심 키워드
* Adam
* 최적화(Optimization)

# 학습 내용
* Adam은 Momentum과 RMSProp을 섞은 알고리즘입니다.
* 알고리즘은 아래와 같습니다.
![image](https://user-images.githubusercontent.com/50114210/65747736-23b79800-e13d-11e9-83fc-11a42ac4b313.png)           
* Adam은 Adaptive moment estimation의 약자입니다.

# 인트로 
딥러닝의 역사에서 잘 알려진 사람들을 포함한 딥러닝 연구원들은           
가끔 최적화 알고리즘을 제안하고 몇 가지 문제에 잘 작동하는지를 증명합니다.           
그러나 그런 최적화 알고리즘이 훈련을 원하는 넓은 범위의 신경망에           
일반적으로 잘 작동하지 않는다는 것을 보여주었습니다.           
따라서 딥러닝 커뮤니티는 새로운 최적화 알고리즘에 약간의 의심을 갖게 되었습니다.            
모멘텀이 있는 경사 하강법이 아주 잘 작동하기 때문에
더 잘 작동하는 알고리즘을 제안하기 어려운 것도 있구요.              
따라서 오늘 다룰 RMSprop과 Adam 최적화 알고리즘은           
넓은 범위의 딥러닝 아키텍처에서 잘 작동하는 알고리즘으로 우뚝 섰습니다.          
따라서 시도를 망설이지 않아도 되는 알고리즘입니다.             
왜냐하면 많은 사람들이 시도했고 많은 문제에 잘 작동한다는 것을 보았기 때문입니다.              
Adam 최적화 알고리즘은 RMSprop과 모멘텀을 합친 알고리즘입니다.            
어떻게 작동하는지 살펴봅시다.           

# 초기화 
![image](https://user-images.githubusercontent.com/50114210/65748127-29fa4400-e13e-11e9-8b94-abb907083d3e.png)         
Adam을 구현하기 위해서v_dw와 s_dw를 0으로 초기화합니다 .           
v_db와 s_db도 0으로 초기화합니다.                

![image](https://user-images.githubusercontent.com/50114210/65748159-3c747d80-e13e-11e9-9463-2519dc23f4f4.png)        
그리고 반복 t에서 도함수 dw와 db를 현재의 미니배치를 써서 계산합니다.          
주로 미니배치 경사 하강법을 사용합니다.           

# Momentum 부분
![image](https://user-images.githubusercontent.com/50114210/65748241-6a59c200-e13e-11e9-90f7-21360c9d29eb.png)         
그리고 모멘텀 지수가중평균을 계산합니다.           
따라서 v_dw는 RMSprop의 하이퍼파라미터와            
구분하기 위해 β_1이라고 부르겠습니다.               
따라서 이것은 모멘텀을 구현할 때 사용하는 식입니다.           
β 대신에 하이퍼파라미터로 β_1을 사용한 점만 다릅니다.            
비슷하게 v_db는 다음과 같습니다 (1-β_1)*db        

# RMSProp 부분
![image](https://user-images.githubusercontent.com/50114210/65748273-80678280-e13e-11e9-9cf8-76d171f665a4.png)          
RMSprop의 업데이트는 다음과 같습니다.           
이번에는 하이퍼파라미터 β_2를 사용하고           
여기 제곱은 dw의 요소별 제곱입니다.          
s_db = β_2*s_db + (1-β_2)db입니다.            
따라서 이것은 하이퍼파라미터 β_1을 사용한 모멘텀 업데이트고            
이것은 하이퍼파라미터 β_2를 사용한 RMSprop 업데이트입니다.     

# 편향보정을 하는 Adam
![image](https://user-images.githubusercontent.com/50114210/65748296-9117f880-e13e-11e9-938a-f5ddec8d8a51.png)           
전형적인 Adam의 구현에서는 편향 보정을 합니다.            
v_dw^corrected는 편향 보정을 의미합니다.          
이 값은 v_dw/(1-β_1^t)와 같습니다.           
비슷하게 v_db^corrected는 v_db/(1--β_1^t)와 같고,       
비슷하게 s_dw에 대한 편향 보정 역시 s_dw/(1--β_2^t)와 같습니다.           
s_db^corrected는 s_db/(1--β_2^t)와 같습니다.             

# 가중치 업데이트 
![image](https://user-images.githubusercontent.com/50114210/65748331-a12fd800-e13e-11e9-9118-80dcbcef8bd0.png)           
최종적으로 업데이트를 실행합니다.            
W는 W-α 곱하기 모멘텀을 구현하고 있기 때문에 v_dw^corrected이고             
RMSprop을 부분적으로 추가하기 때문에            
s_dw^corrected의 제곱근+ε로 나눠줍니다.           
b도 비슷한 방식으로 b - α 곱하기 v_db^corrected를          
s_db^corrected의 제곱근 + ε입니다.             
s_db^corrected의 제곱근 + ε으로 나눠줍니다.           
따라서 이 알고리즘은 모멘텀이 있는 경사 하강법의 효과와            
RMSprop이 있는 경사 하강법의 효과를 합친 결과가 나옵니다.          

# 최종적인 구현
![image](https://user-images.githubusercontent.com/50114210/65748371-c1f82d80-e13e-11e9-99a9-66a2fff79914.png)      

# 하이퍼파라미터
![image](https://user-images.githubusercontent.com/50114210/65748423-e227ec80-e13e-11e9-9905-9ea8baa73a24.png)           
이것은 매우 넓은 범위의 아키텍처를 가진       
서로 다른 신경망에 잘 작동한다는 것이          
증명된 일반적으로 많이 쓰이는 학습 알고리즘입니다.             
이 알고리즘은 많은 하이퍼파라미터가 있습니다.         
학습률 하이퍼파라미터 α는 매우 중요하고 보정될 필요가 있으므로             
다양한 값을 시도해서 잘 맞는 것을 찾아야 합니다.         
β_1의 기본적인 값으로 0.9를 보통 선택합니다.           
이것은 dw의 이동평균, 가중평균입니다. 모멘텀에 관한 항입니다.            
β_2에 대한 하이퍼파라미터는 Adam 논문에서 저자가 추천하는 값이 0.999입니다.                 
이것은 dw^2와 db^2의 이동가중평균을 계산한 것입니다. 그리고 ε입니다.             
이 값은 크게 상관 없습니다만 Adam 논문의 저자에 따르면 10^(-8)을 추천합니다.             
그러나 이 값을 설정하지 않아도 전체 성능에는 영향이 없습니다.            
그러나 Adam을 구현할 때 보통 사람들은 β_1과 β_2, 그리고 ε도 기본값을 사용합니다.                  
ε의 값을 보정하는 사람은 본 적이 없습니다.               
보통 α에 여러 값을 시도해 가장 잘 작동되는 값을 찾습니다.             
β_1과 β_2도 보정할 수 있지만 자주 하지는 않습니다.             

# 아담의 뜻
![image](https://user-images.githubusercontent.com/50114210/65748443-ec49eb00-e13e-11e9-9b6a-e8b84a00189f.png)         
Adam이라는 용어는 어디서 온 것일까요?          
Adam은 Adaptive moment estimation에서 온 용어입니다.             
β_1이 도함수의 평균을 계산하므로 이것이 첫 번째 모멘트이고    .               
모두 그냥 Adam 최적화 알고리즘이라고 부릅니다.        

# 아웃트로
Adam 최적화 알고리즘이었습니다.         
신경망을 더 빠르게 훈련시킬 수 있을 것입니다.         

다음 비디오에서는 학습률

감소를 말씀드리겠습니다

