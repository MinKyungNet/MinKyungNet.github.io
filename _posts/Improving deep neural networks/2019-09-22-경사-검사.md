---
layout: post
title: "경사 검사"
tags: [gradient checking]
categories: [Improving deep neural networks]
---

# 학습 목표
경사 검사의 구현하는 방법을 알 수 있다.

# 핵심 키워드
경사 검사(gradient checking)

# 학습 내용
* 우선, 모델 안에 있는 모든 변수(W, b)를 하나의 벡터(세타)로 concatenate합니다.     
* 그러면 비용 함수는 J(W, b)에서 J(세타)로 변홥니다.     
* 그후, 수치 미분을 구합니다.        
![image](https://user-images.githubusercontent.com/50114210/65389261-4e959b00-dd8f-11e9-8b8a-2cf196bb8d21.png)        
* 최종적으로 수치 미분과 일반 미분을 비교합니다.       
![image](https://user-images.githubusercontent.com/50114210/65389268-6cfb9680-dd8f-11e9-8857-5782b01d542d.png)    
* 유사도를 계산하는 방법은 유클리디안 거리를 사용합니다.        
![image](https://user-images.githubusercontent.com/50114210/65389280-84d31a80-dd8f-11e9-957e-09de332af3ba.png)
* 보통 거리가 10^-7보다 작으면 잘 계산되었다고 판단합니다.

# 큰 벡터 세타
![image](https://user-images.githubusercontent.com/50114210/65389850-2dd04400-dd95-11e9-84c6-ec7a738f1df8.png)     
신경망은 매개변수 W[1], b[1]부터 W[L], b[L]까지 가지고 있습니다.    
경사 검사를 위한 첫번째 단계는 이 매개변수들을 하나의 큰 벡터로 세타로 바꾸는 것입니다.    
행렬 Wp1[을 벡터 크기로 바꿉니다. 이어서 모든 W행렬을 받아서 벡터로 바꾸고 모두 연결시킵니다.     
그 결과 매우 큰 벡터 매개변수 세타를 얻게됩니다.    

### 비용함수
![image](https://user-images.githubusercontent.com/50114210/65389863-3de82380-dd95-11e9-9ebc-f16016ab59bd.png)      
비용함수 J를 W와 b의 함수로 만드는 대신에 세타의 함수가 되도록합니다.    
W와 b에서 했던것과 같은 순서로 dW[1], db[1], ... dW[L], db[L]의 매개변수를 매우 큰 벡터 d세타로 만듭니다.     
세타와 같은 차원을 가지고 있습니다. 방금과 같은 방식으로 매우 큰 d세타 백터를 만듭니다.    

#### 여기서 포인트는 d세타가 비용함수 J(세타)의 기울기인가입니다.    

# grad check
![image](https://user-images.githubusercontent.com/50114210/65389892-696b0e00-dd95-11e9-95ad-c2f147bf9059.png)       
J는 이제 매우 큰 매개변수 세타에 관함 함수입니다.      
J(세타)를 J(세타1, 세타2, 세타3...)으로 확장시켜도 됩니다.   

### 경사 검사
![image](https://user-images.githubusercontent.com/50114210/65389903-8273bf00-dd95-11e9-91f5-5988d96c871e.png)     
매우 큰 매개변수 벡터 세타의 차원이 어떤든지 말입니다. 경사 검사를 구현하기 위해 반복문을 구현합니다.     
세타의 요소에 각각에 대하여 d세타approx[i]를 계산해봅시다.      
양 쪽 차이를 이용해 J(세타1, 세타2, ..., 세타i+입실론....)살짝 이동시키겠습니다.     
다른 것은 그대로 두고 세타i에만 입실론을 더해줍니다.     
양 쪽 차이를 취하기 때문에 다른 쪽도 똑같이 하는데 세타i-입실론을 이용합니다.   
세타의 다른 요소들은 그대로 둡니다. 그리고 이 값을 2입실론으로 나눠줍니다.     

### 경사가 근사한가?
![image](https://user-images.githubusercontent.com/50114210/65389915-961f2580-dd95-11e9-9b7c-ab5e8ebaad63.png)     
이전 비디오에서 본 것은 이 값이 근사적으로 d세타i와 같아야합니다.    
이것은 함수 J의 세타i에 대한 편미분과 같습니다. 만약 d세타i가 J의 도함수라면 말입니다.     
이 값을 모든 i의 값에 대하여 계산하게 됩니다.     
마지막에 두 개의 벡터로 계산이 마무리됩니다. d세타approx 그리고 이것은 d세타와 같은 차원이 됩니다.     
이 두 가지 모두 세타와 같은 차원입니다. 이 두 벡터가 근사적으로 같은지 확인해야 합니다.     

### 두 벡터의 체크
![image](https://user-images.githubusercontent.com/50114210/65389947-c2d33d00-dd95-11e9-9102-1cdf9d128b18.png)     
두 벡터가 꽤 가까운지 어떻게 정의할 수 있을까요? 이 두 벡터의 유클리드 거리를 계산하도록 하겠습니다.    
d세타approx - d세타의 L2노름을 구합니다. 이 값을 제곱하지 않는다는 것을 명심하세요.     
유클리드 거리를 얻기 위해서는 이 원소의 차이를 제곱한 것의 합의 제곱근을 구해야합니다.    
벡터의 길이로 정규화하기 위해 ||d세타approx|| + ||d세타||의 유클리드 길이로 나눠줍니다.    
이 벡터가 아주 작거나 큰 경우에 대비해 분모는 이 식을 비율로 바꿉니다.    
![image](https://user-images.githubusercontent.com/50114210/65389964-df6f7500-dd95-11e9-9a9c-a1daee155fd8.png)     
저는 입실론을 10^-7로 사용합니다. 이 범위의 입실론에서 이 수식은 10^-7이나 그 보다 작은 결과가 됩니다.    
이것은 근사가 매우 잘 되었다는 뜻입니다. 아주 작은 값이기 때문입니다.     
만약 10^-5라면 자세히 살펴보겠지만 이 값도 괜찮을 것입니다.     
그러나 벡터의 원소를 이중으로 확인해볼 것입니다. 너무 큰 원소가 있는 것은 아닌지 말입니다.    
원소의 차이가 너무 크다면 버그가 있을 수도 있습니다.    
만약 왼쪽 수식이 10^-3을 결과로 내놓는다면 버그의 가능성이 커서 더 자세히 살펴볼 것입니다.     
10^-3보다 더 작은 값이 나오도록 하는 것이 좋습니다.   
만약 10^-3보다 큰 값이라면 우려할만한 상황입니다.    
그럼 버그가 있을 가능성이 크기 때문에 세타의 개별적인 원소를 신중하게 살펴서    
특정 i에 대해 d세타approx[i]와 d세타[i]의 차이가 심한 값을 추적해서      
미분의 계산이 옳지 않은 곳이 있는지 확인해야 합니다.    
디버깅이 끝나고 이 정도의 작은 값이 나온다면 올바른 구현을 한 것일것입니다.    
따라서 신경망의 정방향전파 혹은 역전파를 구현할 때 경사 검사에서 상대적으로 큰 값이 나온다면    
버그의 가능성을 의심해야합니다.      
디버깅의 과정을 거친 후 경사검세에서 작은 값이 나온다면 구현에 대한 자신감을 가져도 좋습니다.    







