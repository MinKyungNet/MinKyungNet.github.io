---
layout: post
title: "Softmax 분류기 훈련시키기"
tags: [Softmax, Loss function]
categories: [Improving deep neural networks]
---

# 학습 목표
Softmax 층을 써서 모델을 학습시키는 법을 배운다.

# 핵심 키워드
* Softmax
* 손실함수(Loss function)

# 학습 내용
* 학습을 위한 손실함수는 다음과 같습니다.       
![image](https://user-images.githubusercontent.com/50114210/66050126-8f4b9c00-e567-11e9-956e-a3ab8c42c700.png)              
  - y = [0, 1, 0, 0]
  - y^ = [0.3, 0.2, 0.1, 0.4]
  - 두번째를 제외한 나머지 yj값은 0이기 때문에 -log(y2)값만 남을 것 입니다.
  - 즉 이 값을 최소화하여 클래스2가 될 확률을 최대화 시키는 것입니다.
* Softmax와 손실함수를 결합한 역전파의 값은 아래와 같습니다.
  - dz[L] = y^ - y

# 인트로
지난 영상에서 소프트맥스 층과 소프트맥스 활성화 함수에 대해 배웠습니다.               
이번 영상에는 소프트맥스 분류에 대해 더 깊이 이해하고               
소프트맥스 층을 써서 모델을 학습시키는 법을 배워봅시다.                

# 소프트맥스 결과
![image](https://user-images.githubusercontent.com/50114210/66051217-7ba13500-e569-11e9-9958-85abc372ea08.png)            
지난 예시에서는 위쪽 층에서 z^[L]을 이렇게 계산했던 것 기억하나요?               
여기서 C=4 개의 클래스를 다루니 z^[L]은 (4,1) 벡터입니다.           
그리고 원소 단위로 e를 취해 임시 변수 t를 얻었습니다.              
끝으로 활성화 함수인 g^[L]은 소프트맥스 활성화 함수였죠?             
이 함수는 t를 합이 1이 되도록 정규화시켰습니다.             
이것이 결국 a^[L]과 같죠.                
여기서 z^[L]의 가장 큰 원소가 5였고 가장 큰 확률도 첫 번째 확률입니다.                  

### 하드맥스
![image](https://user-images.githubusercontent.com/50114210/66051262-94114f80-e569-11e9-9aa5-97d45fcf1709.png)               
소프트맥스라는 이름은 하드맥스와 반대되는 뜻을 갖는데요.                
하드맥스는 z 벡터를 받아와서 이런 벡터로 대응시키죠.                 
하드맥스는 z의 원소를 살펴보고 가장 큰 값이 있는 곳에 1을              
나머지에는 0을 갖는 벡터로 대응시키는 겁니다.               
아주 단호한 느낌이죠? 가장 큰 원소만 1이고 나머지는 0이니까요.                
소프트맥스는 반면에 부드러운 느낌으로 z를 이런 확률들로 대응시킵니다.              
좋은 이름인지는 모르겠지만 왜 소프트맥스라고 부를까 생각해보면                   
이런 직관을 얻을 수 있습니다. 하드맥스와는 반대되는 것이죠.               

# 로지스틱 회귀를 일반화한 소프트맥스
![image](https://user-images.githubusercontent.com/50114210/66051293-a4292f00-e569-11e9-9bb8-7b59ef9b5ef5.png)            
제가 꼭 보여드리고 싶은 것은 소프트맥스 회귀나 활성화 함수가              
두 클래스만 다루는 로지스틱 회귀를 일반화했다는 겁니다.            
만약 C=2라면 즉 소프트맥스에서 C가 2라면            
결국 로지스틱 회귀와 같아집니다.              
이 영상에서 따로 증명하지는 않을 건데요.               
대략 증명의 흐름을 보자면 C=2에서 소프트맥스를 적용했을 때               
출력층 a^[L]은 C=2에서 출력값 두 개를 모아둔 거겠죠.              
0.842와 0.158이라고 하면요. 이 두 숫자의 합은 항상 1이 됩니다.                 
이 두 숫자의 합이 항상 1이 되니 둘 다 계산할 필요는 없습니다.              
둘 다 계산하면 번거롭죠. 하나만 계산해도 됩니다.              
그러면 그 숫자를 계산하는 방식이         
로지스틱 회귀가 하나의 출력값을 계산하는 방식과 같습니다.             
이게 증명의 흐름이고요.            
알아두실 것은 소프트맥스 회귀가 클래스가 둘 이상인 경우               
로지스틱 회귀를 일반화한 것이라는 겁니다.               

# 신경망이 잘 작동하나?
![image](https://user-images.githubusercontent.com/50114210/66051506-fcf8c780-e569-11e9-992c-95d0f1948ceb.png)             
이제 소프트맥스 출력층을 이용해 신경망을 학습하는 법을 살펴봅시다.        
우선 신경망을 학습하기 위해 사용했던 손실 함수를 정의해보죠.             
예를 들어 봅시다. 한 샘플이 목표로 하는 출력값이            
관측을 기반으로 0, 1, 0, 0이라고 합시다.             
지난 영상을 생각해보면 이 벡터는 고양이를 뜻하죠. 클래스 1이니까요.         
그리고 우리 신경망의 출력값 y^은 다음과 같다고 합시다.           
y^은 합이 1인 확률로 구성된 벡터겠지요?             
1이 되는 걸 확인하실 수 있고 이건 곧 a^[L]과 같을 겁니다.               
여기에서 신경망이 잘 작동하는 것 같지는 않습니다.             
고양이일 확률이 20%에 불과하니까요.             
이 샘플에 대해서는 잘 작동하지 않네요.          

# 손실함수
![image](https://user-images.githubusercontent.com/50114210/66051595-18fc6900-e56a-11e9-92d4-514cbbc49d33.png)         
여기에서 신경망을 학습시키기 위한 손실 함수는 무엇일까요?              
소프트맥스 분류에서 주로 사용하는 손실 함수는             
j=1부터 4까지 일반적으로는 1부터 C까지겠죠?               
y_j * log (y^_j)의 합의 음수값입니다.            
위 샘플에서 무슨 일이 일어난건지 같이 보면서 얘기해보죠.                
여기에서 y_1=y_3=y_4=0이죠? 그리고 y_2만 유일하게 1입니다.        
이 합을 구할 때 y_j가 0이면 고려해주지 않아도 됩니다.           
따라서 유일하게 남는 항은 -y_2*log(y^_2)겠죠.              
왜냐하면 이 항들을 모두 합할 때           
j가 2인 경우를 제외하고는 모두 0이 될테니까요.          
여기에다 y_2는 1이니 결국 -log(y^_2)가 되겠네요.               
우리 학습 알고리즘이 경사하강법을 이용해서              
이 손실 함수의 값을 작게 만들려고 하겠죠?              
결국 이 값을 작게 만드는 것이고요.              
그러면 결국 y^_2의 값을 가능한 한 크게 만들어야 합니다.              
이 값들이 확률이므로 1보다 커질 수는 없죠?             
여기서 말이 되는 것이 입력값 x가 고양이의 사진이었으니            
그에 대응하는 출력값인 확률을 최대한 키워야겠죠.              
즉 일반적으로 손실 함수는          
훈련 세트에서 관측에 따른 클래스가 뭐든 간에             
그 클래스에 대응하는 확률을 가능한 한 크게 만드는 겁니다.              
통계학에서 최대 우도 추정을 본 적이 있다면         
최대 우도 추정과 비슷하다는 것을 알 수 있을 겁니다.            

# 비용함수
![image](https://user-images.githubusercontent.com/50114210/66051734-582aba00-e56a-11e9-9022-c0a06e11ece0.png)            
하나의 훈련 샘플에서 손실 함수를 봤고요.              
전체 훈련 세트에 대해 비용 함수 J는 뭘까요?            
편향 등의 매개변수를 설정할 때 비용 함수는             
여러분이 생각하는 것처럼 전체 훈련 세트에서 학습 알고리즘의          
예측에 대한 손실 함수를 합하는 겁니다. 훈련 샘플들에 대해서 말이죠.             
그리고 이 비용 함수를 최소로 하기 위해 경사하강법을 써야하죠.               

# Y와 Y^의 차원
![image](https://user-images.githubusercontent.com/50114210/66051826-81e3e100-e56a-11e9-926f-ef84681f39af.png)            
마지막으로 구현에 관해 살펴보자면            
C=4이고 y는 (4,1) 벡터인 상황에서 y^도 (4,1) 벡터죠.             
Y라는 행렬은 y^[1], y^[2]부터 y^[m]이 되겠죠?            
이렇게 수평하게 쌓습니다.            
위에 있는 샘플이 첫 번째 훈련 샘플라고 한다면           
첫 번째 열은 0 1 0 0이 되겠고요. 두 번째 샘플은 개가 될 수 있겠죠?            
세 번째 샘플은 아무 것도 아닐 수 있고요. 이렇게 계속합니다.           
그러면 Y는 (4,m) 차원의 행렬이 되겠네요.             
비슷하게 Y^은 y^(1)부터 y^(m)을 수평하게 쌓은 걸 겁니다.             
즉 이것이 y^(1)이 되겠네요.            
첫 번째 훈련 샘플에 대한 출력값이니까요.            
그러면 Y는 0.3 0.2 0.1 0.4를 갖겠고요.              
그러면 Y^도 (4,m) 차원이 될 겁니다.             

# 손실함수를 구하기 위해 사용되는 y^
![image](https://user-images.githubusercontent.com/50114210/66051907-a63fbd80-e56a-11e9-8e1c-18fe06bd7b82.png)           
끝으로 소프트맥스 출력층이 있는 경우          
경사하강법을 어떻게 구현할지 봅시다.             
이 출력층이 z^[L]을 계산하겠죠?         
(C,1) 차원이겠죠? 우리 예시에서는 (4,1) 차원이고요.               
여기에다 소프트맥스 활성화 함수를 취해서          
a^[L] 또는 y^을 얻는 겁니다.             
그러면 그 값을 이용해서 손실 함수를 계산할 수 있죠.             
이전에 신경망의 정방향 전파를 다룬 적이 있습니다.            
손실 함수를 구하는 데 썼죠?          

# 소프트맥스의 도함수
![image](https://user-images.githubusercontent.com/50114210/66051933-b22b7f80-e56a-11e9-89c5-ebe5b5190b68.png)           
역방향 전파나 경사하강법은 어떤가요?             
역방향 전파에서 초기화를 위한 핵심 단계           
핵심이 되는 식은 마지막 층에서 z^[L]의 미분이            
(4,1)벡터인 y^에서 (4,1)벡터인 y를 뺀 것과 같다는 거죠.               
클래스가 4개일 때 모두 (4,1) 벡터가 되는 겁니다.             
일반적으로 (C,1) 차원이겠죠?            
우리의 일반적인 정의를 빌리자면 여기서 dz^[L]은             
비용 함수를 z^[L]에 대해 편미분한 겁니다.                
미적분학에 능숙하다면 여러분이 직접 유도해볼 수 있습니다.              
이런 식으로 시작을 해서 신경망 전체에 대한 미분을            
역방향 전파로 구하는 거죠.            

# 딥러닝 프레임워크에서 소프트맥스
딥러닝 프레임워크에서는 주로 정방향 전파에 초점을 맞추면 됩니다.              
즉 여러분이 순방향 전파를 하는 법을 정해주면              
프레임워크가 스스로 역방향 전파를 어떻게 할지 결정해주니까요.         
여러분이 소프트맥스 분류나 회귀를 처음 한다면            
이 식을 기억해두시는 것을 추천합니다.            
예제에서는 굳이 필요하진 않지만요.           

# 아웃트로
이렇게 소프트맥스 분류가 끝났습니다.          
이제 두 클래스 중 하나가 아니라            
C개의 서로 다른 클래스 중 하나로 분류하는           
학습 알고리즘을 구현할 수 있습니다.           
다음에는 딥러닝 프레임워크를 소개하겠습니다.          
딥러닝 알고리즘을 구현하는데 더욱 효율적인 도구가 될 겁니다.           
