---
layout: post
title: "지역 최적값 문제"
tags: [Local optima, Optimization problem, Saddle point, Plateaus]
categories: [Improving deep neural networks]
---

# 학습 목표
지역 최적값과 최적화 문제를 소개한다.

# 핵심 키워드
* 지역 최적값(Local optima)
* 최적화 문제(Optimization promblem)
* 안장점(Saddle point)
* 안정지대(Plateus)

# 학습 내용
* 고차원 비용함수에서 경사가 0인경우는 대부분 지역 최적값이 아니라 대개 안장점입니다.
* 안장점으로 향하는 구간인 안정지대는 미분값이 아주 오랫동안 0에 가깝게 유지되는 지역을 말합니다.
* 대개 충분히 큰 Network 학습시 지역 최적값에 갇히는 일은 거의 없습니다.
* 안정지대의 문제점은 경사가 거의 0에 가깝기 때문에 학습속도가 느려진다는 것입니다. 또한, 다른 쪽으로 방향변환이 없다면 안정지대에서 벗어나기 어렵습니다. 이는 Adam, RMSProp등 알고리즘이 해결해줍니다. 

# 인트로
딥러닝 학문의 초기에는 나쁜 지역 최적값에 갇히는           
최적화 알고리즘을 많이 사용하곤 했습니다.            
하지만 딥러닝 이론이 계속 발전하면서 지역 최적값에 대한 이해도 바뀌었습니다.            
이제 지역 최적값에 대해 이제는 어떻게 생각하고           
딥러닝 문제 안의 최적화 문제를 소개하겠습니다.            

# 지역 최적값
![image](https://user-images.githubusercontent.com/50114210/66182779-58819d00-e6b1-11e9-8805-ab4076f47b1b.png)         
사람들이 지역 최적값에 대해 고민할 때 이 그림을 생각합니다.                
여러분이 w_1과 w_2라는 매개변수를 최적화한다고 할 때                
이 면적의 높이가 비용 함수가 되겠네요.              
이 그림에서는 지역 최적값이 많아 보입니다.             
이런 곳들에서 경사하강법 등의 알고리즘이          
전역 최적값에 도달하기 전 지역 최적값에 갇혀버리기 십상이죠.               
이렇게 2차원에서 그림을 그린다면
서로 다른 지역 최적값이 많은 그림을 쉽게 접할 수 있습니다.             
이렇게 낮은 차원의 그림을 통해서 직관을 얻곤 하죠.                 
하지만 직관이 항상 옳지만은 않습니다.           

# 안장점
![image](https://user-images.githubusercontent.com/50114210/66182815-6fc08a80-e6b1-11e9-96b4-e7f9039cd0a8.png)          
경사가 0인 점은 대부분 지역 최적값이 아니라            
비용 함수의 경사가 0인 경우 대개 안장점입니다.              
역시 경사가 0인 점 중에 하나죠.             
마찬가지로 w_1과 w_2일테고 높이는 비용 함수 J입니다.               
사실 고차원의 함수에서 경사가 0이면             
각 방향에서 볼록 함수나 오목 함수가 되기 마련이죠.          
예를 들어 20,000 차원의 공간에서 지역 최적값이 되기 위해서는             
20,000개의 방향이 모두 이렇게 생겨야 합니다.                
그런 일이 일어날 확률은 매우 낮죠. 아마 2^(-20,000)쯤 되겠죠?             
대신 어떤 방향에서는 위로 굽어져있고            
어떤 방향에서는 아래로 굽어져있는 형태가 주로 발생할 겁니다.              
모두 위로 굽어있는 것보다는요.            
따라서 고차원 공간에서는 오른쪽 그림처럼             
지역 최적값보다 안장점이 되기 쉽습니다.               
여기서 왜 안장점이라고 불리냐하면            
이 모양이 말에 얹는 안장과 비슷하기 때문이죠.                 
따라서 경사가 0인 이 점을 안장점이라고 부릅니다.               
이 점이 경사가 0이라는 것을 알아두기 바랍니다.              

# 직관
![image](https://user-images.githubusercontent.com/50114210/66182828-79e28900-e6b1-11e9-8c76-35c7100d259d.png)            
딥러닝의 역사에서 배울 수 있는 것은              
왼쪽 그림처럼 낮은 차원의 공간에서 얻었던 직관이            
학습 알고리즘이 높은 차원에서 돌아갈 때 적용되지 않을 수 있단 거죠.               
왜냐하면 20,000 개의 매개변수가 있을 때             
J는 20,000 차원의 벡터에 대한 함수일 테고               
지역 최적값보다 안장점을 훨씬 많이 볼 수 있을 겁니다.          

# 문제점
![image](https://user-images.githubusercontent.com/50114210/66182858-91217680-e6b1-11e9-8066-607585563489.png)          
그럼 지역 최적값이 문제는 아니고 뭐가 진짜 문제일까요?               
안정지대가 학습을 아주 지연시킬 수 있습니다.              
안정 지대는 미분값이 아주 오랫동안 0에 가깝게 유지되는 지역을 말합니다.              
만약 여기에 있다고 치면 경사하강법에 따라 면을 따라서 아래로 움직이겠죠?               
여기서 경사가 0이거나 0에 가까울 테니 면이 거의 평평할 겁니다.             
아마 아주 오랜 시간이 걸릴 거에요.             
안정지대에서 여기까지 찾아가는 데 말이죠.               
그리고 왼쪽이나 오른쪽에 무작위로 작은 변화가 주어지면             
알고리즘이 안정지대를 이렇게 벗어날 수 있을 겁니다.               
하지만 이 점에 도달하기 전에 아주 긴 시간동안 경사를 탄 후에야           
안정지대를 벗어날 수 있습니다.              
이 영상에서 알아두셔야할 것은 충분히 큰 신경망을 학습시킨다면             
지역 최적값에 갇힐 일이 잘 없다는 사실입니다.               
여러 매개변수와 비용 함수 J가 상대적으로 고차원에서 정의된다면 말이죠.               

# 해결법
![image](https://user-images.githubusercontent.com/50114210/66182899-b1513580-e6b1-11e9-84c6-9e708d2c39f4.png)                      
하지만 둘째로 안정지대는 문제입니다. 학습 속도가 매우 느려지니까요.             
여기에서는 모멘텀이나 RMSprop, Adam 등 이런 알고리즘의 도움을 받을 수 있습니다.              
이런 경우에는 Adam과 같은 최적화 알고리즘이              
안정지대 내에서 움직이거나 벗어나는 속도를 올릴 수 있습니다.           
신경망이 일반적으로 고차원에서 최적화 문제를 해결할 때              
사실 어떤 사람도 이 공간이 어떻게 생겼는지 잘 모를 것입니다.              
이 공간에 대한 이해도 계속 발전하고 있고요

# 아웃트로
이 글을 통해 최적화 알고리즘이          
맞닥뜨릴 법한 문제에 대해 직관을 얻었길 바랍니다.              
