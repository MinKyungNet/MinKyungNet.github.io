---
layout: post
title: "심층 신경망의 가중치 초기화"
tags: [Weight Initialization]
categories: [Improving deep neural networks]
---

# 학습 목표
경사 소실 및 폭발을 막을 수 있도록 가중치를 초기화할 수 있다.

# 핵심 키워드
가중치 초기화(Weight Initialization)

# 학습 내용
* 가중치 초기화 방법
1. W[i]의 분산을 $1/n$으로 설정합니다. (n : 입력 특성의 개수)
2. ReLU 활성화 함수를 사용하는 경우, W[i]의 분산을 2/n^[L-1]으로 설정합니다.
3. tanh 활성화 함수를 사용하는 경우, W{i]의 분산을 1/n[L-1] 또는 2/n[L-1] + n[L]으로 설정합니다.

# 인트로
저번 비디오에서 아주 깊은 신경망에서 경사소실과 폭발의 문제가 있다는 것을 보았습니다.    
문제를 완전히 해결하지는 못하지만 많은 도움을 주는 해결법은    
신경망에 대한 무작위의 초기화를 더 신중하게 선택하는 것입니다.    

# 단일 뉴런에 대한 가중치를 초기화해보자     
![image](https://user-images.githubusercontent.com/50114210/65292321-ea59b800-db91-11e9-9720-b9b43b55e25b.png)   
하나의 뉴런에 특성 4개, x1부터 x4까지를 입력합니다. a는 g(z)이고 출력은 y의 예측값입니다.     
나중에 다룰 더 깊은 망에서 이 입력은 a[L]인 어떤 층이 될 것입니다.    
지금은 그냥 x라고 부르겠습니다.   

### W의 값을 적당히 조절할 분산
![image](https://user-images.githubusercontent.com/50114210/65292400-3c9ad900-db92-11e9-889f-95320a931907.png)   
z는 w1*x1 + w2*x2 + ... + wn * xn의 값을 갖습니다. b를 0이라고 하고 지금은 b의 값을 무시합니다.      
따라서 z의 값이 너무 크거나 작아지지 않도록 만들어야합니다. n의 값이 클수록 wi의 값이 작아져야합니다.         
z는 wi * xi의 합이기 때문에 이 많은 항들을 더하면 각각의 항이 작아지기를 바랄 것입니다.     
한 가지 합리적인 일은 wi의 분산을 1/n으로 설정하는 것입니다.   
n은 뉴런으로 들어가는 입력 특성의 개수입니다. 따라서 실제로 특정 층에 대한 가중치 행렬 w[L]을 

```python
W[L] = np.random.randn(shape) * np.sqrt(1 or 2 / n[L-1])
```
으로 만듭니다.   

# ReLU의 경우
여기서 나누는 값은 층 L의 뉴런에 들어가는 특성의 개수의 역수입니다.   
이 것이 층 L의 각각의 유닛에 들어가는 유닛의 개수이기 때문입니다.    
ReLU 활성화 함수를 사용하는 경우 분산을 1/n보다 2/n으로 설정하는 것이 더 잘동합니다.    
특히 ReLU 활성화 함수를 사용하는 경우 초기화에서 많이 보게 될 것입니다.   

# 좀 더 통계스러운 설명
랜덤 변수에 익숙하면 좀 더 자세히 말해서 가우시간 랜덤 변수를 분산 2/n으로 설정하는 이 값의 제곱근에 곱해줍니다.     
n[L-1]을 사용하는 이유는 이 예제의 로지스틱 회귀에서는 n개의 입력 특성을 갖지만        
더 일반적인 경우에 층 L은 해당 층의 각 유닛에 대해 n[L-1]의 입력을 갖습니다.   
따라서 입력 특성 혹은 활성화 값의 평균이 대략 0이고 표준편차 1을 갖는다면 이것 역시 비슷한 크기를 갖게 될 것입니다.    
완전히 해결하지는 못하지만 경사 소실과 폭발 문제에 확실히 도움을 줄 수 있습니다.    
왜냐하면 각각의 가중치 행렬 w를 1보다 너무 커지거나 너무 작아지지 않게 설정해서 소실되지 않게 합니다.    

# tanh 활성화 함수의 경우    
![image](https://user-images.githubusercontent.com/50114210/65292447-7370ef00-db92-11e9-9f15-967dce7b4e38.png)   
tanh 활성화 함수를 사용한다면 상수 2 대신 상수 1을 사용하라는 논문이 있습니다.    
따라서 1/n[L-1]의 제곱근을 구해줍니다. 이러한 초기화를 세이비어 초기화라고 부릅니다.    
또 다른 버전은 2/(n[L-1] + n[L])이 있습니다.    
그러나 ReLU를 사용한다면 처음에 봤던 식을 사용할 것입니다.   

# 분산 변수
그러나 실제로 이 모든 식들은 그저 시작점을 초기화하는 값일 뿐입니다.         
여기서 사용하는 분산 매개변수는 하이퍼파라미터로 조정할 또 다른 값이 됩니다.    
따라서 매개변수를 가질 수 있고 하이퍼파라미터 탐색의 일부로 그 곱하는 수를 조정할 수 있습니다.    
가끄 그 하이퍼파라미터를 조정하는 것은 적당한 크기의 효과를 가집니다.     
하지만 제가 조정을 시도하는 첫번째 하이퍼파라미터는 아닙니다.       
그러나 조정을 하면 상당한 도움이 되는 경우를 많이 보았습니다.    

# 아웃트로
경사 소실과 폭발 문제에 대한 직관을 얻는데 도움이 되셨기를 바랍니다.   
가중치를 초기화하는 합당한 크기를 선택하는데에도 말입니다.   
이를 통해 가중치가 너무 빨리 폭발하거나 0으로 수렴하지 않기를 바랍니다.   
가중치 혹은 경사의 폭발과 소실 없이 상당히 깊은 네트워크를 훈련시킬 수 있습니다.   
깊은 네트워크를 훈련시킬 때 신경망을 더 빨리 훈련시키는 또 다른 기법이었습니다.
