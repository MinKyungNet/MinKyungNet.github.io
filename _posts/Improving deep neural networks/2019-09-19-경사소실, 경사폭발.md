---
layout: post
title: "경사소실, 경사폭발"
tags: [Vanishing gradients, Exploding gradients]
categoires: [Improving deep neural networks]
---

# 학습 목표
경사의 소실 및 폭발 문제를 알 수 있다.

# 핵심 키워드
* 경사의 소실(Vanishing gradients)
* 경사의 폭발(Exploding gradients)

# 학습 내용
* 매우 깊은 신경망을 훈련시킬 때 나타나는 문제점은 경사의 소실과 폭발입니다.
* 예를 들어 g(z) = z, b[L] = 0이라고 가정했을 때 Y = W[L] * W[L-1] * ... * W[2] * W[1] * X가 됩니다. 이 때 모든 가중치 행렬이 W = 1.5E 라고 가정하면, Y^ = 1.5^(L-1)EX가 되고 더 깊은 신경망일수록 Y^의 값은 기하급수적으로 커집니다. 반대로 모든 가중치 행렬 W = 0.5E라고 가정하면 Y^ = 0.5^(L-1)EX가 되고 더 깊은 신경망일수록 Y^의 값은 기하습수적으로 감소합니다. 이를 토대로 생각하면 경사 하강법에서 W의 값이 단위행렬보다 큰 값이라면 경사의 폭발, W의 값이 단위행렬보다 작은 값이라면 경사의 소실 문제점이 생깁니다.
* 경사의 소실과 폭발로 인해 학습 시키는데 많은 시간이 걸리기에 가중치 초기화 값을 신중하게 해야합니다.

