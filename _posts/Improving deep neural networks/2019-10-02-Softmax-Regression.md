---
layout: post
title: "Softmax Regression"
tags: [Softmax]
categoires: [Improving deep neural networks]
---

# 학습 목표
여러 클래스가 분류시 사용하는 softmax를 배운다.

# 핵심 키워드
Softmax

# 학습 내용
* Softmax는 여러개의 클래스 분류시 사용됩니다.
* 마지막 층의 출력값이 주어졌을 때 해당 클래스에 속할 확률을 Softmax층을 통해서 구할 수 있습니다. 마지막 선형 출력값(z)들을 각각 지수화시켜 임시변수 t = e^z를 만듭니다. 그 후 모든 값들의 합이 1이 될 수 있도록 모든 임시 변수값들의 합을 나눠서 정규화시킵니다.            
![image](https://user-images.githubusercontent.com/50114210/66021370-7ae6af80-e525-11e9-8339-3220d79a8229.png)           

# 인트로
지금까지 우리가 봤던 분류 문제는 이진 분류였습니다.        
0과 1의 두 가지 선택이 있었죠. 고양이냐 아니냐처럼 말이에요.        
여러 개의 선택지가 주어진다면 어떨까요?          
로지스틱 회귀을 일반화한 소프트맥스 회귀가 있는데요.          
이걸 여러 클래스나 C 중 하나를 인식할 때 예측에 사용할 수 있습니다.          
클래스가 두 개인 경우 밖에 말이죠. 살펴봅시다.          

# 여러 종류를 분류해보자
![image](https://user-images.githubusercontent.com/50114210/66021873-0ca2ec80-e527-11e9-8199-e98d5dff06fd.png)         
고양이를 인식하는 것에서 나아가 개, 고양이, 병아리를 인식해봅시다.            
고양이는 클래스 1, 개는 클래스 2 병아리는 클래스 3입니다.         
그리고 여기에 해당하지 않는 경우 클래스 0이라고 하죠.          
여기에 이미지와 속해 있는 클래스를 적어뒀습니다.           
여기 병아리 사진에는 클래스 3이 배정됐고요.          
고양이는 클래스 1입니다. 개는 클래스 2고요.           
제가 추측하기로는 코알라인데 아무 것도 아니니 클래스 0이겠죠.             
이렇게 클래스를 부여합니다.           

# 클래스의 수를 나타내는 C
![image](https://user-images.githubusercontent.com/50114210/66021902-2b08e800-e527-11e9-8767-cee1ef822bf7.png)         
여기에서 대문자 C는 클래스의 숫자를 나타내는 데 사용할 겁니다.             
여러분의 입력값을 분류하는 것이죠? 지금은 총 4가지 클래스가 있습니다.             
아무 것도 아닌 경우를 포함해서요.         
만약 클래스에 숫자를 붙인다면 0에서 C-1까지 부여가 될 겁니다.           
여기서는 0, 1, 2, 3이죠. 이제 이런 신경망을 하나 만들었다고 합시다.            
출력층에는 C개, 이 경우 4개의 출력 단위가 있는 신경망을요.            
출력층 L의 단위 개수인 n은 4, 또는 일반적으로 C가 될 겁니다.           

# 클래스를 분류하는 신경망
![image](https://user-images.githubusercontent.com/50114210/66021933-44119900-e527-11e9-84fb-2a6da0a82ce2.png)         
우리는 각 단위와 상위층에서 각 클래스의 확률을 알려줬으면 합니다.              
첫 번째 단위에서는 우리가 원하는 출력값이           
입력값 X가 주어졌을 때 기타 클래스가 나올 확률이겠죠?          
따라서 출력값인 y^은 (4, 1)차원의 벡터가 될 겁니다.          
왜냐하면 출력값으로 네 개의 확률값이 주어질 테니까요.          
그리고 y^의 각 값들의 합은 1이 되어야할 겁니다.        
이런 신경망을 얻기 위한 가장 표준적인 모델은          
출력층에 이런 출력값을 만들 수 있도록 소프트맥스층을 사용하는 겁니다.          

# 소프트맥스
![image](https://user-images.githubusercontent.com/50114210/66022027-9357c980-e527-11e9-8f73-12a6db2e6c4c.png)         
큰 그림을 그린 뒤에 다시 돌아가서 소프트맥스가 뭐하는 건지 살펴보죠.            
신경망의 최종층에서 평소처럼 층의 선형적인 부분인 z^[L]을 계산할 겁니다.           
최종층인 L의 z값이죠.        
평소에는 z^[L]을 w^[L]과 이전 층의 활성화 함수를 곱한 뒤             
그 최종층의 편향을 더하여 계산했습니다.      
이제 z 값을 계산하기 위해서는 소프트맥스 활성화 함수라는 것을 사용해야 합니다.           
소프트맥스 층의 활성함수는 조금 다른데요. 함께 보시죠.         
우선 t=e^(z^[L])이라는 임시 변수를 사용합니다.            
각 원소에 대해 계산하는 데요. 즉 z^[L]이 (4, 1) 차원의 벡터였잖아요?             
t는 z^[L]의 각 원소에 e를 취한 것이지요. 즉 결과도 (4, 1)이 될 겁니다.            
그러면 출력값인 a^[L]은 벡터 t와 같습니다. 다만 합이 1이 되도록 정규화하죠.         
즉 a^[L]은 z^[L]을 j가 1부터 4까지, 왜냐하면 원소가 4개니까요.           
t_j를 모두 더한 값으로 나눕니다. 즉 a^[L]은 (4, 1) 벡터이고요.          
이 벡터의 i 번째 원소는 a^[L]_i라고 적죠.           
t_i를 t_i 값들의 합으로 나눈 것과 같겠죠.          


# 소프트맥스 예시
![image](https://user-images.githubusercontent.com/50114210/66022052-aa96b700-e527-11e9-8920-8fcdc3d7780d.png)          
아직 감이 잘 안 잡히는 분을 위해 예시를 하나 준비했습니다.        
예를 들어 (4,1) 벡터인 z^[L]이 [5, -2, 1, 3]이라고 하죠.          
우선 원소별로 e를 취해 t를 구합시다.         
t는 e의 5승, e의 2승, e의 -1승, e의 3승이 되겠죠?          
이걸 계산기에서 계산하면 e의 5승은 148.4         
e의 2승은 7.4, e의 -1승은 0.4 e의 3승은 20.1이 됩니다.       
이제 t에서 a^[L]으로 합이 1이 되도록 정규화시키기 위해서          
t의 네 개 값의 합을 구하면 176.3가 됩니다.          
이제 a^[L]은 t를 176.3으로 나눈 것이 되겠죠.       

### 확률을 계산
![image](https://user-images.githubusercontent.com/50114210/66022075-c8fcb280-e527-11e9-97dd-e8f55149a98f.png)     
즉 예시에서 첫 번째 노드의 값은 e의 5승을 176.3으로 나눈 것이겠죠.          
그 값은 0.842입니다.          
만약 이런 z 값을 얻었다면 클래스 0이 될 확률이 84.2%인 것이죠.          
다음 노드의 출력값은 e 제곱을 176.3으로 나눈 값인 0.042고요.          
다음은 e의 -1승을 176.3으로 나눠서 0.02입니다.          
끝으로 e의 3승을 저 수로 나눠서 0.114라는 값을 얻을 수 있습니다.              
즉 11.4%의 확률로 클래스 3이 되는 것이죠.         
이렇게 클래스 0,1,2,3이 될 확률을 구할 수 있습니다.            
이 신경망의 출력값 y^과도 같은 a^[L]은 (4,1) 벡터가 되겠고 그 안에는       
계산한 이 숫자들이 들어가있겠죠.          
이 알고리즘은 z^[L]이라는 벡터를 취해서 합이 1이 되는 네 개의 확률 값을 내놓습니다.           
z^[L]에서 a^[L]으로 되는 과정을 요약하자면       
e를 취해서 임시 변수 t를 얻어서 정규화한 이 과정을          
소프트맥스 활성화 함수로 요약할 수 있습니다.        
즉 a^[L]은 z^[L] 벡터에 활성함수 g^[L]을 적용한 것이죠.           

# 소프트맥스의 특이한 점
이 활성화 함수 g의 특이한 점은 (4,1) 벡터를 받아서 (4,1) 벡터를 내놓는다는 것입니다.           
이전에는 활성함수가 하나의 실수값을 받았습니다.         
예를 들어 시그모이드나 Relu 활성화 함수 등은 실수를 받아서 실수를 내놨죠.         
소프트맥스 활성화 함수의 특이한 점은 정규화를 하기 위해서         
입력값과 출력값이 모두 벡터라는 것입니다.       

# 은닉층이 없는 단순한 예시
![image](https://user-images.githubusercontent.com/50114210/66022194-1d079700-e528-11e9-978a-0eafcfdd033d.png)       
소프트맥스 분류로 할 수 있는 것을 보여드리기 위해      
예를 들어 x_1과 x_2의 입력값이 있다고 가정해봅시다.          
이 값들은 곧바로 소프트맥스 층에 들어갑니다.         
안에는 서너개의 노드가 있고 출력값은 y^입니다.             
이렇게 은닉층이 없는 신경망을 보여드릴 거에요.          
여기에서 하는 일은 z^[1]을 w^[1]과 x를 곱한뒤 b^[1]을 더하여 계산하고          
출력값 y^이자 a^[1]을 z^[1]에 소프트맥스 활성화 함수를 적용시켜 얻는 것입니다.            
은닉층이 없는 신경망에서 소프트맥스 함수가 뭘 하는 지 감을 잡아봅시다.          

### C = 3 예시
![image](https://user-images.githubusercontent.com/50114210/66022352-b171f980-e528-11e9-909b-12ca70151e0e.png)         
이 예시에서는 입력값 x_1과 x_2에 대하여 이 결정 기준을 나타내는         
C=3의 클래스를 가진 소프트맥스 층을 사용했습니다.         
여기서 선형적인 기준에 따라 데이터가 세 개의 클래스로 나뉩니다.        
우리가 했던 것은 이 그림처럼 학습 세트를 가져와서         
비용 함수와 세 개의 선택지에 따라 분류하는 소프트맥스 함수를 학습시키는 것이죠.             
색깔은 소프트맥스 분류 함수에 따라 출력값을 나타낸 것이고        
입력값은 가장 높은 확률의 출력값에 따라 색을 입혔습니다.            
선형 기준을 갖고 있는 로지스틱 회귀의 일반적인 형태죠.         
하지만 클래스는 0,1 또는 0,1,2가 될 수 있고요.       
이것은 소프트맥스 분류 함수가 나타낸 또다른 경우인데요.         
세 개의 클래스에 따라 데이터를 학습시켰습니다. 이건 또 다른 분류이고요.        
여기서 얻을 수 있는 직관은 두 클래스 사이의 경계가 선형이라는 것입니다.         
따라서 예시를 보시면 노란색과 빨간색 사이에도 선형 경계가 그려져 있고          
보라색과 빨간색, 빨간색과 노란색 사이에도 선형 결정 경계가 그려져있습니다.          
하지만 다른 선형 함수를 사용해서 공간을 세 개의 클래스로 나눌 수도 있습니다.          
더 많은 클래스를 다룬 예시도 살펴봅시다.          

### C = 4, 5, 6 예시
![image](https://user-images.githubusercontent.com/50114210/66022390-d23a4f00-e528-11e9-87d9-48b2d9da8a5b.png)     
이 예시는 C=4인 경우네요.         
여기에서 소프트맥스가 선형 경계를 그리고 있습니다.        
이건 C=5인 또다른 예시입니다. 이건 C=6인 마지막 예시입니다.         
이렇게 은닉층이 없을 때 소프트맥스 분류 함수가 하는 일을 살펴봤습니다.         
만약 은닉 유닛이 여러 개인, 더 깊은 신경망을 다룬다면.         
여러 클래스를 분류하기 위해 더 복잡하고 비선형의 경계도 볼 수 있을 겁니다.         

# 아웃트로
이 글 통해 신경망에서 소프트맥스 층에 대해서나         
소프트맥스 활성함수에 대해 감을 잡았길 바랍니다
다음 글에서는 소프트맥스 층을 이용해 신경망을 학습시키는 법을 배워봅시다.         
