---
layout: post
title: "Tensorflow"
tags: [Tensorflew]
categories: [Improving deep neural networks]
---

# 학습 목표
딥러닝 프레임워크 중 하나인 Tensorflow를 소개한다.

# 핵심 키워드
Tensorflow 

# 인트로
좋은 딥러닝 프레임워크가 많이 있는데요. 그 중 하나는 텐서플로우입니다.            
이 글에서는 텐서플로우 프로그램의 기초적인 구조를 알려드리고 예제를 통해 연습할 겁니다.             

# 비용함수를 생각해보자
문제를 하나 다뤄볼까요?             
여러분이 최소화하고 싶은 비용 함수 J가 있다고 합시다.           
저는 아주 간단한 비용 함수인 J(w)=w^2-10w+25라는 비용 함수를사용할겁니다.             
이 함수는 (w-5)^2과 같죠? 이 제곱식을 풀면 위의 식을 얻을 수 있습니다.            
따라서 이 식을 최소로 하는 w는 5입니다.           
여러분이 이걸 모른다고 가정하고 이 함수를 봅시다.          

# .
이제 텐서플로우에서 이 식을 어떻게 최소화하는지 살펴봅시다.           
프로그램이 모든 매개변수와 관련이 있어서           
다소 복잡한 비용 함수 J(w,b)를 학습시키는 신경망과 구조가 비슷하므로               
비슷한 방법으로 텐서플로우를 이용해서 자동으로 이 비용 함수를 최소화하는             
w와 b의 값을 찾을 수 있습니다. 하지만 왼쪽의 단순한 경우에서 시작해보죠.                

# 임포트
```python
import numpy as np
import tensorflow as tf
```
numpy를 np로 불러왔고 통상 텐서플로우를 tf로 불러옵니다.             

# 매개변수 정의
```python
w = tf.Variable(0, dtype=tf.float32)
```
그리고 매개변수 w를 정의합시다.             
텐서플로우에서는 tf.Variable을 사용해서 매개변수를 정의합니다.                
dtype은 tf.float32와 같죠.               

# 비용함수 정의
```python
# cost = tf.add(tf.add(w**2, tf.multiply(-10.0,w)), 25.0)
# cost = w**2 + 10*w + 25
cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]
```
그리고 비용 함수를 정의합시다.  비용 함수는 w^2-10w+25였는데요.                
이렇게 비용 함수 J를 정의했습니다

# optimizer 설정
```python
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
```
그리고 train을 tf.train.GradientDescentOpimizer로 정의합니다.            
학습 속도는 0.01로 하고요. 그리고 목표는 cost를 최소화하는 거죠.             

# 전역변수 초기화
```python
init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
```
그리고 이제 관용적인 식들을 쓰겠습니다.               
init = tf.global_variables_initializer를 쓰고 session = tf.Session()입니다.                
이렇게 텐서플로우의 세션을 시작합니다.              
session.run(init)을 써서 전역변수를 초기화합니다.               
그리고 텐서플로우가 변수의 값을 알 수 있도록 sess.run(w)를 사용합니다.               

# 그냥 실행해보기
```python
print(session.run(w))
```
위에서는 w를 0으로 초기화했고 비용 함수를 정의했습니다.            
train은 경사하강법을 써서 비용 함수를 최소화하는 학습 알고리즘이 될거고요.            
하지만 학습 알고리즘을 아직 실행하지는 않았습니다.              
session.run에서 w의 값을 알게되겠죠 그리고 session.run을 출력합시다.                
그러면 아직 아무것도 안 했기 때문에 w가 0으로 계산됩니다.           

# 한 단계만 실행해보기
```python
session.run(train, feed_dict={x:coefficients})
print(session.run(w))
```
session.run(train)을 합시다. 경사하강법을 한 단계 실행할 겁니다.           
경사하강법 후에 w의 값을 계산해보죠. 그걸 출력하고요.           
한 단계의 경사하강법 후에는 w 값이 0.1이군요.             

# 1000번 실행해보기
```python
for i in range(1000):
    session.run(train, feed_dict={x:coefficients})
print(session.run(w))
```
경사하강법을 1,000번 실행해봅시다. session.run(train)을요.              
그리고 session.run(w)를 출력하면 경사하강법을 1,000번 시행한 끝에               
w의 값이 4.99999로 계산되었습니다.            
우리가 최소화하는 식이 (w-5)^2였으니          
w의 최적값은 5이고, 거기에 아주 가까워졌네요.                 
이 과정을 통해 텐서플로우의 대략적인 구조를 깨달으셨기 바랍니다.                

# 알아서 도함수를 찾아준다.
여기에서 말씀드리고 싶은 것은 w는 최적화하고 싶은 변수이니 변수로 선언했고                
우리가 해야했던 것 비용 함수를 정의하는 것이었습니다.               
그러면 텐서플로우는 자동으로 미분을 어떻게 계산할지 알게 되는 거죠.              
이것이 정방향 전파만 구현하면 되는 이유입니다.                
그러면 경사계산법에서 역방향 전파를 잘 계산해내니까요.            

# placeholder
텐서플로우의 또다른 특징을 한 가지 더 보여드릴게요.           
이 예시에서는 w에 관해 고정된 함수를 최소화합니다.              
여러분이 최소화하고 싶은 함수 중 하나는 학습 세트와 관련한 함수겠지요?                
따라서 여러분이 어떤 학습 데이터 X를 갖고있든 간에              
신경망을 학습시키면 데이터 X가 바뀔 수 있습니다.             
그러면 학습 데이터를 텐서플로우에서 어떻게 사용할 수 있을까요?             
학습 데이터의 역할을 하는 x를 찾아봅시다.              
또는 실제로 학습 데이터가 x, y일 때 예시에서 x만 쓴다고 생각할 수도 있습니다.               
x를 플레이스홀더로 정의하면 타입이 float32일거고요.                 
이걸 (3,1) 차원의 배열로 만듭시다. 여기 비용 함수에서 계수는 고정되어 있죠.               
1*w^2-10*w+25니까요. 1, -10, 25라는 숫자를 데이터로 바꿀 수 있습니다.               
여기서 cost를 x[0][0] * w^2 + x[1][0]*w+x[2][0]로 대체하는 거죠.              
그럼 이제 x는 이 이차 함수의 계수를 조절하는 데이터가 됩니다.                 
이 플레이스홀더 함수는 텐서플로우에게 x에 값을 나중에 줄 거라고 말해줍니다.               

# 실행해보기
이제 다른 배열을 정의해보죠. coefficient = np.array([[1.], [-10.], [25.]])으로요.              
마지막 값은 -25입니다. 이 데이터를 x에다 집어넣을 겁니다.             
이제 이 coefficient 배열을 변수 x에 집어넣을 방법이 필요합니다.              
학습 중에 x를 제공하는 문법이 필요하겠죠?             
feed_dict={x:coefficients}라고 쓰겠습니다.           
이 이차 함수의 계수를 바꾸고 싶다면 여기 10을 20으로 바꾼다고 합시다.             
-20으로요. 그리고 이건 100으로 바꾸고요. 이제 함수가 (x-10)^2이 됐습니다.            
이제 (x-10)^2를 최소화하는 값으로 w가 10이 나오길 바라야겠네요.              
경사하강법을 1,000번 실행하니 10과 아주 가까운 w를 얻었습니다.               

# 값을 나중에 넣기
텐서플로우에서 이 플레이스홀더는 값을 나중에 넣는 변수입니다.              
이 방법을 사용하면 학습 데이터를 비용 함수에서 쉽게 얻을 수 있죠.               
여러분이 비용 함수에 데이터를 불러오는 문법은 이것입니다.              
학습을 반복할 때 feed_dict에서 x를 coefficient로 두면 되죠/              
만약 각 학습에서 미니 배치 경사하강법을 쓴다면              
서로 다른 미니 배치를 집어넣어야겠죠?              
그럼 각 학습마다 feed_dict에 학습 세트의 다른 부분집합을 넣어야 할 겁니다.                  
데이터를 기다리고 있는 비용 함수에 서로 다른 미니 배치를 집어넣는 거죠.                        

# 관용구
![image](https://user-images.githubusercontent.com/50114210/66186576-91277380-e6bd-11e9-8791-c4dab927adf9.png)           
텐서플로우에서 이 세 줄의 코드가 관용적으로 쓰입니다.            
몇몇 프로그래머들은 대신 이런 형태를 사용하기도 합니다.              
기본적으로 같은 일을 하죠. 세션을 시작하기 위해            
session을 tf.session으로 두고 session을 이용해 init을 run합니다.             
그리고 session에서 w를 계산해서 그 결과를 출력하죠.               
여기서 with은 수많은 텐서플로우 프로그램에서 사용됩니다.               
하지만 파이썬에서 with문을 실행하고 있을 때는              
오류나 예외의 경우에 더 깔끔합니다.                  

# 간단하게 갈아끼우기
![image](https://user-images.githubusercontent.com/50114210/66186605-a69c9d80-e6bd-11e9-9cf6-27d174479502.png)          
프로그래밍 프레임워크에서는 한 줄의 코드로 많은 일을 할 수 있습니다.             
만약 경사하강법을 쓰기 싫다면 대신 이 코드 한 줄을 바꿔서             
Adam을 사용할 수도 있겠죠?              
아주 빠르게 더 나은 최적화 알고리즘으로 바꿀 수 있습니다.               
최근의 모든 딥러닝 프레임워크들은 이런 기능을 지원하고 있어서               
복잡한 신경망도 쉽게 코딩할 수 있습니다.            

# 아웃트로
텐서플로우가 어떤 프로그램인지 이 글을 통해 잘 이해하셨길 바랍니다.           
