---
layout: post
title: "RMSProp 최적화 알고리즘"
tags: [RMSProp, Optimization]
categories: [Improving deep neural networks]
---

# 학습 목표
RMSProp 최적화 알고리즘을 배운다.

# 핵심 키워드
* RMSProp
* 최적화(Optimization)

# 학습 내용
* 알고리즘은 아래와 같습니다.        
![image](https://user-images.githubusercontent.com/50114210/65679170-8bafa500-e08f-11e9-8a41-8fc88f06bf4a.png)        
* RMSProp의 장점은 미분값이 큰 곳에서는 업데이트 시 큰 값으로 나눠주기 때문에 기존 학습률보다 작은 값으로 업데이트됩니다. 따라서 진동을 줄이는데 도움이 됩니다. 반면 미분값이 작은 곳에서는 업데이트시 작은 값으로 나눠주기 때문에 기존 학습률 보다 큰 값으로 업데이트 됩니다. 이는 더 빠르게 수렴하는 효과를 불러옵니다.

# 인트로
모멘텀을 사용하는 것이 경사 하강법을 빠르게 할 수 있다는 것을 배웠습니다.       
root mean square prop, 줄여서 RMSprop이라는 알고리즘이 있습니다.         
이 알고리즘 역시 경사 하강법을 빠르게 하는데 작동 방식을 살펴봅시다.      

# 수직방향으로 진동
![image](https://user-images.githubusercontent.com/50114210/65680188-70459980-e091-11e9-9e34-807ede300e3a.png)   
지난 번에 했던 예제를 살펴보면 경사 하강법에서 수평 방향으로           
진행을 시도해도 수직 방향으로 큰 진동이 있다는 것을 알 수 있습니다.     

# 단순화
![image](https://user-images.githubusercontent.com/50114210/65680230-83f10000-e091-11e9-911b-6c5050407f27.png)      
이 예제에 대한 직관을 제공하기 위해 수직축은 매개변수 b, 수평축은 매개변수 w라고 가정합시다.           
w1과 w2라고 해도 되지만 직관을 위해 w와 b라고 이름을 지었습니다.         
b 방향 또는 수직 방향의 학습 속도를 낮추기 위한 것이고          
그리고 수평 방향의 속도를 빠르게 하기 위한 것입니다.        

# 알고리즘
![image](https://user-images.githubusercontent.com/50114210/65680364-c0bcf700-e091-11e9-94cf-067afda4bb05.png)       
RMSprop 알고리즘이 하는 일은 다음과 같습니다.         
반복 t에서 현재의 미니배치에 대한 보통의 도함수 dw와 db를 계산할 것입니다.         
지수가중평균을 유지하기 위해서 새로운 표기법인 s_dw를 사용하겠습니다.        
이 값은 β*s_dw + (1-β)*dw^2입니다 **을 사용해 표현하기도 하지만          
여기서는 그냥 제곱을 사용하겠습니다.            
명확히 말하면 이 제곱 표시는 요소별 제곱을 나타냅니다.         
이것은 도함수의 제곱을 지수가중평균하는 것입니다.         
비슷하게 s_db도 β*s_db + (1-β)*db^2과 같습니다.            
다음으로 RMSprop은 매개변수를 다음과 같이 업데이트합니다.         
w는 w에서 학습률 α * dw를 s_dw의 제곱근으로 나눠준 값을 뺀 것입니다.            
그리고 b는 b - α  * db만을 하는 대신에 s_db의 제곱근으로 나눠준 값을 뺍니다.          

# 직관
![image](https://user-images.githubusercontent.com/50114210/65680282-9c611a80-e091-11e9-9d1c-33abcdec7f37.png)       
이것이 어떻게 작동하는지에 대한 직관을 얻어봅시다.       
수평 방향, 이 예제에서의 w 방향에서는 학습률이 꽤 빠르게 가기를 원합니다.         
반면에 수직 방향, 즉 b 방향에서는 느리게 혹은 수직 방향의 진동을 줄이고 싶습니다.         
따라서 이 두 항, s_dw와 s_db에서 우리가 원하는 것은 s_dw가 상대적으로 작고,          
즉 여기서 상대적으로 작은 숫자로 나눠주고 반면에 s_db는 상대적으로 커서,         
여기서 상대적으로 큰 숫자로 나눠주는 것입니다. 수직 방향에서의 업데이트를 줄이기 위해서요.          
실제로 수직 방향에서의 도함수가 수평 방향의 것보다 훨씬 큽니다.         
b 방향에서의 경사가 매우 큽니다. 도함수 db는 매우 크고 dw는 상대적으로 작습니다.          
왜냐하면 수직 방향 b에서 기울기가 더 가파르기 때문입니다.        
수평 방향 w에서의 기울기보다 가파릅니다.      
따라서 db^2은 상대적으로 크고 dw가 작기 때문에 dw^2은 상대적으로 더 작습니다.       
다음에 오는 효과는 더 큰 숫자로 나눠서 수직 방향에서 업데이트하기 때문에    
진동을 줄이는데 도움을 줍니다.        
반면에 수평 방향에서는 작은 숫자로 나눠서 업데이트하기 때문에          
RMSprop을 사용한 업데이트는 다음과 같습니다.      
수직 방향에서의 업데이트는 감소하지만 수평 방향은 계속 나아가게 합니다.        
이것의 효과는 큰 학습률을 사용해 빠르게 학습하고 수직 방향으로 발산하지 않습니다.                

# 튀는 값은 잡고 너무 약한 값은 올리고
![image](https://user-images.githubusercontent.com/50114210/65680450-ee09a500-e091-11e9-9399-b97bb98862d1.png)    
더 명확히 말하면 제가 수직과 수평 방향을 b와 w로 나타냈는데       
실제로는 매우 매개변수의 고차원 공간에 있기 때문에        
진동을 줄이려는 수직 차원은 w1, w2, .. w17의 매개변수 집합이고         
수평 방향의 차원은 w3, w4, ... 처럼 나타날 것입니다.          
따라서 w와 b의 분리는 표현을 위한 것이고 실제로 dw와 db는 매우 고차원의 매개변수 벡터입니다.          
그러나 직관적으로 이런 진동을 얻는 차원에서 더 큰 합 또는 가중평균 도함수의 제곱을 계산하기 때문에
결국에는 이런 진동이 있는 방향을 감쇠시키게 됩니다.          
따라서 제곱 평균 제곱근, 줄여서 RMSprop이었습니다.        
도함수를 제곱해서 결국 제곱근을 얻기 때문입니다.      

# 0으로 나누지 않게 입실론 더하기
![image](https://user-images.githubusercontent.com/50114210/65680520-142f4500-e092-11e9-9f9e-9aa4adef8576.png)    
마지막으로 넘어가기 전에 세부 사항을 말씀드리겠습니다.       
알고리즘이 0으로 나눠지지 않도록 주의하세요.       
만약 s_dw의 제곱근이 0에 매우 가깝다면 이 값은 폭발할 수 있습니다.       
실제로 구현할 때 이러한 수학적 안정성이 보장되야 합니다.        
ε이 10^(-8)이면 합리적인 기본값이지만 이보다 약간 큰 값도 수학적 안정성을 보장합니다.         
너무 작은 숫자로 나누지 않도록 주의하세요.         

# 아웃트로
RMSprop은 진동을 줄이는 효과가 있다는 점에서 모멘텀과 비슷합니다.      
더 큰 학습률을 사용할 수 있게 해서 속도를 올려줍니다.      
알고리즘의 학습 속도를 말합니다. 이제 RMSprop을 구현하는 방법을 배웠고      
학습 알고리즘의 속도를 올리는 방법입니다. 
RMSprop의 재밌는 사실은 학계의 논문에서 나온 것이 아니라       
몇 년 전 제프리 힌튼이 가르친 코세라 강의에서 나왔습니다.         
코세라는 학술 연구의 보급을 위한 플랫폼이 되기를 의도한 것은 아니었지만,        
이 경우에는 그렇게 되었습니다. RMSprop이 널리 알려진 것은 코세라 강의 덕분이었습니다
RMSprop과 모멘텀을 함께 사용하면 더 나은 최적화 알고리즘을 얻을 수 있습니다.      
다음 글에서 살펴봅시다.      

