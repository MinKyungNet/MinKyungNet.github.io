---
layout: post
title: "드롭아웃의 이해"
tags: [Dropout]
categories: [Improving deep neural networks]
---

# 학습 목표
드롭아웃을 이해한다.

# 핵심 키워드
드롭아웃(Dropout)

# 학습 내용
* 드롭아웃은 랜덤으로 노드를 삭제시키기 때문에, 하나의 특성에 의존하지 못하게 만듦으로서 가중치를 다른 곳으로 분산시키는 효과가 있습니다.
* 드롭아웃의 keep.prop확률을 층마다 다르게 설정할 수 있습니다.
* 모든 반복에서 잘 정의된 비용함수가 하강하는지 확인하는게 어려워집니다. 따라서 우선 드롭아웃을 사용하지 않고, 비용함수가 단조감수인지 확인 후에 사용해야합니다.

# 인트로
드롭아웃은 무작위로 신경망의 유닛을 삭제시키는 이상한 기법이었습니다.    
왜 이것이 정규화로 잘 작동할까요?

# 첫번째 직관
이전 글에서 신경망의 유닛을 무작위로 삭제하는 것의 직관을 말씀드렸습니다.    
따라서 모든 반복마다 더 작은 신경망에서 작업하게 됩니다.      
더 작은 신경망을 사용하는 것이 정규화의 효과를 주는 것처럼 보입니다.    

# 두번째 직관
![image](https://user-images.githubusercontent.com/50114210/64977144-cece8300-d8ed-11e9-8789-de3775d4dcb1.png)      
단일 유닛의 관점에서 살펴봅시다. 이 유닛이 해야하는 일은 입력을 받아 의미있는 출력을 생성하는 것입니다.    
드롭아웃을 통해 입력은 무작위로 삭제될 수 있습니다 .   

### 그 누구도 믿을 수 없다!
![image](https://user-images.githubusercontent.com/50114210/64977207-edcd1500-d8ed-11e9-893a-4d2676a5505a.png)      
어떤 경우는 이 두 유닛이 삭제되고 어떤 경우에는 다른 유닛이 삭제됩니다.   
따라서 제가 보라색으로 표시한 유닛은 어떤 특성에도 의존할 수 없습니다.   
그 특성이 무작위로 바뀌거나 특성의 고유한 입력이 무작위로 바뀔 수 있으니까요.    
따라서 특정 입력에 모든 것을 걸 수 없는 상황입니다.    
즉, 특정 입력에 유난히 큰 가중치를 부여하기가 꺼려지는 상황입니다.    
따라서 이 네 개의 입력 각각에 가중치를 분산시키는 편이 낫습니다.    
가중치를 분산시킴으로써 가중치의 노름의 제곱값이 줄어들게 됩니다.   
L2 정규화에서 봤던 것처럼 드롭아웃을 구현하는 효과는 가중치를 줄이는 것이고,    
L2 정규화처럼 과대적합을 막는데 도움이 됩니다.    
드롭아웃은 예전에 L2 정규화의 적응형으로 보여지기도 했습니다.     
그러나 L2 정규화에서 다른 가중치는 다르게 취급합니다.   
그 가중치에 곱해지는 활성화의 크기에따라 다릅니다.    
정리하자면 드롭아웃은 L2 정규화와 비슷한효과를 낼 수 있습니다.    
L2 정규화가 다른 가중치에 적용된다는 것과 서로 다른 크기의 입력에 더 잘 적응한다는 것만 다른 부분입니다.     

# 과대적합이 우려되는 층에 keep_prob를 높게 설정하자
![image](https://user-images.githubusercontent.com/50114210/64977671-d3476b80-d8ee-11e9-81f3-c0a801b210ee.png)    
드롭아웃을 구현할 때의 세부사항을 하나 더 말씀드리면     
여기 3개의 입력 특성와 7, 7, 3, 2, 1개의 은닉 유닛이 있는 네트워크가 있습니다.   
우리가 선택해야 할 매개변수 중에 keep_prob이라는 것이 있습니다.    
각 층에 해당 유닛을 유지할 확률입니다. 층마다 keep_prob를 바꾸는 것도 가능합니다.    
첫번째 층의 가중치 행렬 w[1]은 (3, 7)행렬입니다.     
두번째 층의 가중치 행렬 w[2]는 (7, 7)행렬입니다. 
세번째 층의 가중치 행렬 w[3]은 (7, 3)행렬입니다.    
이런 식으로 계속 진행됩니다.    
w[2]가 가장 많은 매개변수를 갖기 때문에 가장 큰 가중치 행렬입니다.    
따라서 이 행렬의 과대적합을 줄이기 위해 층 2는 상대적으로 낮은 keep_prob를 가져야합니다.    
반면에 과대적합의 우려가 적은 층에서는 더 높은 keep_prob를 설정해도 됩니다.   
1.0의 값을 갖는 keep_prob은 모든 유닛을 유지하고 해당 층에서는 드롭아웃을 사용하지 않는다는 의미입니다.    
L2 정규화에서 다른 층보다 더 많은 정규화가 필요한 층에서 매개변수 람다를 증가시키는 것과 비슷한 것입니다.   
이론적으로 드롭아웃을 입력층에도 적용시킬 수 있습니다.   
한 개나 그보다 더 많은 개수의 입력 특성을 삭제할 수도 있습니다.    
그러나 이것을 실제로 자주 사용하지 않는 것이 좋습니다.    
입력 층에 대해서는 1.0의 keep_prob이 가장 흔한 값입니다. 0.9의 값을 사용하기도 합니다.    
입력 특성의 절반 이상을 삭제하고 싶지는 않기 때문이죠.      
정리하자면 다른 층보다 과대적합의 우려가 더 큰 층에 대해서는     
다른 층보다 더 낮은 값의 keep_prob를 설정할 수 있습니다.    
단점은 교차 검증을 위해 더 많은 하이퍼파라미터가 생긴다는 것입니다.     
또 다른 대안으로는 어떤 층에는 드롭아웃을 적용하고 어떤 층에는 적용하지 않아서    
매개변수를 드롭아웃을 적용한 층에 대한 keep_prob하나만 갖는 것입니다.    

# 드롭아웃은 정규화 기법
마무리 전에 몇가지 구현 팁을 드리자면     
컴퓨터 비전 분야에서 드롭아웃의 구현에 관한 최초의 성공들이 많이 나왔습니다.     
컴퓨터 비전은 아주 많은 픽셀 값을 모두 사용하기 때문에 대부분의 경우 데이터가 부족합니다.    
따라서 컴퓨터 비전에서 드롭아웃이 매우 빈번하게 사용됩니다.    
최근 비전 분야의 연구원들은 거의 항상 기본값으로 드롭아웃을 사용합니다.    
그러나 여기서 기억해야 할 것은 드롭아웃은 정규화 기법이고 과대적합을 막는데 도움을 줍니다.    
따라서 제 네트워크가 과대적합의 문제가 생기기 전까지는 드롭아웃을 사용하지 않을 것입니다.     
컴퓨터 비전은 충분한 데이터가 없기 때문에 거의 대부분 과대적합이 일어나고 드롭아웃을 많이 사용하는 이유입니다.    
제가 말하는 직관들이 다른 분야에도 항상 일반화되는 것은 아닙니다.    

# 드롭아웃과 비용함수
![image](https://user-images.githubusercontent.com/50114210/64977712-ece8b300-d8ee-11e9-957a-f23c61c051ae.png)   
드롭아웃의 큰 단점은 비용함수 J가 더 이상 잘 정의되지 않는다는 것입니다.    
모든 반복마다 무작위로 한 뭉치의 노드들을 삭제하게 됩니다.    
따라서 경사 하강법의 성능을 이중으로 확인한다면      
모든 반복에서 잘 정의된 비용함수 J가 하강하는지 확인하는게 어려워집니다.     
여러분이 최적화하는 비용함수는 잘 정의되지 않아 계산하기 어렵습니다.    
따라서 이런 모양인지 확인해서 디버깅하는게 어려워집니다.      
보통은 keep_prob을 1로 설정해서 드롭아웃 효과를 멈추고 코드를 실행시켜 J가 단조감소하는지 확인합니다.    
그리고 드롭아웃 효과를 다시 주고 드롭아웃이 있을 때 코드를 바꾸지 않도록 합니다.    
왜냐하면 드롭아웃이 있을 때 코드와 경사 하강법이 잘 작동하는지      
함수만 보고 확인하는 것 외에 다른 방법이 필요하기 때문입니다.    



