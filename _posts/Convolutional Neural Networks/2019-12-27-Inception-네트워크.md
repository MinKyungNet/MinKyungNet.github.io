---
layout: post
title: "Inception 네트워크"
tags: [Inception Network, GoogLeNet]
categories: [Convolutional Neural Networks]
---

# 학습 목표
- 이전 시간에 배운 인셉션 네트워크 핵심 개념을 조합해서 구축을 해본다.

# 핵심 키워드
- 인셉션 네트워크
- 구글넷

# 학습 내용
![image](https://user-images.githubusercontent.com/50114210/71513800-3f1e7480-28df-11ea-93f5-4a4a512ed2d2.png)     
- 인셉션 네트워크는 위 그림과 같이 여러개의 인셉션 모듈로 구성되어있습니다.

![image](https://user-images.githubusercontent.com/50114210/71513830-5bbaac80-28df-11ea-8c01-f70b22a96153.png)        
- 중간 중간에 차원을 바꾸기 위한 최대 풀링층을 포함해서 여러개의 인셉션 블록이 계속 반복되는 것을 볼 수 있습니다.
- 인셉션 네트워크는 구글넷이라고도 합니다.

# 인셉션 모듈
![image](https://user-images.githubusercontent.com/50114210/71513997-1e0a5380-28e0-11ea-8d23-b413513cb0c5.png)            
인셉션 모듈은 활성값이나 이전 층의 출력을 입력값으로 받습니다         
그래서 이것을 이전 영상과 같이 28 x 28 x 192 라고 해봅시다         
우리가 살펴본 예시는 1 x 1 과 5 x 5 의 층을 사용한 것인데          
1 x 1 의 경우에는 16 개의 채널이 있었고          
5 x 5 의 경우에는 28 x 28 x 32 의 출력 크기를 가졌습니다         
이것이 이전 영상의 마지막 슬라이드에서 본 것이죠         
3 x 3 합성곱의 계산을 줄이기 위해 동일한 것을 수행할 수 있습니다            
그리고 3 x 3 의 경우에는 28 x 28 x 128 의 출력이 있겠죠             
또 1 x 1 합성곱을 해줄 수도 있습니다            
1 x 1 합성곱에는 또 다른 1 x 1 합성곱이 필요하지는 않습니다            
그리고 출력으로는 28 x 28 x 64 가 나온다고 합니다            
마지막으로는 풀링 층이 있습니다 여기서 조금 우스운 일을 할 것입니다            
이 결과들을 엮어주기 위해서 풀링에 동일 패딩을 적용해서            
높이와 너비가 28  x 28 로 유지되어서 다른 결과와 엮을 수 있게 됩니다            
하지만 동일 패딩을 가진 최대 풀링을 사용한다면            
3 x 3 필터에 스트라이드를 1 로 놓는다면 그 결과는 28 x 28 x 192 의 크기를 가집니다            
입력과 동일한 수의 채널을 가지게 되죠 딱 보기에 채널이 너무 많아 보입니다            
그래서 여기 1 x 1 의 합성곱 층을 추가해줘서 영상에서 본 것 처럼             
채널의 수를 줄여서 28 x 28 x 32 로 만드는 것이죠             
그러기 위해서는 32 개의 1 x 1 x 192 필터가 필요합니다            
그래서 출력의 채널 수가 32 로 줄어드는 것이죠 풀링 층이 채널을 모두 차지하지 않게 말입니다            
마지막으로 이 블록들을 모아서 연결해 줍니다            
64 + 128 + 32 + 32 개를 하나로 연결하게 되면 28 x 28 x 256 크기가 됩니다            
채널 연결은 이전 영상에서 본 블록들을 연결해주는 것입니다            
이것이 하나의 인셉션 모듈입니다 인셉션 네트워크는 이런 모듈들을 하나로 모아놓은 것이죠            

# 인셉션 네트워크 구조
![image](https://user-images.githubusercontent.com/50114210/71514003-25316180-28e0-11ea-9861-b6bd46eb0138.png)               
여기 논문에서 가져온 인셉션 네트워크 그림이 있습니다            
많은 블록들이 반복되고 보기에 복잡해 보이지만             
하나의 블록을 살펴보면 좀 전에 배운 인셉션 모듈입니다            
굳이 설명할 필요도 없이 이것도 인셉션 블록이고             
여기에는 차원을 바꾸기 위한 최대 풀링층이 추가적으로 있고            
이것도 하나의 인셉션 블록이고 또 최대 풀링이 있지만 역시나 또 다른 인셉션 블록입니다            
인셉션 네트워크는 방금 배운 이런 블록들이 네트워크의 서로 다른 곳에 반복되는 것입니다            
이전 슬라이드에서 배운 인셉션 블록을 이해했다면 인셉션 네트워크도 이해할 수 있을 것입니다            

# 곁가지들
![image](https://user-images.githubusercontent.com/50114210/71514013-2f536000-28e0-11ea-982e-779ec01ece84.png)                
원래의 논문을 읽으면 인셉션 네트워크에 대한 또 다른 점이 있는데 이러한 곁가지들이 있다는 것입니다         
네트워크의 마지막 몇 개의 층은 완전 연결 층이고 그 뒤에는 예측을 위한 소프트맥스 층이 있는데         
이 곁가지가 하는 일은 은닉 층을 가지고 예측을 하는 것입니다         
그래서 이 부분이 소프트맥스죠 그리고 또 다른 곁가지도 은닉층을 가지고         
완전 연결 층을 지나서 소프트맥스로 결과를 예측합니다         
이것을 인셉션 네트워크의 또 다른 세부 사항이라 할 수 있지만         
은닉층이나 중간 층에서 계산된 특성들이라도 이미지의 결과를 예측하는데 아주 나쁘지는 않다는 것입니다         
인셉션 네트워크에 정규화 효과를 주고 네트워크의 과대적합을 방지해 줍니다         

# GoogLeNet
![image](https://user-images.githubusercontent.com/50114210/71514023-38443180-28e0-11ea-9c44-097f6184b837.png)                
그리고 이 인셉션 네트워크는 Google 의 일원에 의해 개발되어서 GoogLeNet 이라고 불립니다         
이전 영상에서 배웠었던 LeNet 에 경의를 표하기 위해서죠         
딥러닝 사회는 협력적인 것이 매우 큰 장점입니다         
딥러닝 사회에서 서로에 대한 강하고 건전한 존경이 있죠         
마지막으로 인셉션 네트워크 라는 이름은 어디에서 왔을까요         
논문은 실제로 "더 깊이 가야해" 라는 말을 인용하고 있습니다         
이 이미지로 연결되는 이 URL 은 실제로 논문에 언급되어 있습니다         
"Inception" 이라는 영화를 봤다면 이 말이 이해가 될 것입니다         
작자는 이 밈을 더 깊은 신경망의 동기라고 했습니다 이렇게 인셉션 구조를 구상하게 된 것이죠         
논문에서 이런 인터넷 밈을 인용하는 것이 흔치는 않지만 이 경우에는 꽤나 좋았다고 생각합니다         

# 정리
정리하자면 인셉션 모듈을 이해하면 인셉션 네트워크를 이해할 수 있습니다         
인셉션 모듈이 네트워크 상에서 반복되는 것이기 때문이죠         
기존의 인셉션 모듈이 개발되던 때부터 계속 새롭게 만들고 다른 버전을 내왔습니다         
그래서 새로운 인셉션 알고리즘에 대한 논문도 있죠         
그래서 사람들이 이후에 나온 인셉션 v2, 인셉션 v3, 인셉션 v4 등을 사용하는 것도 볼 수 있습니다         
그리고 그 중 하나는 ResNet 의 스킵 연결을 활용하는데 훨씬 더 나은 성능을 보여줍니다         
하지만 그 모든 것들이 이번에 배운 것에 기초를 두고 있습니다         
다수의 인셉션 모듈을 모아서 쌓는 개념 말이죠         
그래서 이 영상으로 인셉션 논문을 읽고 이해할 뿐만 아니라         
다른 버전의 논문도 이해할 수 있어야 합니다         
이것으로 꽤 많은 특수한 신경망의 구조를 살펴보았습니다         
