---
layout: post
title: "Resnets"
tags: [Residual Networks, skip connection, short cut, residual block, plain network]
categories: [Convolutional Neural Networks]
---

# 학습 목표
- Residual Networks를 학습한다.

# 핵심 키워드
- Residual Networks
- 스킵 연결 / 지름길
- 잔여 블록
- 평형망

# 학습 내용
- 아주 깊은 신경망을 학습하지 못하는 이유 중 하나는 경사가 소실되거나 폭발적으로 증가하기 때문입니다. 하지만 ResNet에서는 스킵 연결로 이 문제를 해결했습니다.
- 잔여 블록을 설명하기 전에 아래의 두층의 신경망이 있다고 생각해봅시다.
- 아래 그림처럼 모든 층을 지나는 연산 과정을 main path라고 부릅니다. 즉 a[l]의 정보가 a[l+2]로 흐르기 위해서는 모든 과정을 거쳐야합니다.
![image](https://user-images.githubusercontent.com/50114210/71479349-5e61c700-2837-11ea-82ff-cc28067ed635.png)         

- 하지만 ResNet에서는 아래 그림 처럼 z[l+2]에 비선형성을 적용해주기 전에 a[l]을 더하고 이것에 다시 비선형성을 적용해줍니다.
![image](https://user-images.githubusercontent.com/50114210/71479370-79ccd200-2837-11ea-98f1-c0431e1f29c7.png)         

- a[l]을 더해서 다시 활성화 함수에 넣는 부분까지를 잔여 블록이라고 합니다. 지름길 혹은 스킵 연결은 a[l]의 정보를더 깊은 층으로 전달하기 위해 일부 층을 뛰어 넘는 역할입니다.
- ResNet은 여러개의 잔여 블록으로 구성되어있습니다. ResNet을 만드려면 평형망에 스킵 연결을 더해주면 됩니다.
![image](https://user-images.githubusercontent.com/50114210/71479407-a08b0880-2837-11ea-94d7-dcb4bef4457b.png)       

- 또한, 경험적으로 층의 개수를 늘릴 수록 훈련 오류는 감소하다가 다시 증가합니다. 하지만 이론 상으로는 신경망이 깊어질 수록 훈련세트에서 오류는 계속 낮아져야합니다. 하지만 ResNet에서는 훈련오류가 계속 감소하는 성능을 가질 수 있습니다.
![image](https://user-images.githubusercontent.com/50114210/71479448-cf08e380-2837-11ea-8190-e23d4b965254.png)        

# 인트로
아주 깊은 신경망을 훈련시키기 어려운 이유는 경사가 소실되거나 경사가 폭발적으로 증가하는 문제 때문입니다       
이 영상에서는 스킵 연결에 대해서 배울 것입니다 한 층의 활성값을 가지고 훨씬 깊은 층에 적용하는 방식이죠       
이것을 통해서 ResNet 을 형성할 수 있습니다 100 개도 넘는 깊이의 신경망을 학습할 수 있게 해주죠       
ResNet 은 잔여 블록이라는 것으로 구성되어 있는데 이것이 무엇인지 설명해 드리겠습니다       

# ResNet
![image](https://user-images.githubusercontent.com/50114210/71479676-c369ec80-2838-11ea-8c25-18f3b7291033.png)                       
여기 신경망의 두 층이 있습니다              
활성값 a^[l] 에서 시작해서 a^[l+1] 으로 가고 두 층이 지난 뒤 활성값은 a^[l+2] 입니다             
이 계산 과정을 한 번 살펴보면 a^[l] 에 선형 연산자를 적용해 줍니다             
이러한 식으로 나타내죠 a^[l] 에서 z^[l+1] 을 계산하기 위해서는 가중치 행렬을 곱해주고 편향 백터를 더해줍니다             
그 뒤에는 ReLU 비선형성을 적용해 a^[l+1] 을 계산합니다 이러한 식으로 나타내죠             
a^[l+1] = g(z^[l+1]) 그 다음 층에서는 또 선형 연산을 적용해주면              
이러한 식으로 나오는데 왼쪽의 이 식과 비슷합니다             
그리고 마지막으로 또 한 번 ReLU 연산을 적용해줍니다             
이러한 식으로 나타내고 g 가 ReLU 비선형성이죠 그리고 이것으로 a^[l+2] 를 얻습니다             
다시 말해 a^[l] 의 정보가 a^[l+2] 로 흐르기 위해서는 이 모든 과정을 거쳐야 합니다             
이것을 여러 층의 "main path" 라 부르고 ResNet 에서는 이것을 조금 바꿔서 a^[l] 을 복제해서             
ResNet 에서는 이것을 조금 바꿔서 a^[l] 을 복제해서 신경망의 더 먼 곳까지 단번에 가게 만든 뒤             
ReLU 비선형성을 적용해주기 전에 a^[l] 을 더해주고 이것을 "short cut" 이라고 부를 겁니다             
그러면 "main path" 를 따르는 대신 a^[l] 의 정보는              
"short cut" 을 따라서 신경망의 더 깊은 곳으로 갈 수 있겠죠             
이것으로 마지막 식이 필요가 없어지고 대신 a^[l+2] = g(z^[l+2]) 에 a^[l] 을 더해주고              
이 a^[l] 은 잔여 블록이 됩니다             
위에 있는 그림에서도 여기로 가는 지름길을 그려주면 되죠             
두 번째 층으로 가는 이유는 ReLU 전에 더해지기 때문입니다             
여기 각 노드들은 선형 연산과 ReLU 를 적용하는데 a^[l]은              
선형 연산 뒤에 삽입되고 ReLU 연산 전에 들어갑니다             
그리고 "short cut" 대신 스킵 연결이라는 표현을 사용하기도 하는데             
a^[l] 이 정보를 전달하기 위해 층을 뛰어넘는 걸 의미합니다 신경망의 더 깊은 곳으로 말이죠             

# 깊은 모델의 학습을 가능하게 함
![image](https://user-images.githubusercontent.com/50114210/71479679-cc5abe00-2838-11ea-996f-242284c91b67.png)                      
그리고 ResNet 의 개발자인 Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun이 발견한 것은             
잔여 블록을 사용하면 훨씬 깊은 신경망을 훈련시킬 수 있다는 것입니다             
그래서 ResNet 을 구축하는 방법은 이러한 잔여 블록들을 쌓아서 깊은 신경망을 만드는 것입니다             
이 네트워크를 살펴봅시다 ResNet 은 아니고 ResNet 논문의 표현으로는 평형망이라고 불리는데             
이것을 ResNet 으로 바꾸려면 스킵 연결을 더해주면 됩니다 이렇게 말이죠              
그래서 두 층 마다 이전 슬라이드에서 본 잔여 블록으로 만들어 주기 위한 변화가 생깁니다             
이 그림은 다섯 개의 잔여 블록이 합쳐진 것이고 이것이 ResNet 입니다             
만약 표준 최적화 알고리즘을 사용한다면 경사 하강법이나              
다른 멋진 최적화 알고리즘을 사용해 평형망을 훈련시키고 방금 그린 스킵 연결을 사용하지 않고 말이죠             
그리고 경험적으로 층의 개수를 늘릴 수록 훈련 오류는 감소하다가 다시 증가하는데             
이론 상으로는 신경망이 깊어질 수록 훈련 세트에서 점점 더 나아져야 합니다             
이론 상으로는 깊은 신경망이 도움이 되지만 실제로는 평형망의 깊이가 매우 깊다면             
최적화 알고리즘으로 훈련을 하는 것이 더 어려워질 것이고             
너무 깊은 신경망을 선택하면 훈련 오류는 더 많아집니다             
하지만 ResNet 에서는 층이 깊어져도 훈련 오류가 계속 감소하는 성능을 가질 수 있습니다             
100 개 이상의 층을 훈련시키더라도 말이죠             
그리고 몇몇의 사람들은 수 천개의 층을 가진 신경망을 실험 중입니다              
아직 실용적인 것 같아 보이진 않지만 말이죠             
활성값 x 또는 중간 활성값을 취하는 것으로 훨씬 더 깊은 신경망을 사용할 수 있게 해줍니다             
이것이 경사 소실 문제에 많은 도움이 되고 더 깊은 신경망을 훈련시키게 해줍니다 성능의 큰 저하 없이 말이죠             
언젠가는 이것들이 펼쳐져서 더 이상 깊어지지 못할 수도 있지만              
ResNet 이 깊은 신경망 훈련에 효과적인 것은 분명합니다             
