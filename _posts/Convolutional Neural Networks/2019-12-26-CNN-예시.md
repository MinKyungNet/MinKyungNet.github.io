---
layout: post
title: "CNN 예시"
tags: [convolutional neural network, convoloution layer, pooling layer, fully connected layer]
categories: [Convolutional Neural Networks]
---

# 학습 목표
- 합성곱 신경망에 필요한 요소들을 종합하여 합성곱 신경망의 예제를 살펴본다.

# 핵심 키워드
- 합성곱 신경망
- 합성곱 층
- 풀링 층
- 완전 연결 층

# 학습 내용
- LeNet-5라는 고전적인 신경망과 유사한 구조를 살펴봅니다.
![image](https://user-images.githubusercontent.com/50114210/71462293-3736d580-27f6-11ea-8fb2-24992066a6ed.png)       
- 합성곱 신경망의 분야에서는 두 종류의 관습이 있는데, 하나는 합성곱 층과 풀링 층을 하나의 층으로 보고, 다른 하나는 합성곱 층과 풀링 층을 각각의 층으로 간주하는 것입니다. 여기서는 전자의 방법을 사용합니다. 풀링 층에 학습해야할 변수가 없기 때문에 합성곱 층과 풀링 층을 하나로 간주합니다.

# 입력이미지, LeNet-5
![image](https://user-images.githubusercontent.com/50114210/71462790-d27c7a80-27f7-11ea-894e-90a1d7449589.png)                     
32 x 32 x 3 의 RGB 이미지가 있다고 해봅시다                 
여기서 손글씨를 인식할 때 7 이라는 글자가 32 x 32 의 RGB 이미지에 있고                  
0 부터 9 사이의 숫자 중에 그것을 알아내려 합니다 이것을 실행하는 신경망을 한 번 구축해 봅시다                 
LeNet-5 이라는 고전적인 신경망과 유사한 방법을 사용할 것입니다                 
Yann LeCun 에 의해 수 년 전에 개발된 것인데 지금 사용하는 것이 정확히 LeNet-5 는 아니지만                 
여러 변수들의 선택이 여기서 영감을 얻었습니다                 
                 
# 첫번째 레이어
![image](https://user-images.githubusercontent.com/50114210/71462804-dd370f80-27f7-11ea-8ab6-d2c7d71113f4.png)                       
32 x 32 x 3 의 입력에서 첫 번째 층이 5 x 5 필터와 1 의 스트라이드 그리고 패딩은 사용하지 않을 때                 
6 개의 필터를 사용한 출력은 28 x 28 x 6 이 됩니다 이 층을 CONV1 이라고 부를 겁니다                  
6 개의 필터와 편향을 적용하고 ReLU 비선형성을 적용하면 CONV1 의 출력이 될 것입니다                 
다음으로는 풀링 층을 적용해봅시다 여기에서는 최대 풀링을 사용할 것인데                  
f = 2, s = 2 이고 패딩을 적지 않을 때는 0 이라는 의미입니다                 
2 x 2 크기의 필터와 2 의 스트라이드를 사용하게 되면 높이와 너비의 값을 절반으로 줄어들게 만듭니다                 
그래서 28 x 28 이 14 x 14 로 변하게 되고                 
채널의 수는 유지되기 때문에 14 x 14 x 6 이 됩니다                 
이것을 POOL1 이라고 부를 것입니다                 

# 레이어를 부르는 두가지 기준
합성곱 신경망의 분야에서는 두 종류의 관습이 있는데 층이라고 부르는 것과 조금은 맞지 않습니다           
그 중 하나는 이것을 하나의 층이라고 하고 그래서 이것이 신경망의 Layer 1 이 되는 것이고           
다른 하나는 합성곱 층과 풀링 층을 각각 층으로 여기는 것입니다           
일반적으로 신경망의 층의 개수를 말할 때 가중치와 변수를 가지는 층만 말하고는 합니다           
풀링 층은 가중치와 변수가 없고 하이퍼파라미터만 있기 때문이죠           
여기서는 CONV1 과 POOL1 을 하나의 층인 Layer1 으로 부르겠습니다           
온라인 상의 문헌이나 논문을 읽어보면 합성곱 층과 풀링 층을 별개의 층으로 여기는 경우도 있습니다           
서로 일치하지 않는 두 개의 관습이죠           
하지만 층의 개수를 셀 때에는 가중치가 있는 층만 계산해서 이것을 Layer 1 으로 묶어줄 겁니다           
CONV1 과 POOL1 이라는 이름에서 끝에 1 이라는 숫자는 둘 모두 신경망의 Layer 1 에 속해 있다는 의미입니다           
POOL1 은 가중치가 없기 때문에 Layer 1 에 함께 묶이게 되죠           

# 두번째 레이어
![image](https://user-images.githubusercontent.com/50114210/71462816-e9bb6800-27f7-11ea-97d1-99e3c5ba2129.png)                          
여기 또 다른 합성곱 층을 적용해 봅시다                  
5 x 5 필터를 사용해서 f = 5 가 되고 스트라이드는 1 이고 패딩은 없습니다                  
이것이 CONV2 의 출력이 될 것이고 16 개의 필터가 있다고 하면                   
10 x 10 x 16 크기의 출력이겠죠 이것이 CONV2 층입니다                  
그리고 최대 풀링을 적용하면 f = 2, s = 2 이고 이것의 출력을 예상할 수 있습니다                  
f = 2, s = 2 의 최대 풀링은 높이와 너비를 절반으로 만듭니다                  
이것의 출력을 예상할 수 있습니다                   
f = 2, s = 2 의 최대 풀링은 높이와 너비를 절반으로 만들어서 5 x 5 x 16 의 크기를 가집니다                  
이전과 동일한 채널 수이죠 이것을 POOL2 로 부를 겁니다                  
우리의 방식대로라면 이것이 Layer 2 입니다 하나의 가중치만 있기 때문이죠                  

# 세번째 레이어
![image](https://user-images.githubusercontent.com/50114210/71462882-17a0ac80-27f8-11ea-9dcf-be9fadc535ff.png)                         
5 x 5 x 16 은 400 입니다 그래서 POOL2 를 펼쳐서                  
400 x 1 의 백터로 만들어 줍니다 마치 여러 개의 뉴런 같이 말이죠                  
그 다음으로 할 것은 이 400 개의 유닛을 가지고 120 개의 유닛을 가진 층을 만들어 봅시다                  
이것이 우리의 첫 번째 완전 연결 층인 FC3 입니다                  
왜냐하면 400 개의 유닛이 120 개의 유닛과 빽빽이 연결되어 있기 때문이죠                  
이 완전 연결 층은 마치 단일 신경망 층과 유사합니다 표준 신경망에 해당하죠                  
w[3] 라는 가중치 행렬이 있는데 (120, 400) 의 크기를 가지고                  
완전 연결이라 불리는 이유는 400 개의 유닛이 각각 120개의 유닛과 연결되어 있기 때문이죠                  
그리고 편향 변수도 있는데 그냥 120 의 크기를 가집니다 120 개의 출력이 있기 때문이죠                  

# 마지막 레이어
![image](https://user-images.githubusercontent.com/50114210/71462897-212a1480-27f8-11ea-82ab-5a00db4b2010.png)               
마지막으로 120 개의 유닛에 이번에는 좀 더 작은 층을 더해봅시다        
84 개의 유닛이 있고 FC4 라고 부릅시다        
그러면 소프트맥스 유닛에 적용 가능한 84 개의 열 번호가 생깁니다       
만약 손글씨를 인식하려 한다면 0, 1, 2, ..., 9 까지의 숫자를 인식하려고 할 때       
이것은 10 개의 출력을 가진 소프트맥스가 됩니다 이것이 합성곱 신경망의 일반적인 예입니다       

# 하이퍼파라미터 설정
많은 하이퍼파라미터가 있는 것처럼 보이는데 이 하이퍼파라미터를 선정하는 방법을 알려드리겠습니다       
하나의 지침은 직접 하이퍼 파라미터를 선정하지 말고        
문헌에서 다른 사용자들에게 작동했던 하이퍼 파라미터를 보고        
자신의 응용프로그램에도 잘 작동할 구조를 선택하는 것입니다       

# CNN의 패턴
![image](https://user-images.githubusercontent.com/50114210/71462905-2ab37c80-27f8-11ea-9a21-fc3af8cbd9b3.png)               
하지만 지금 시점에서는 신경망이 깊어질 수록 n_H 와 n_W 높이와 너비는 감소합니다       
실제 값으로 살펴보면 32 x 32 에서 28 x 28 에서 14 x 14 에서 10 x 10 에서 5 x 5 로       
더 깊어질 수록 높이와 너비는 감소합니다       
그에 반해 채널의 수는 3 에서 6 에서 16 으로 증가하고 마지막에는 완전 연결 층이 있죠       

# CNN의 패턴
![image](https://user-images.githubusercontent.com/50114210/71462929-37d06b80-27f8-11ea-887e-015324091348.png)                  
또 다른 신경망의 흔한 패턴은 합성곱 층 뒤에 풀링 층이 있고 또 몇 개의 합성곱 층 뒤에 풀링 층이 있고                  
마지막에는 몇 개의 완전 연결 층과 소프트맥스가 있습니다                  
이것이 신경망에서 볼 수 있는 또 다른 흔한 패턴 중에 하나죠                  
그래서 이 신경망을 좀 더 자세히 살펴볼텐데 활성값 형태와 크기와 변수의 개수입니다                  
입력은 32 x 32 x 3 이었고 세 수를 곱하면 3,072 를 얻습니다                  
그래서 활성값 a[0] 는 3,072 의 크기를 가집니다 실제로는 32 x 32 x 3 이죠                  
그리고 입력 층이라 변수는 따로 없습니다                  
다른 층 들을 보면 한 번 직접 계산해보면 이것들이 활성값 형태와 크기가 됩니다                  
한 가지 유의할 점은 최대 풀링 층은 변수가 따로 없다는 것이고                  
둘째로는 합성곱 층이 상대적으로 적은 변수를 간다는 것입니다                  
그리고 신경망의 대부분의 변수는 완전 연결 층에 있습니다                  
그리고 활성값의 크기도 신경망이 깊어질 수록 점점 감소합니다                  
너무 빠르게 감소한다면 성능이 좋지 않을 수 있습니다                  
6,000 정도에서 시작해서 1,600 정도로 감소하고 꾸준히 감소해서                   
84 개로 내려가고 결국 소프트맥스 출력이 되죠                  
많은 합성곱 신경망에서 이 같은 속성 혹은 패턴을 가집니다                  

# 아웃트로
지금까지 합성곱 신경망의 기본 구성 요소를 살펴봤습니다 합성곱 층과 풀링 층과 완전 연결 층이죠                  
많은 컴퓨터 비전 분야의 연구는 이 구성 요소를 어떻게 활용해서                  
효율적인 신경망을 구축할지 알아내는데 노력하고 있고                  
이것들을 합치는 것에는 일종의 통찰력이 필요합니다                  
여기에 대해서 좋은 직관을 얻는 방법은 여러 개의 구체적인 예를 살펴보는 것입니다                  
