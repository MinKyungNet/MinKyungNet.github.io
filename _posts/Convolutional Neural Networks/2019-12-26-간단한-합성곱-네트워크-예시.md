---
layout: post
title: "간단한 합성곱 네트워크 예시"
tags: [convolution layer, pooling layer, fully connected layer]
categories: [Convolutional Neural Networks]
---

# 학습 목표
- 간단한 합성곱 신경망 예제를 살펴본다.

# 핵심 키워드
- 합성곱 층
- 풀링 층
- 완전 연결 층

# 학습 내용
![image](https://user-images.githubusercontent.com/50114210/71458132-adcad780-27e4-11ea-97ba-da401da4e73d.png)    
- 합성곱 신경망의 크기는 깊어질 수록 점점 줄어듭니다.
- 대부분의 신경망에는 합성곱 층, 풀링 층, 완전 연결층으로 구성되어 있습니다.

# 인트로
이전 영상에서는 합성곱 신경망의 단일 합성곱 층의 구성 요소를 살펴보았습니다             
이번에는 심층 합성곱 신경망의 예시를 한 번 살펴봅시다             
그리고 이전 영상에서 본 표기법들에 대한 연습도 될 것입니다             

# 입력 크기
![image](https://user-images.githubusercontent.com/50114210/71458569-be7c4d00-27e6-11ea-8790-9de3cddbf02d.png)      
하나의 이미지가 있다고 해봅시다              
그리고 이 이미지를 분류하거나 인식하려고 할 때 x 라는 이미지를 입력받고              
이것이 고양이 인지 아닌지 0 혹은 1 의 분류 문제이죠              
이 문제에 활용될만한 합성곱 신경망을 형성해봅시다              
이 예시의 경우에는 적당히 작은 이미지인 39 x 39 x 3 를 사용해봅시다              
이것으로 몇몇의 숫자들이 계산하기에 편리해지죠              

# 첫번째 레이어
![image](https://user-images.githubusercontent.com/50114210/71458583-d522a400-27e6-11ea-8658-ab3af5fee7d2.png)       
0 번째 층의 n_H 는 n_W^[0] 와 같이 39 이고 0 번째 층의 채널의 수는 3 입니다              
첫 번째 층은 3 x 3 필터를 이용해 속성을 검출한다고 해보죠              
3 x 3 필터를 사용하기 때문에 f^[1] 은 3 입니다              
그리고 1 의 스트라이드를 사용하고 동일 합성곱이라 패딩은 없습니다              
그리고 10 개의 필터가 있다고 해봅시다              
그러면 신경망의 다음 층의 활성값은 37 x 37 x 10 이고 여기 10 은 10 개의 필터를 의미하고              
37 은 공식이었던 n + 2p - f / s + 1 에서 오죠              
그래서 39 + 0 - 3 / 1 + 1 은 37 이 됩니다 그래서 출력이 37 x 37 이 되는 것이죠              
유효 합성곱이고 이것이 출력의 크기입니다              
우리의 표기법에서는 n_H^[1] = n_W^[1] = 37 입니다              
그리고 n_C^[1] = 10 이며 첫 번째 층의 필터 개수와 같죠              
그리고 이것이 첫 번째 층의 활성값의 크기가 됩니다              

# 두번째 레이어
![image](https://user-images.githubusercontent.com/50114210/71458597-e075cf80-27e6-11ea-857b-adb3ce4b8e8f.png)            
그리고 또 다른 합성곱 층이 있다고 해봅시다             
그리고 이번에는 5 x 5 의 필터를 사용해서 신경망의 다음 층에 해당하는 f^[2] 는 5 가 되고             
이번에는 2 의 스트라이드를 사용하고 패딩은 없고 20 개의 필터가 있다고 해보죠             
이것의 출력은 17 x 17 x 20 의 크기를 가지게 됩니다             
이번에는 스트라이드를 2 로 놓아서 크기가 훨씬 빨리 줄어들었습니다             
37 x 37 이 절반보다 조금 큰 17 x 17 로 줄어들었죠             
그리고 20 개의 필터를 사용해서 채널의 수는 20 입니다             
그래서 활성값 a^[2] 는 이 크기를 가집니다 n_H^[2] = n_W^[2] = 17 이고 n_C^[2] 는 20 입니다             

# 세번째 레이어
![image](https://user-images.githubusercontent.com/50114210/71458614-eff51880-27e6-11ea-832c-f0da71ee10dc.png)      
마지막으로 하나의 합성곱 층을 더 적용해 봅시다             
5 x 5 의 필터와 2 의 스트라이드를 사용하면 계산은 생략하겠습니다             
7 x 7 이 되고 40 개의 필터를 사용하고 패딩 없이 40 개의 필터로는 7 x 7 x 40 가 됩니다             

# 네번째 레이어
![image](https://user-images.githubusercontent.com/50114210/71458630-03a07f00-27e7-11ea-9e25-f4c25aae506e.png)     
그래서 지금까지 한 일은 39 x 39 x 3 의 입력 이미지의 7 x 7 x 40 만큼의 특성을 계산한 것입니다             
그리고 일반적으로 해주는 것은 7 x 7 x 40 은 1960 인데 이것을 계속해서 펼쳐서 7 x 7 x 40 은 1960 인데             
이것을 계속해서 펼쳐서 1960 개의 요소로 만드는 것입니다             
펼쳐서 하나의 백터로 만든 뒤 로지스틱 회귀 유닛이나 소프트맥스 유닛에 넣게 되면             
고양이인지 아닌지 인식하거나 k 개의 물체를 인식하느냐 등             
방식의 차이에 따라서 이것이 신경망의 최종 예측값이 됩니다             
마지막 과정을 좀 더 분명히 하면 그냥 여기 모든 숫자들을 1960 개의 숫자들을 하나의 긴 백터에 넣고             
이 기다란 백터를 소프트맥스 함수나 로지스틱 회귀 함수에 넣어서 최종 예측을 합니다             
이것이 합성곱 신경망의 일반적인 예시입니다 하지만 합성곱 신경망을 디자인 하는 일은 대부분              
필터의 크기나 스트라이드나 패딩 또는 필터의 개수 같은 하이퍼파라미터를 선택하는 과정입니다             
이번 주와 다음 주에 이 값들을 선정하는 지침을 제시할 것입니다             

# 일반적인 합성곱 신경망 양상
하지만 이 시점에서 알 수 있는 것은 신경망이 깊어질 수록             
39 x 39 같은 더 큰 이미지에서 시작해서 높이와 너비가 비슷하게 유지되다가 신경망이 깊어질 수록 줄어듭니다             
39 에서 37, 17, 7 로 변했습니다 반면에 채널의 수는 3 에서 10, 20, 40 으로 늘어났습니다             
그리고 많은 합성곱 신경망에서 이러한 경향을 볼 수 있을 것입니다             
하지만 이후 영상에서 이런 변수들을 선정하는 지침을 드릴 겁니다             

# 세 종류의 레이어
![image](https://user-images.githubusercontent.com/50114210/71458652-161ab880-27e7-11ea-9382-3a63c33f2185.png)       
일반적인 합성곱 신경망에는 세 종류의 층이 있습니다              
하나는 합성곱 층이고 주로 CONV 라는 층으로 표현되고 바로 지금까지의 신경망에서 사용하던 것입니다             
그리고 아직 보지 못했던 두 종류의 층이 있습니다             
이후의 영상에서 다룰 것인데 그 중 하나는 풀링 층인데 POOL 층으로 불리고             
마지막으로는 완전 연결 층입니다 FC 로 표현하죠             

# 아웃트로
비록 합성곱 층만 사용해도 충분히 좋은 신경망을 설계할 수 있지만             
대부분의 신경망의 구성에는 풀링 층과 완전 연결 층이 있습니다             
다행히도 풀링 층과 완전 연결 층은 합성곱 층보다 정의가 간단합니다             
그래서 다음 두 영상에서 빠르게 다루어 볼 것이고             
그러면 합성곱 신경망의 가장 흔한 층에 대해 이해가 될 것입니다             
그리고 같이 조합했을 때 더 강력한 신경망을 형성할 수도 있습니다             
