---
layout: post
title: "고전적인 네트워크들"
tags: [LeNet-5, AlexNet, VGG]
categories: [Convolutional Neural Networks]
---

# 학습 목표
- 고전적인 신경망 구조들을 살펴본다.

# 핵심 키워드
- LeNet-5
- AlexNet
- VGG

# 학습 내용
### LeNet-5
- LeNet-5의 목적은 흑백으로 된 손글씨 인식입니다. 네트워크 구조는 아래와 같습니다.
![image](https://user-images.githubusercontent.com/50114210/71474336-53e70380-281e-11ea-9d7f-baacf476c869.png)        
- 해당 신경망은 요즘에 비하면 상대적으로 적은 변수를 가지고 있습니다.
- 다만, 해당 논문에서는 요즘과는 다르게 풀링층 뒤에 비선형함수를 적용했습니다. 비선형함수도 ReLU가 아닌 Sigmoid를 적용했습니다.

### AlexNet
- AlexNet의 목적은 이미지를 1000개에 해당하는 클래스로 분류하는 것이었습니다. AlexNet의 네트워크 구조는 아래와 같습니다.
![image](https://user-images.githubusercontent.com/50114210/71474429-b9d38b00-281e-11ea-9be2-fb5d35a67d6c.png)        
- LeNet에 비해서 굉장히 많은 변수를 가지고 있습니다. 또한, 활성화 함수로 ReLU를 사용했습니다.
- 합성곱을 같게 가져간다라는 말은 이전 층의 높이와 넓이를 같게 만드는 패딩을 가진다는 것과 같은 말입니다.

### VGG-16
- AlexNet의 복잡한 구조에 비해, VGG Net은 더 간결한 구조를 가지고 있습니다.
![image](https://user-images.githubusercontent.com/50114210/71474465-e4254880-281e-11ea-8f14-254ddff684e4.png)     
- 이 모델의 특징은 모든 합성곱 연산은 3 * 3의 필터를 가지고 패딩 크기는 2, 스트라이드는 1로하고, 2 * 2 픽셀씩 최대 풀링하는 것입니다.
- 산출값의 높이와 넓이는 매 최대 풀링마다 1/2씩 줄어들며, 채널의 수는 두배 혹은 세배로 늘어나게 만드는 것이 VGG모델의 체계적인 점입니다.
- 다만, 훈련시킬 변수의 개수가 많아 네트워크의 크기가 커진다는 단점이 있습니다.

# LeNet-5
![image](https://user-images.githubusercontent.com/50114210/71476470-9f061400-2828-11ea-82be-f8a87c1c909b.png)      
여기 LeNet-5 구조가 있습니다 32 x 32 x 1 의 이미지가 있을 때               
LeNet-5 의 목적은 손글씨의 숫자를 인식하는 것입니다              
그리고 LeNet-5 는 흑백의 이미지에 훈련되었기 때문에 32 x 32 x 1 인 것입니다              
이 신경망의 구조는 저번 주의 마지막 예시와 유사합니다              
첫 번째로는 6 개의 5 x 5 필터와 1 의 스트라이드를 사용합니다              
6 개의 필터를 사용하기 때문에 28 x 28 x 6 의 결과를 얻게 되죠              
그리고 스트라이드는 1 이고 패딩을 사용하지 않기 때문에 32 x 32 가 28 x 28 로 감소합니다              
그리고 LeNet 신경망에서 풀링을 적용시키는데 이 당시에는 평균 풀링을 더 많이 사용했습니다              
만약 요즘 시대였다면 아마 최대 풀링을 사용했겠죠              
하지만 이 예시에서는 평균 풀링을 사용하고 필터 크기가 2 이고 스트라이드가 2 여서              
높이와 너비의 크기를 절반으로 줄여서 14 x 14 x 6 가 됩니다              
그 다음에는 또 다른 합성곱 층을 적용하는데 이번에는 16 개의 5 x 5 필터를 사용해 16 개의 채널이 만들어집니다              
이 논문이 쓰여질 당시인 1998 년에는 패딩을 사용하지 않거나 유효 합성곱을 사용했습니다              
그래서 합성곱 층을 적용할 때마다 높이와 너비가 감소했습니다              
그래서 14 x 14 이 10 x 10 으로 줄어든 것이죠              
또 하나의 풀링 층으로 높이와 너비가 절반인 5 x 5 로 변하고               
이 숫자들을 곱하면 5 x 5 x 16 은 400 입니다 25 x 16은 400이기 때문이죠              
다음 층은 완전 연결 층인데 400 개의 노드를 120 개의 뉴런에 각각 연결해줘서 완전 연결 층을 만들어 줍니다              
이 경우에는 대충 그린 것이라 400 개의 노드는 생략되어 있습니다              
또 다른 완전 연결 층이 있고 마지막 과정은 이 84 개의 속성을 가지고 하나의 최종 출력에 적용합니다              
여기 노드 하나를 더 그려서 y 의 예측값을 나타내고               
그리고 y 의 예측값은 10 가지 가능성이 있는데 0 부터 9 까지의 숫자 인식에 해당합니다              
요즘이라면 소프트맥스 층을 사용해서 10 분류의 결과과 나오게 되지만              
LeNet-5 는 요즘은 잘 사용하지 않는 분류기를 출력 층에서 사용하죠              
이 신경망은 요즘 기준으로 적은 60,000 개의 변수를 가집니다              
요즘에는 1000만 또는 1억 개의 변수를 가지는 신경망도 볼 수 있죠              
이것보다 1000배 정도 큰 신경망을 보는게 그리 어려운 일은 아닙니다              
하지만 요즘도 깊이가 깊어질 수록 신경망의 왼쪽에서 오른쪽으로 이동할 때 높이와 너비가 감소합니다              
32 에서 28 에서 14 에서 10 그리고 5 로 변하고 채널의 수는 증가합니다         
1 에서 6 에서 16 으로 신경망의 층이 깊어질 수록 말이죠         
이 신경망에서 볼 수 있고 요즘에도 사용되는 또 다른 패턴은 몇 개의 합성곱 층 뒤에         
풀링 층이 따라오고 또 합성곱 층에 풀링 층이 또 따라서 온 뒤         
완전 연결 층이 있고 그 다음에 출력이 있는 것입니다         
이러한 층의 배치는 요즘도 흔하죠         
마지막으로는 이것은 논문을 읽을 사람에게만 해당될 수 있는데 여러 가지 다른 점들이 있습니다         
이후의 슬라이드는 논문을 읽으려는 사람들에게 해당되는 말들입니다         
그래서 빨간색으로 쓴 글씨는 마음 편히 생략해도 됩니다         
흥미롭지만 부가적인 요소라 이해하지 못해도 괜찮습니다         
그래서 원래의 논문을 읽어보면 당시 사람들은 시그모이드와 tanh 비선형성을 사용했는데         
당시에는 ReLU 비선형성을 사용하지 않았습니다         
그래서 논문을 읽어보면 시그모이드와 tanh 가 언급되어 있습니다         
요즘의 기준으로 이 신경망이 조금은 우스운 점은         
n_H x n_W x n_C 의 신경망에서 n_C 의 채널이 있을 때 f x f x n_C 크기의 필터를 사용해         
필터가 모든 채널을 살펴보게 하는데 반해 당시에는 컴퓨터가 훨씬 느려서         
변수처럼 계산을 줄이기 위해 초기의 LeNet-5 는 각각의 필터가 서로 다른 채널에 적용되었습니다         
논문에서는 상세히 설명하지만 요즘에는 이런 복잡한 걸 사용하지 않습니다         
또 한 가지 당시에는 사용됐고 지금은 사용되지 않는 것은 기존의 LeNet-5 는 비선형성이 풀링 뒤에 있었습니다         
시그모이드 비선형성을 풀링 층 뒤에 적용했겠죠         
이 논문을 살펴보면 다음 영상에서 살펴볼 것보다 조금 더 읽기 까다롭습니다         
다음 것은 아마 좀 더 쉬울 것입니다 이 슬라이드에 있는 내용은 대부분 논문의 섹션 2, 3 에 있습니다         
그 이후의 섹션에서는 또 다른 개념을 다루고 있죠 그래프 변형 신경망을 다루는데 요즘에는 많이 사용되지 않습니다         
그래서 만약 이 논문을 읽는다면 섹션 2 에 초점을 두는 것을 추천해 드립니다         
이런 구조에 대해서 설명하고 있고 또 섹션 3 을 훑어보면 여러 개의 실험 결과가 있습니다         

# AlexNet
![image](https://user-images.githubusercontent.com/50114210/71476480-aa593f80-2828-11ea-94f0-be1782132308.png)             
두 번째로 보여줄 신경망은 논문의 첫 저자인 Alex Krizhevsky 의 이름을 딴 AlexNet 입니다            
다른 저자는 Ilya Sutskever 와 Geoffrey Hinton 이죠            
AlexNet 의 입력은 227 x 227 x 3 의 이미지로 시작하는데             
실제로 논문에서는 224 x 224 x 3 의 이미지를 사용하지만            
숫자를 살펴보면 227 x 227 일 때 좀 더 그럴듯한 숫자가 됩니다            
첫 번째 층에서는 96 개의 11 x 11 필터를 사용하고             
4 의 스트라이드를 이용해서 크기가 55 x 55 로 줄어듭니다            
스트라이드의 값 때문에 크기가 4 분의 1 로 줄어드는 것이죠            
그리고 3 x 3 의 크기와 2 의 스트라이드인 최대 풀링을 적용하면 크기가 27 x 27 x 96 으로 변하게 됩니다                  
그리고 5 x 5 의 동일 합성곱 연산을 실행해주면 27 x 27 x 256 로 크기가 변하게 됩니다                  
또 한 번 최대 풀링을 적용해주면 높이와 너비가 13 으로 줄어들고                  
그리고 또 동일 합성곱을 적용해주면 13 x 13 x 384 의 필터로 변하고 그리고 3 x 3 의                  
동일 합성곱을 하면 이렇게 나오고 또 3 x 3 합성곱을 하면 이것이 나오고                   
최대 풀링을 적용하면 6 x 6 x 256 로 변하게 되죠 6 x 6 x 256 을 계산해보면 9216 이 됩니다                  
그래서 이것을 전개해서 9216 개의 노드로 만들면완전 연결 층을 가지게 되고                  
소프트맥스를 사용해서 가능한 1,000 개의 출력을 나타내게 됩니다                  
이 신경망은 LeNet 과 매우 유사하지만 훨씬 큰 크기를 가집니다                  
LeNet 이나 LeNet-5 는 6만 개 정도의 매개 변수를 가졌지만                  
AlexNet 의 경우는 6천만 개 정도의 매개 변수를 가지죠 실제로 유사한 구성 요소를 가질 수 있지만                  
더 많은 은닉 유닛과 더 많은 데이터를 통해 훈련하기 때문에                  
훨씬 더 뛰어난 성능을 보여줄 수 있는 것입니다                  
이 구조가 LeNet 과 구별되는 또 다른 하나의 특성은 ReLU 활성화 함수를 사용한다는 것입니다                  
논문을 읽지 않으면 모르겠지만 읽었을 때 알 수 있는 것은                   
논문이 쓰여질 당시의 GPU 는 여전히 느려서 두 개의 GPU 를 훈련하는 복잡한 방법을 가졌습니다                  
기본 개념은 이러한 층들이 두 개의 구별된 GPU 에 나눠져서 두 개의 GPU 가 서로 소통하는 방식입니다                  
논문에서는 AlexNet 의 기존 구조에서는 지역 응답 정규화라는 층이 있습니다                  
자주 사용되는 층이 아니라 여기서 다루지는 않았었죠                  
하지만 지역 응답 정규화의 개념은 이런 블록을 살펴보면 그 크기를 보았을 때 예를 들어                  
13 x 13 x 256 이라고 하면 지역 반응 정규화인 LRN 이 하는 일은            
높이와 너비가 지정된 한 지점의 256 개의 모든 채널을 보고 정규화 하는 것입니다            
그 이유는 13 x 13 이미지의 각 위치에서 높은 활성값을 가진 뉴런을 너무 많이 원치 않기 때문이죠            
하지만 이후의 연구에서 그리 유용하지 않다는 것이 밝혀졌습니다            
그래서 빨간 글씨로 적어서 중요하지 않다는 것을 표현한 것이고             
저는 지역 반응 정규화를 거의 사용하지 않습니다            
오늘 다룰 신경망에서는 말이죠 만약 딥러닝의 역사에 관심 있다면            
AlexNet 이전에도 음성 인식과 여러 분야에서 딥러닝은 집중을 받았었지만            
이 논문을 통해서 많은 컴퓨터 비전 커뮤니티가 딥러닝이 컴퓨터 비전 분야에 적용 가능하다는 것을 발견한 것이죠            
컴퓨터 비전 뿐만이 아니라 그 이상의 분야에서도 말입니다            
이러한 논문을 읽어보고 싶다면 이 과정에서는 읽을 필요가 없지만 이 논문은 쉬운 편이라 읽기 편할 것입니다            
AlexNet 은 상대적으로 복잡함 구조를 가지고 있습니다             
Alex 와 그의 동료들이 수많은 하이퍼 파라미터를 사용했기 때문이죠            

# VGG-16
![image](https://user-images.githubusercontent.com/50114210/71476606-2ce1ff00-2829-11ea-90e7-7016da4664a4.png)                     
세 번째이자 마지막으로 보여줄 예는 VGG - 16 네트워크입니다            
VGG - 16 의 주목할 만한 점은 많은 하이퍼 파라미터를 가지는 대신 합성곱에서            
스트라이드가 1 인 3 x 3 필터만을 사용해 동일합성곱을 하고            
최대 풀링층에서는 2 의 스트라이드의 2 x 2 를 사용합니다            
VGG - 16 은 아주 간결한 구조를 가집니다 그 구조를 한 번 살펴봅시다            
이미지에서 시작해서 첫 두 층에서는 합성곱을 해줍니다 그래서 첫 두 층에서는 64 개의 필터를 사용해주죠            
동일 합성곱이기 때문에 224 x 224 의 크기이고 64 개의 채널이 있습니다            
VGG - 16 은 상대적으로 깊은 편이라 그 크기를 전부 그리지는 않을 것입니다                 
이 그림이 의미하는 것이 무엇이냐면 224 x 224 x 3 의 이미지가 합성곱을 통해                 
224 x 224 x 64 가 되고 224 x 224 x 64 의 또 다른 한 개의 층이 존재합니다                 
그래서 이 [CONV 64] x 2 는 64 개의 필터를 가진 두 개의 합성곱층을 사용한다는 의미죠                 
앞서 말했듯이 모든 필터는 3 x 3 의 크기를 가지고 1 의 스트라이드와 동일 합성곱을 사용합니다                 
여기서는 그림 대신에 글자로 표현했죠                 
그 뒤로는 풀링층을 사용하는데 풀링층은 224 x 224 의 크기를 112 x 112 x 64 로 감소시킵니다                 
그리고는 몇 개의  합성곱층이 더 있는데 128 개의 필터를 가진 동일 합성곱이면 크기가 어떻게 변할까요                 
112 x 112 x 128 입니다 그리고 이어서 풀링층이 있고 새로운 크기는 이렇게 됩니다                 
그리고 256 개의 필터의 합성곱층 3 개를 사용하고 그 다음으로는 풀링층을 사용하고                 
그리고 몇 개의 합성곱층과 풀링층을 번갈아가며 사용하면 결국에는 7 x 7 x 512 의 완전 연결층이 됩니다                 
4096 개의 유닛과 1000 개의 소프트맥스 출력이 나오게 되죠                 
VGG - 16 의 16 이라는 숫자가 의미하는 것은 16 개의 가중치를 가진 층이 있다는 것입니다                 
1억 3천 8백만 개 정도의 변수를 가진 상당히 큰 네트워크 입니다                 
요즘 기준으로도 꽤나 큰 편이죠 하지만 VGG - 16 의 구조적인 장점은 꽤나 균일하다는 것입니다                 
몇 개의 합성곱층 뒤에 풀링층이 높이와 너비를 줄여줍니다 여기 몇 개가 있죠                  
그리고 합성곱층의 필터의 개수를 한 번 살펴보면 64 가 128, 256, 512 로 두 배씩 늘어납니다                 
아마 충분히 크기 때문에 더 늘리지 않은 것 같습니다                 
합성곱층에서 매 번 두 배씩 증가시키는 것은 이 네트워크를 이루는 간단한 규칙이 되죠                 
그래서 이 구조의 상대적인 획일성이 가지는 단점은 훈련시킬 변수의 개수가 많아                 
네트워크의 크기가 커진다는 것입니다                 
그리고 문헌을 읽다보면 보이는 VGG- 19 은 이것보다 더 큰 버전이죠                 
아래 인용한 Simonyan 과 Zisserman 의 논문에 상세히 나와있습니다                 
하지만 VGG - 16 도 VGG - 19 만큼의 성능을 보여주기에 더 많이 사용됩니다                 
제가 좋아하는 부분은 깊이가 깊어질 수록 보여지는 패턴이                 
풀링층에서는 높이와 너비가 매번 절반으로 줄어들고 합성곱층에서는 채널의 수가 매번 두 배 가량 늘어납니다                 
수치가 커지고 작아지는 것이 상당히 체계적으로 이루어집니다                 
그런 관점에서 매우 매력적인 논문이죠 이것이 3 개의 고전적인 구조입니다                 

# 아웃트로
원한다면 논문을 읽어볼 수도 있는데 AlexNet 으로 시작해서 VGG 논문으로 넘어가고                 
그 다음으로 조금은 어려운 LeNET 을 보면 되고 충분히 볼 만한 가치가 있습니다                 
