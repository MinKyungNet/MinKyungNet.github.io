---
layout: post
title: "합성곱 네트워크의 한 계층 구성하기"
tags: [convolution, bias, activation function]
categories: [Convolutional Neural Networks]
---

# 학습 목표
- 합성곱 신경망의 한 계층 구성하는 법을 배운다.

# 핵심 키워드
- 합성곱
- 편향
- 활성화 함수

# 학습 내용
- 합성곱 신경망의 한 계층은 아래와 같이 구성됩니다.
  - 합성곱 연산 > 편향 추가 > 활성화 함수
  - 활성화 함수는 비선형성을 적용하기 위함입니다. 보통 ReRU를 많이 사용합니다.
  
- 표기법을 다시 정리 해보겠습니다.
  - l : l번째 계층
  - f[l] : 필터의 크기
  - p[l] : 패딩의 양
  - s[l] : 스트라이드 크기
  - nH : 이미지의 높이
  - nW : 이미지의 넓이
  - nc : 채널의 수

- 다음 연산이 l번째 층의 연산이면, 이전 층 (l - 1)의 이미지의 크기는 nH[l-1] * nW[l-1] * nc[l-1]가 됩니다. 그 결과로 낭는 이미지의 크기는 nH[l] * nW[l] * nc[l]입니다.
- l번째 층의 높이 혹은 넓이의 크기연산 공식은 아래와 같습니다.
- nH[l] = (nH[l-1] + 2p[l] - f[l]) / s[l] + 1
- nW[l] = (nW[l-1) + 2p[l] - f[l]) / s[l] + 1
- nc[l]개의 크기가 f[l] * f[l] * nc[l-1]인 필터가 합성곱 연산을 진행하게 됩니다. 그리고 활성화 함수를 거쳐 l번째 층의 결과값이 계산됩니다. 합성곱 연산에 사용된 변수는 총 f[l] * f[l] * nc[l-1] * nc[l]개 입니다. 추가된 평향까지 더하면 한 층의 합성곱 신경망에 필요한 변수는 f[l] * fpl[ * nc[l] + nc[l]개가 됩니다.
- 기존의 단순 신경망을 사용한다면 가중치 행렬 W[l]의 크기 (nH[l-1] * nW[l-1] * nc[l-1]) * (nH[l] * nW[l] * nc[l])인데, 이보다 더 적은 변수로 계산이 가능해졌습니다.
  - 예를 들어 28 * 28 * 3 이미지를 동일한 5 * 5 필터 20개를 사용해서 계산(패딩없고 스트라이드는 1)한다면, 24 * 24 * 20 크기의 결과가 나옵니다.
  - 합성곱 연산에 필요한 총 변수의 크기는 5 * 5 * 3 * 20 + 20 = 1520이지만, 단순 신경망을 사용하여 같은 크기의 결과를 나타내려면 (28 * 28 * 3) * (24 * 24 * 20) + (24 * 24 * 20) = 38,106,560만큼의 변수가 필요합니다.

# 합성곱 신경망의 한 계층 구성
이제 합성곱 신경망의 한 계층을 구성할 준비가 되었습니다 예시를 한 번 살펴봅시다       
이전 글에서 3D 입체형을 두 개의 필터와 합성곱을 하는 법을 살펴보았습니다        
이 경우에는 두 개의 4 x 4 의 결과가 나왔죠                        
먼저 첫 번째 필터와 합성곱하여 하나의 4 x 4 의 결과가 나왔고                 
두 번째 필터와 합성곱하여 또다른 4 x 4 의 결과가 나왔습니다                 
이것을 합성곱 신경망 층으로 만들기 위해 마지막으로 할 일은
각각에 편향을 더해주는 것인데 이 수는 실수여야 하고                 
파이썬 브로드캐스팅을 통해서 16 개의 요소에 동일한 수를 더해주고                 
비선형성을 적용해주는데 예시로 ReLU 비선형성을 적용하면 4 x 4 의 결과가 나옵니다                 
편향과 비선형성을 적용하게 되면 말이죠                 
그리고 아래 것도 또다른 실수의 편향을 추가해주고                 
16 개 의 숫자에 모두 더해준 뒤 ReLU 비선형성을 적용해주면                 
또 다른 4 x 4 의 결과를 얻게 됩니다                 
이전에 했던 것처럼 이것을 가지고 이렇게 쌓게 되면 4 x 4 x 2 의 결과가 됩니다                 
이 6 x 6 x 3 에서 4 x 4 x 2 로 변환된 계산이 바로 합성곱 신경망의 한 계층이 됩니다           

# 연습 1
![image](https://user-images.githubusercontent.com/50114210/71444014-d05fe080-2751-11ea-84b0-e43ad67239a4.png)       
이것을 합성곱이 아닌 표준 신경망의 한 계층으로 연결시키기 위해서는                 
전파의 바로 전 단계가 이랬었습니다                 
z[1] = w[1]a[0] 에 a[0] 는 x 와 같고 z[1] = w[1]a[0] + b[1]에                 
비선형성을 적용해줌으로써 a[1] 곧 g(z[1]) 을 얻습니다                 
이 예시에서는 이것이 a[0] 실제로는 x 에 해당하고                  
이 필터는 w[1] 과 유사한 역할을 가지고 있습니다                 
그래서 합성곱 연산을 할 때에 27 개의 숫자 정확히는 두 개의             
필터이기 때문에 27 × 2 개를 가지고 각각 곱해주는 것이죠             
그래서 마치 일차 함수를 계산하듯이 이 4 x 4 행렬을 얻고             
그래서 이 합성곱 연산이 w[1] 에 a[0] 를 곱하는 것과 유사합니다             
여기 두 개의 4 x 4 행렬 모두 말이죠             
그리고 편향을 더해주고 실제 값을 추가하기 전 이부분은             
z[1]과 유사하고 비선형성을 적용해주면 이것과 같이 되는 것이죠             
다음 계층에서 입력 값이 될 것입니다             
이런 식으로 a[0] 에서 a[1] 으로 선형 연산을 해주고             
합성곱에서는 이것들을 곱해주는 것으로 선형 연산을 합니다             
그리고 편향을 더해주고 ReLU 연산을 해주게 되면             
6 x 6 x 3 의 a[0] 에서 신경망의 다음 층인 4 x 4 x 2 의 a[1] 으로 변하는 것이죠             
6 x 6 x 3 가 4 x 4 x 2 로 바뀌었습니다             
이것이 합성곱 신경망의 한 계층을 형성하는 것이죠             
이 예시에는 두 개의 필터가 있기 때문에 두 개의 속성을 지니고 있습니다             
그래서 4 x 4 x 2 의 결과가 나온 것이죠             
하지만 만약 2 개가 아닌 10 개의 필터가 있다면 4 x 4 x 10 크기의 결과 행렬이 나올 것입니다             
이제는 두 개가 아닌 10 개의 행렬을 쌓아서 4 x 4 x 10 이 되고 그것이 a[1] 이 될 것입니다             

# 연습 2
좀 더 잘 이해하기 위해 연습을 한 번 해봅시다             
만약 10 개의 필터가 있고 각각 3 x 3 x 3 크기로             
신경망의 한 계층에 있다면 이 층은 몇 개의 매개변수를 가질까요?             
각각의 필터는 3 x 3 x 3 의 크기로 27 개의 변수를 가집니다             
또 편향을 더해줘야 합니다 이것을 B 라는 변수라고 하면 총 28 개의 변수를 가지게 되죠             
이전 슬라이드에서는 두 개의 필터만 그렸었는데 이번에는 10 개의 필터가 존재합니다             
1, 2, ..., 10 개가 있기 때문에 28 에 10 을 곱하면 280 개의 변수를 가지게 됩니다             
한 가지 좋은 점은 입력 이미지의 크기가 어떻게 되던지              
1000 × 1000 이나 5000 × 5000 이라도 변수의 수는 280 개로 여전히 고정되어 있습니다             
그래서 이 10 개의 필터로 여러 가지 다른 속성들을 검출할 수 있습니다             
아주 큰 이미지라도 적은 수의 변수로 가능하죠             
이것이 과대적합을 방지하는 합성곱 신경망의 한 성질입니다             
이 10 개의 속성 검출기가 잘 작동한다면 훨씬 더 큰 이미지에도 적용이 가능하고             
변수의 수는 여전히 280 으로 고정되어 있습니다             
그래서 합성곱 계층을 설명하는 표현을 한 번 요약해보자면            

![image](https://user-images.githubusercontent.com/50114210/71444023-e8376480-2751-11ea-92d2-4c88551fe350.png)

# 필터
l 은 합성곱 계층이고 f^[l] 로 필터의 크기를 나타낼 것입니다            
이전에는 필터 크기를 f × f 로 나타냈지만            
이제는 [l] 이라는 표현으로 필터의 크기가 f × f 라는 것을 나타내게 될 것입니다            
그리고 일반적으로 [l]  은 특정 계층 l 을 나타내는 표현입니다            

# 패딩
그리고 p^[l] 로 패딩의 양을 나타내고 역시나 패딩의 양은             
유효 합성곱으로 패딩이 없을 수도 있고 동일 합성곱으로            
출력의 크기가 입력의 크기가 동일할 수도 있습니다            

# 스트라이드
그리고 s^[l] 로 스트라이드를 나타낼 것입니다 이 계층의 입력은 특정한            
크기를 가지게 되는데 n × n × 채널의 수 의 크기를 가지게 되겠죠             
이 표현을 [l－1] 으로 바꿔줄 것입니다          
이전 계층의 활성값이기 때문이죠 [l－1] 이 있고 다음은 n_C^[l－1] 입니다          

# 높이와 너비
지금까지의 예시에서는 높이와 너비가 같은 이미지를          
사용했지만 다를 경우를 대비해 H 와 W  첨자로 이전 계층의 입력의 높이와 너비를 나타낼 것입니다          
그래서 계층 l 에서는 n_H × n_W × n_C 에 윗첨자로는 이것이 계층 l 에 있는 것이 아니라           
이전 계층에서부터 온 것이기 때문에 [l－1] 을 사용합니다          
그리고 이 신경망의 계층이 그 자체로 출력이 될 것입니다          

# 출력의 크기
그래서 출력은 n_H^[l] × n_W^[l] × n_C^[l] 이죠 이것이 출력의 크기가 됩니다            
이전에 살펴보았듯이 출력의 크기           
또는 적어도 높이와 너비는 다음 공식으로 표현했었죠           
n + 2p - f / s + 1 의 바닥 값이었는데           
새로운 표현에서는 계층 l 의 출력의 크기가           
이전 계층의 크기가 되고 현재 계층 l 의 패딩에다           
이 계층 l 에 해당하는 필터의 크기를 사용하고           
계속해주면 정확히 이것은 높이에 해당하죠           
그래서 출력 크기의 높이가 주어졌고 동일한 공식으로 너비를 계산할 수 있습니다           
H 를 지우고 W 를 적으면 되죠           

# 높이와 너비 계산
그래서 높이던 너비던 이 공식에 넣게 되면 출력의 높이와 너비를 계산할 수 있습니다           
이것이 n_H^[l - 1] 과 n_W^[l - 1] 과의 관계죠           

# 필터의 수
그렇다면 채널의 수는 어디서 오는 것일까요?           
만약 출력의 크기가 이러한 깊이를 가진다면 이전에 배워서 알듯이 이것은 필터의 수입니다           
이전에는 두 개의 필터가 있어서 출력이 4 x 4 x 2 인 2차원이었죠           
만약 열 개의 필터가 있다면 출력의 크기는 4 x 4 x 10 이 됩니다           
그리고 이 채널의 수는 그저 신경망 계층에서 사용하는 필터의 개수입니다          

# 필터의 크기
다음은 각 필터의 크기입니다 각각의 필터는 f^[l] x f^[l] x  또 다른 숫자인데          
어떤 숫자일까요 6 x 6 x 3 이미지는 3 x 3 x 3 필터로 합성곱을 해야 합니다          
필터의 채널의 수와 입력의 채널의 수가 일치해야 하죠           
그래서 이 숫자가 이 숫자와 일치해야 합니다          
그래서 각각의 필터가 f^[l] x f^[l] x n_C^[l - 1] 의 크기이죠          
편향과 비선형성을 적용한 뒤의 출력이 이 계층의 활성값인          
a^[l] 이 되고 이것은 이미 살펴봤듯이 n_H^[l] × n_W^[l] × n_C^[l] 의 크기입니다          
만약 벡터화된 방식을 사용하거나 배치 기울기 하강 또는          
미니 배치 기울기 하강을 사용할 때 실제로는 A^[l] 을 구하는 것인데          
m 개의 예시가 있으면 m 개의 활성값을 가져서 m x n_H^[l] × n_W^[l] × n_C^[l] 입니다           
배치 기울기 하강을 사용할 때 말이죠             

# 가중치
다음은 가중치 또는 w 라는 변수인데 필터의 크기에 대해서는 이미 살펴봤는데             
f^[l] x f^[l] x n_C^[l - 1] 입니다 하지만 한 필터의 크기죠             
이것이 필터의 개수이고 가중치의 개수는 필터를 전부 모은 것이기 때문에             
그 크기는 이것에 필터의 개수를 곱해주면 됩니다             
마지막 값이 계층 l 의 필터의 개수이기 때문이죠             

# 
그리고 마지막으로 편향에 대한 변수인데 필터마다 하나의 실수값인 편향을 가지기 때문에 이만큼의 개수가 있습니다             
이 크기의 백터에 해당하죠 그러나 이후의 코드에서는             
(1, 1, 1, n_C^[l]) 의 4 차원 행렬 또는 텐서로 나타냅니다 많은 표현법들이 있었습니다             
많은 경우에 일반적으로 사용되는 표현들이죠             
온라인 상의 오픈 소스 코드를 살펴보게 되면 높이와 너비와 채널의 배치 순서에 대해 정해진 기준이 없습니다             
GitHub 의 소스코드나 오픈소스의 코드를 보면 어떤 저자들은 이 순서 대신 채널을 먼저 놓기도 합니다             
그래서 이런 순서의 변수를 보게 됩니다 채널의 수는 앞에 놓거나 뒤에 놓는 식으로 크기를 나타냅니다             
두 방식 모두 납득이 갑니다 법은 일관성 있지만 이것은 그렇지는 않죠             
법은 일관성 있지만 이것은 그렇지는 않죠             
그리고 딥러닝의 분야에서는 합의가 된 표현이 아닐 수 있습니다             
그래도 이 글에서는 이 기준을 따를 것입니다             
높이, 너비 그리고 채널의 순서로 나열하게 되는 것이죠             

# 요약
이 글의 요점은 합성곱 신경망의 한 계층이         
어떻게 구성되어 있는지와 한 계층의 활성값에서 다른 계층으로 전달될 때 필요한 계산의 과정입니다         
이제 합성곱 신경망의 한 층이 어떻게 구성되어 있는지 알게 되었고         
이것들을 쌓아서 더 깊은 합성곱 신경망을 형성해볼 것입니다         
