---
layout: post
title: "컴퓨터 비전 시스템을 구축하기 위한 팁"
tags: [Spectrum, labeled data, benchmarking, ensemble]
categories: [Convolutional Neural Networks]
---

# 학습 목표
- 직접 컴퓨터 비전 시스템을 구축하는데 도움이 되는 지식을 배운다.

# 핵심 키워드
- 스펙트럼
- 라벨링된 데이터
- 벤치마킹
- 앙상블

# 학습 내용
- 대부분 머신러닝 문제는 데이터의 양에 따른 스펙트럼안에 존재한다고 볼 수 있습니다.
- 데이터가 많을 수록 간단한 알고리즘을 사용하고 수작업 비중을 적게 가져갑니다. 하지만 반대로 데이터가 적을 수록 복잡한 알고리즘과, 많은 수작업 설계를 요구합니다. 후자를 보통 "hacks"이라고 합니다.
- 머신이 지식을 학습하는 소스는 두가지 입니다.
  - 수많은 라벨링 데이터들
  - 만약에 데이터가 부족하다면, 직접 특성과 네트워크 구조를 설계하고, 시스템 요소를 만드는 것입니다.
- 많은 사람들이 표준화된 벤치마킹 데이터 세트 경연에서 승리하는 것에 관심이 있습니다. 벤치마킹의 장점은 사회가 가장 효율적인 알고리즘을 찾을 수 있도록 도와줍니다.
- 벤치마킹을 잘 되게 하는 몇가지의 팁이 있습니다. 다만 비용이 많이 들기때문에 제품으로서는 잘 활용하지 않습니다.
  - 앙상블 방법 : 여러개의 신경망을 사용하여 독립적으로 훈련시킨 후, 평균을 내는 것입니다.
  - 다중 크로핑 : 이미지를 여러개로 크로핑하여 테스트 후 평균을 내서 결과를 얻습니다.
- 오픈 소스를 활용하는 것도 중요합니다. 온라인에서 공개된 네트워크 구조, 하이퍼 파라미터, 또는 미리 훈련된 모델을 사용하는 것이 빠르게 진행할 수 있습니다.

# 인트로
딥러닝은 컴퓨터 비전이나 음성 인식 광고 분야 등 많은 문제들에 적용되어 왔습니다      
그 중 컴퓨터 비전 분야에 딥러닝을 적용하는 측면의 특별한 점이 있죠      
이 영상에서 다룰 내용은 컴퓨터 비전 분야의 딥러닝에 대해 관찰한 부분인데      
여러 개념들과 문헌들을 이해하고 또 직접 컴퓨터 비전 시스템을 구축하는데 도움이 되기를 바랍니다      

# 머신러닝 문제의 스펙트럼
![image](https://user-images.githubusercontent.com/50114210/71526292-86c1f200-2919-11ea-8d13-4c950b792971.png)                  
대부분의 머신 러닝 문제를 어떤 스팩트럼 안에 있다고 볼 수 있는데             
상대적으로 적은 데이터에서 많은 데이터까지 있을 때             
예를 들면 음성 인식 분야는 괜찮은 양의 데이터가 있다고 생각됩니다             
적어도 문제의 복잡함에 비교했을 때 말이죠             
이미지 인식 분야는 오늘날 상당히 많은 데이터가 있지만             
모든 픽셀들을 살펴보고 알아내야 하는 복잡한 문제이기 때문에             
온라인 상의 데이터가 상당히 많아 보이지만 여전히 더 많은 데이터를 필요로 하게 됩니다             
그리고 물체 인식 같은 경우는 데이터의 양이 더 적습니다             
이미지 인식이 특정 사진을 보고 고양이인지 아닌지 구별하는 것이라면             
물체 인식은 사진을 보고 실제 물체가 어디있는지 알고 경계선을 그려야 하는 문제죠             
이런 경계선을 그리는 비용이 훨씬 더 크기 때문에 물체 인식 분야는 데이터가 적습니다             
이미지 인식과 비교했을 때 말이죠             

# 데이터의 양에 따른 특징 추출
![image](https://user-images.githubusercontent.com/50114210/71526305-96413b00-2919-11ea-8d91-a9570dde6a7b.png)                   
이 머신러닝 문제의 스팩트럼을 살펴보면 평균적으로 데이터가 많다면             
간단한 알고리즘을 사용하고 수작업이 적은 경향이 있습니다             
특성들을 너무 조심스럽게 디자인할 필요가 없고             
대신 네트워크를 더 간단한 구조로 디자인할 수 있고 신경망이 학습하고 싶은 것을 학습하면 됩니다             
반면에 데이터가 충분히 많지 않다면 평균적으로 수작업 설계가 많아지게 되고              
안 좋게 말하면 핵이 많아집니다             
데이터가 충분치 않다면 직접 설계하는 것이 좋은 성능을 내기 위한 최고의 방법이죠           
머신 러닝 응용 프로그램을 보면 학습 알고리즘은 지식에 있어서           
두 개의 소스를 가지고 있는데 그 중 하나는 지도 학습에서 x, y 쌍 같은 레이블 데이터입니다           
두 번째는 직접 설계한 특성입니다 그 방법에는 여러 가지가 있는데            
직접 특성을 설계하는 것과 네트워크 구조를 설계하는 것과 시스템의 다른 요소를 설계하는 것이 있죠           
레이블 데이터가 충분하지 않다면 직접 설계에 더 의존해야 합니다           
컴퓨터 비전은 아주 복잡한 함수를 학습하게 되어서 데이터가 충분하지 않다고 느껴질 때가 많습니다           
데이터 세트가 점점 커져도 원하는 만큼 데이터가 없기 때문이죠           
그래서 오늘날의 컴퓨터 비전 분야도 직접 설계에 더 의존하는 것입니다           
컴퓨터 비전 분야에서 더 복잡한 네트워크 구조가 개발된 이유가 데이터의 부재 때문입니다           
더 좋은 성능을 내려면 구조 설계에 공을 들여야 하죠           
직접 설계를 비판적으로 보는 것이 아니라 데이터가 부족하게 되면           
직접 설계는 굉장히 까다로운 작업이고 많은 영감을 필요로 합니다           
뛰어난 영감을 가진 사람이 더 좋은 성능을 보여주고           
데이터가 부족할 때 프로젝트에 아주 큰 기여를 할 수 있겠죠           
만약 충분히 많은 데이터가 있다면 설계하는데 시간을 쏟는 대신           
학습 시스템을 구축하는데 시간을 들일 것입니다 하지만 역사적으로 컴퓨터 비전 분야는            
작은 데이터 세트를 사용했기에 직접 설계에 많이 의존해야 했습니다           
최근 몇 년간 컴퓨터 비전 작업의 데이터 양이 급격하게 늘었고           
직접 설계를 하는 정도를 상당히 많이 줄여줬지만           
여전히 컴퓨터 비전 분야에서는 많은 네트워크 구조 설계를 필요로 합니다           
이것이 컴퓨터 비전에서 복잡한 하이퍼 파라미터를 선택하는 이유죠           
다른 분야에서보다 더 복잡하게 말입니다           
물체 인식은 이미지 인식보다 작은 데이터 세트를 가지고 있어서           
훨씬 더 복잡하고 특화된 요소가 들어간 알고리즘이죠           
데이터가 적을 때 큰 도움이 되는 것이 바로 전이 학습입니다           
이전 슬라이드에서 본 Tigger, Misty 그리고 둘 다 아닌 예시를 보면           
데이터가 너무 적어서 전이 학습이 많은 도움을 줄 수 있는 방법 중 하나입니다           
상대적으로 데이터가 적을 때 말이죠           

# 벤치마킹에서 유용한 기술들
컴퓨터 비전 분야를 살펴보면 상당수의 사람들이      
표준화된 벤치마킹 데이터 세트와 경연에서 승리하는 것에 관심 있습니다      
컴퓨터 비전 연구자들은 벤치마킹이 성공적이면 논문을 쓰기 좋죠      
그래서 벤치마킹을 잘 하는데 많이들 집중합니다      
이런 현상의 장점은 커뮤니티가 가장 효율적인 알고리즘을 찾을 수 있도록 도와줍니다      
하지만 어떤 논문에서는 벤치마킹에서는 잘 되지만       
실제로는 잘 사용되지 않는 것도 종종 있습니다      
여기 벤치마킹을 잘 되게 하는 몇 가지의 팁이 있습니다      
만약 소비자에게 제공하기 위한 제품을 만든다면 사용하지 않겠죠      

# 앙상블
![image](https://user-images.githubusercontent.com/50114210/71526319-a35e2a00-2919-11ea-8011-17a2bf0f4a71.png)               
첫 번째로는 앙상블인데 어떤 신경망을 사용할지 결정한 후          
몇 개의 신경망을 독립적으로 훈련시킨 뒤 평균을 내는 것입니다          
그래서 3 개나 5 개 또는 7 개의 신경망을 무작위로 초기화하고           
그 신경망을 훈련시킨 뒤 출력들의 평균을 구하는 것이죠          
그리고 가중치가 아닌 Y 의 예측값의 평균을 내야 합니다          
7 개의 신경망에 7 개의 예측값이 있으면 그 값들의 평균이 되겠죠          
이것으로 1 퍼센트던 2 퍼센트던 벤치마킹에서 더 나은 결과를 보일 수 있습니다          
때로는 정말 1, 2 퍼센트로 시합에서 승리할 수도 있겠죠          
앙상블은 이미지를 테스트하기 위해서는 이미지를          
3 개에서 15 개의 네트워크에 실행하는 것이 일반적입니다          
이것으로 3 에서 15 만큼 실행시간을 지연시키죠          
그래서 앙상블은 벤치마킹이나 시합을 이기기 위한 좋은 팁이지만          
소비자에게 제공될 제품에는 거의 사용되지 않습니다          
예산이 넘치도록 많아서 고객 이미지가 상관 없는 것이 아니라면 말이죠          

# 다중 크로핑
![image](https://user-images.githubusercontent.com/50114210/71526328-ae18bf00-2919-11ea-93be-3f9876235350.png)             
또 벤치마킹에 도움이 되는 것이 바로 다중 크로핑입니다        
데이터 확대를 어떻게 하는지 이미 살펴보았고        
다중 크로핑 역시 테스트 이미지에 적용되는 데이터 확대의 한 형태입니다        
여기 두 개의 대칭 버전을 포함해 네 개의 고양이 사진이 있습니다        
10-크로핑이라는 기술이 있는데 이 중간 영역을 잘라서 분류기를 실행해주고         
또 왼쪽 위 영역과 오른쪽 위 영역, 또 왼쪽 아래 영역, 오른쪽 아래 영역을         
분류기에 넣고 실행해주고 대칭 이미지도 동일하게 해줍니다        
가운데 영역을 잘라내고 다음에는 네 개의 모서리 영역을 잘라내죠        
그래서 중간 조각 하나와 네 모서리 조각이 여기와 여기 하나씩 있습니다        
이것을 전부 더하면 10 개의 조각이 됩니다        
그리고 이 10 개의 이미지를 분류기에 실행하고 평균을 내줍니다        
만약 계산에 있어서 여유가 있다면 굳이 10 개까지 필요는 없습니다        
더 적은 조각을 사용함으로써 더 나은 성능을 보여줄 수 있죠        
실제 사용자에게 제공될 제품의 관점에서 말입니다        
하지만 이 방법도 실제 제품보다는 벤치마킹에 더 많이 사용됩니다        
ensembling 의 큰 문제점은 여러 개의 네트워크를 계속 유지해야 하기 때문에        
많은 양의 메모리를 필요로 합니다 다중 크로핑은 하나의 네트워크만 유지해도 돼서        
메모리를 많이 차지하지는 않지만 여전히 실행 시간을 꽤나 늦추게 되죠        

# 오픈 소스를 사용하자             
![image](https://user-images.githubusercontent.com/50114210/71526345-c12b8f00-2919-11ea-90ca-373a544c6d77.png)                    
여기까지가 팁이었고 논문들도 이 팁들을 언급하기는 하지만             
개인적으로는 이 방법들을 실제 제품에는 절대 사용하지 않습니다              
벤치마킹이나 시합에는 유리하더라도 말이죠             
컴퓨터 비전 문제는 작은 데이터에 의존하기 때문에 신경망 구조 설계에 많은 공을 들여야 했습니다             
그리고 한 문제에서 잘 작동하는 신경망이 놀랍게도 정말 가끔 다른 문제에서도 잘 작동할 때가 있죠             
그래서 실용적인 시스템을 구축하기 위해서는 다른 누군가의 신경망 구조로 시작하는 것이 좋습니다             
오픈 소스 구현을 사용하면 좋은 이유는 이미 누군가가 귀찮고 세부적인 것들을 찾아놓은 것이기 때문이죠             
학습률 감쇠나 하이퍼 파라미터 같은 것들 말입니다             
누군가가 몇 주간 다수의 GPU 를 이용해서 수 백만 개의 이미지를 훈련해놓았을 수 있습니다             
그래서 이미 훈련된 모델과 데이터 세트를 사용함으로 훨씬 더 빠르게 진행할 수 있겠죠             
물론 많은 자료와 그럴 의향이 있다면 처음부터 네트워크를 훈련하는걸 결코 막지 않겠습니다             
자신만의 알고리즘을 발명하려면 그렇게 해야 하죠             
