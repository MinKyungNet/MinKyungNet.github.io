---
layout: post
title: "왜 ResNets이 잘 작동할까요?"
tags: [Residual Networks, skip connection, short cut]
categories: [Convolutional Neural Networks]
---

# 학습 목표
- ResNet이 효과적인 이유를 알아본다.

# 핵심 키워드
- Ressidual Networks
- 스킵 연결, 지름길

# 학습 내용
- 신경망의 깊이가 깊어 질 수록 훈련세트를 다루는데 지장이 있을 수 있습니다. ResNet은 이를 잘 해결했습니다.
![image](https://user-images.githubusercontent.com/50114210/71480094-cbc32700-283a-11ea-8701-b01d80390a2f.png)        
- 위와 같은 큰 신경망에서 두개의 층을 더 추가하고 지름길을 연결해줍니다. 여기서 활성화함수 g는 ReLU입니다.
- 스킵 연결을 더해준 출력값 a[l+2]은 g(z[l+2] + a[l]) = g(W[l+2]*a[l+1] + b[l+2] + a[l])로 쓸 수 있습니다. 여기서 만약에 W[l+2]와 b[l+2]의 값이 0이 된다면, 위의 식은 a[l+2] = g(a[l]) = a[l]으로 항등식이 됩니다.
- 위 항등식의 의미는 신경망으로 하여금 스킵 연결을 통해 두 층이 없는 더 간단한 항등식을 학습하여, 두 층 없이도 더 좋은 성능을 낼 수 있게 만든다는 것입니다.
- 다만, z[l+2]와 a[l]이 같은 차원을 가져야합니다. 따라서 보통 동일합성곱 연산(출력 크기가 입력크기와 같게하는 합성곱연산)을 하거나 차원을 같게 만들어주는 행렬 Ws를 잔여블록 앞에 곱해줘서 같게 만듭니다.

# 인트로
왜 ResNet 이 효과적인지 예시를 한 번 살펴볼텐데
적어도 그 깊이가 깊어져도 훈련 세트를 다루는데 지장이 없다는 점이죠
그리고 저번 강의에서 말했듯이 훈련 세트를 잘 다루는 것은 
테스트나 다른 어떤 것에서 효과적이기 위한 전제 조건이고
ResNet 을 훈련시켜서 훈련 세트를 잘 다루는 것은 좋은 출발입니다 예시를 한 번 살펴봅시다.

![image](https://user-images.githubusercontent.com/50114210/71480452-6b34e980-283c-11ea-8d6f-62ba9ac46a58.png)                   

# 지름길
이전의 영상에서는 네트워크가 깊어질수록 훈련 세트를 다루는데에 지장을 줄 수 있다고 배웠습니다         
그래서 너무 깊은 네트워크를 선호하지 않는 것이죠         
하지만 적어도 ResNet 의 경우에는 이것이 완전히 맞지는 않습니다 예시를 한 번 살펴봅시다         
X 를 하나의 거대한 신경망에 투입해서 a^[l] 이라는 활성값을 출력한다고 해봅시다         
a^[l] 이라는 활성값을 출력한다고 해봅시다         
그리고 이 예시에서는 그 신경망을 좀 더 깊게 만들어볼텐데         
동일한 신경망이 있고 a^[l] 을 출력한 다음 이 네트워크에 몇 개의 층을 추가해줄 것입니다         
하나의 층을 더하고 또 다른 층을 더해주면 a^[l + 2] 가 나오게 되죠         
그리고 이러한 지름길을 추가해 잔차 블록으로 만들어 줍니다         

# 깊은 모델에서도 성능 저하가 일어나지 않는 이유
변수의 측면에서 보았을 때 신경망 전반에 걸쳐서 ReLU 활성화 함수를 사용하기 때문에           
입력값인 X 를 제외하고는 모든 활성값이 0 보다 크거나 같죠           
ReLU 활성화 함수는 0 또는 양수를 출력하기 때문입니다           
a^[l + 2] 가 어떤 것일지 한 번 살펴보면           
이전 영상에서 a^[l + 2] 는 (z^[l + 2]+ a^[l])  에 ReLU 를 적용한 것이었습니다           
여기 더해진 a^[l] 이 뜻하는것은 좀 전에 추가한 스킵 연결이고 이 식을 전개하게 되면           
g(w^[l + 2] a^[l]) + b^[l + 2] + a^[l]) 에서            
"w^[l + 2] a^[l]) + b^[l + 2]" 이 부분이 이것과 같고           
거기에 a^[l] 을 더해주는데 한 가지 짚을 점은           
만약 L2 규제나 가중치 붕괴를 사용하면 w^[l + 2] 의 값이 감소합니다           
만약 가중치 붕괴를 b 에 적용해도 마찬가지로 감소하죠           
실제로는 그럴 때도 있고 아닐 때도 있지만 여기서는 w 에 집중을 해서 살펴봐야 합니다           
w^[l + 2] 가 0 이고 b 도 마찬가지로 0 이라면 0 이기 때문에 이 부분은            
사라지고 g(a^[l]) 은 0 이기 때문에 이 부분은 사라지고 g(a^[l]) 은 a^[l] 과 같습니다           
여기서는 ReLU 활성화 함수를 사용해서 모든 활성값이 양수기 때문에           
g(a^[l]) 은 양수에 ReLU 를 적용한 것이라 a^[l] 을 돌려받는 것이죠           
그래서 항등 함수는 잔여 블록의 훈련을 용이하게 만들어 줍니다           
그리고 a^[l + 2] 가 a^[l] 과 같게 되는 이유는 바로 이 스킵 연결때문입니다           
이것이 의미하는 바는 신경망에 이 두 층을 추가해도 이 두 층이 없는 더 간단한           
네트워크만큼의 성능을 가지는 이유가 항등 함수를 학습하여            
a^[l + 2] 에 a^[l] 을 대입하면 되기 때문인 것입니다           
이 두 층을 추가했더라도 말이죠 이것이 바로 잔차 블록을           
거대한 신경망 어딘가에 추가해도 성능에 지장이 없는 이유입니다           

# 스킵 연결을 했을 때 성능이 좋아지는 이유
하지만 우리의 목표는 성능에 지장을 주지 않는 정도가 아니라 성능을 향상시키는 것이기 때문에           
만약 이 은닉 유닛들이 학습을 할 수 있다면 항등 함수를 학습하는 것보다 더 나은 성능을 보여줄 수 있죠              
이러한 스킵 연결 없이 만들어진 아주 깊은 네트워크의 문제는               
네트워크를 깊게 만드려고 하면 할수록 변수를 선택하기 어려워집니다              
항등 함수를 학습하는 것이라도 말이죠              
그래서 많은 수의 층이 오히려 성능을 저하시킬 수 있는 것입니다              
ResNet 이 잘 작동하는 주된 이유는 추가된 층이 항등 함수를 학습하기 용이하기 때문입니다              
그래서 성능의 저하가 없다는 것을 보장할 수 있고              
또는 운이 좋다면 성능을 향상시킬 수도 있는 것이죠              
기본적으로 성능에 손해가 없고 경사 하강법으로 더 나아질 뿐입니다              

# 스킵 연결 차원 맞춰주기
ResNet 에 대해서 한 가지 더 다룰 점은 여기의 덧셈은              
z^[l + 2] 와 a^[l] 이 같은 차원을 가진다고 가정하는 것입니다              
그래서 ResNet 에서 많이 볼 수 있는 것은 동일 합성곱인데 이곳의 차원이 출력 층의 차원과 같아지게 합니다              
그래서 이 단락 연결이 가능한 것이죠 동일 합성곱이 차원을 유지시켜줘서              
이러한 단락을 가질 수 있게 되고 또 두 개의 동일한 차원의 벡터의 합을 구할 수 있게 됩니다              
입력과 출력의 차원이 다른 경우는 예를 들어 이것이 128 차원이고              
z 또는 a^[l + 2] 가 256 차원이라면 하나의 W_s 라는 하나의 행렬을 추가해주는데              
여기서 W_s 는 256 x 128 의 행렬이라 W_s 와 a^[l] 의 곱은 256 차원이 됩니다              
그래서 이 덧셈은 두 개의 256 차원 벡터의 합이죠              
W_s 에는 여러 선택지가 있는데 학습된 변수를 가진 행렬일 수도 있고              
제로 패딩으로 고정값을 가진 행렬이라서 a^[l] 을 가지고 0 을 채워줘서 256 차원이 되는 것이죠              
둘 중 어느 것도 가능합니다              

# 이미지로 ResNet 살펴보기
![image](https://user-images.githubusercontent.com/50114210/71480458-7720ab80-283c-11ea-98e2-48bf9dc35556.png)                       
마지막으로 이미지에서 ResNet 을 살펴봅시다            
이 이미지는 왼쪽 아래의 논문으로부터 가져온 것인데 평범한 네트워크의 한 예시인데            
이미지를 입력시키면 여러 개의 합성곱 층이            
소프트맥스 출력이 있을 때까지 있는데 이것을 ResNet 으로 바꾸려면            
이런 스킵 연결들을 추가해줘야 합니다            
여기 아주 많은 3 x 3 합성곱이 있는데 그 중 대부분은 동일 합성곱입니다            
그래서 같은 차원의 벡터를 더해주는 것이죠 완전 연결 층이 아닌 합성곱 층이라 할 수 있는데            
동일 합성곱이라 차원이 유지됩니다 그래서 z^[l + 2] + a^[l] 이 성립이 되는 것이죠            
전에 봤던 네트워크와 같이 다수의 합성곱 층을 가지고 있고            
때때로 풀링 층이나 유사한 것을 가지고 있는데 그런 것이 나올 때마다 차원을 조정해줘야 합니다            
이전 슬라이드에서 봤던 W_s 말이죠            
합성곱, 합성곱, 합성곱, 풀링 층이 매우 흔한 형태이고            
끝에서는 완전 연결 층과 소프트맥스를 이용한 예측이 나옵니다            
