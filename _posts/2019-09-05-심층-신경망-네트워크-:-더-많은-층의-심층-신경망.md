---
layout: post
title: "심층 신경망 : 더 많은 층의 심층 신경망"
tags: [Shallow Model, Deep Neural Network]
cateogires: [Neural Networks and Deep Learning]
---

# 학습 목표
심층 신경망이 무엇이지 알 수 있다.

# 핵심 키워드
얕은 모델(Shallow Model)
심층 신경망(Deep Neural Network)

# 학습 내용
* 얼마나 깊은 신경망으 사용해야 하는지 미리 정확하게 예측하기는 어렵습니다.
* 표기법
1. L : 네트워크 층 수
2. n[l] : l층에 있는 유닛 개수
3. a[l] : l층에서의 활성값
4. a[0] : 입력 특징(X)
5. a[L] : 예측된 출력값(y^)

# 인트로
지금까지 로지스틱 회귀 분석뿐만 아니라 단일 은닉층을 가진 신경망의 순방향 전파와 역전파를 살펴보았습니다.   
그리고 벡터화와 왜 무작위로 가중치를 초기화하는 것이 중요한지도 배웠습니다.    
지난 몇 주 간의 과제를 했다면 이 아이디어들이 동작하는 과정을 스스로 구현해보았을 것입니다.    
심층 신경망을 구현하기 위한 대부분의 아이디어를 배웠습니다.    
이번 주에 할 것은 그 아이디어들을 받아 함께 합쳐서 여러분만의 심층 신경망을 구현할 수 있게 하는 것입니다.     

# 심층 신경망
![image](https://user-images.githubusercontent.com/50114210/64337827-2120c180-d01b-11e9-8a56-47e5c0e017fb.png)     
심층 신경망이란 무엇일까요? 로지스틱 회귀에서 이 그림을 보셨을 겁니다. 그리고 하나의 은닉층을 가진 신경망도 보셨을 겁니다.    
여기 두 개의 은닉층과 다섯 개의 은닉층을 가진 신경망의 예시가 있습니다.   
우리는 로지스틱 회귀를 매우 얕은 모델이라고합니다.    
반면에 여기있는 모델은 더 깊은 모델입니다. 얕음과 깊은은 정도의 문제입니다.   
따라서 하나의 은닉층을 가진 신경망은 두 개의 층이 있는 신경망입니다.    
신경망의 층을 셀 때는 입력층은 세지 않습니다. 은닉층과 출력층만 세게됩니다.    
이 두개의 층을 가진 신경망은 여전히 얕지만 로지스틱 회귀만큼은 아닙니다.   
기술적으로 로지스틱 회귀는 한 층의 신경망입니다.   
그러나 지난 몇 년동안 기계 학습의 커뮤니티의 인공지능이 얕은 모델은 보통 할 수 없는 학습을 심층 신경망으로 가능하게 하는 함수가 있다는 것을 깨달았습니다.    
비록 그 어떤 주어진 문제에 대해서 얼마나 깊은 신경망을 사용해야 하는지 미리 정확하게 예측하기는 어려울 것입니다.    
따라서 로지스틱 회귀를 시도하고 그 다음에 두 개의 은닉층을 시도합니다.   
은닉층의 개수가 또 다른 하이퍼파라미터가 됩니다.     
개발 설정 과정에서 다양한 값을 시도하고 검즈 데이터에서 평가합니다.    

# 심층 신경망의 표기법
### 네트워크 층의 수를 나타내는 L
![image](https://user-images.githubusercontent.com/50114210/64337883-431a4400-d01b-11e9-867f-44d72d2c5d33.png)     
이제 심층 신경망을 설명하기 위한 표기법을 알아보겠습니다.    
여기 1, 2, 3, 4개 층의 신경망이 있습니다. 은닉층은 세 개입니다.    
그리고 각각의 은닉층의 유닛 개수는 5, 5, 3이고 한 개의 출력 유닛이 있습니다.   
대문자 L을 써서 네트워크의 층의 수를 나타내도록 하겠습니다.   
![image](https://user-images.githubusercontent.com/50114210/64337899-4dd4d900-d01b-11e9-94fa-c2139592f3d9.png)    
이 경우에 L은 4가 됩니다. 층의 개수입니다.    
### 어느 레이어의 노드인지 나타내 주는 n[l]
![image](https://user-images.githubusercontent.com/50114210/64337924-5c22f500-d01b-11e9-8533-452177b6419c.png)   
그리고 n에 위첨자 l을 써서 소문자 L층의 단위의 개수를 나타내도록 하겠습니다.    
![image](https://user-images.githubusercontent.com/50114210/64337969-71981f00-d01b-11e9-876e-cab0ca65d43f.png)    
이제 이 출력을 layer0으로 색인하면 layer 1, 2, 3, 4, 5가 됩니다.    
그럼 n[1]은 5가 될 것입니다. 왜냐하면 5개의 은닉층이 있기 때문입니다.    
두 번째 은닉층에 대한 단위의 개수는 마찬가지로 5가 될 것입니다.
n[3]은 3이고 n[4]는 n[L]과 같고 이 출력 단위의 개수는 1입니다. 대문자 L이 4와 같으니까요.      
그리고 입력층, 즉 n[0]은 nx와 같고 3입니다. 따라서 서로 다른 층에서 노드의 개수를 나타내는 표시법이었습니다.  
### 어느 레이어의 활성값인지 나타내 주는 a[l]
![image](https://user-images.githubusercontent.com/50114210/64338007-8f658400-d01b-11e9-8ab9-03291257f780.png)    
그리고 각각의 층 l에 대해서 a[l]을 사용하는데 층 l에서의 활성값을 나타냅니다.   
그리고 활성화 함수도 층 l에 의해 순서가 매겨집니다. 그리고 w[l]을 사용해 z[l]의 값을 계산하기 위한 가중치를 나타내거나    
비슷하게 z[l]을 계산하기 위해 b[l]이 사용됩니다.   
![image](https://user-images.githubusercontent.com/50114210/64338017-98565580-d01b-11e9-8941-c2746137be02.png)    
마지막으로 표시법을 정리하면 입력 특징은 X라고 불립니다. X는 층 0의 활성값과 같습니다. 따라서 a[0]은 X와 같습니다.    
마지막 층의 활성값인 a[L]은 신경망의 예측된 출력값과 같습니다.
