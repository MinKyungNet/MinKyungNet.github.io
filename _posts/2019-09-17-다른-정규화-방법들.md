---
layout: post
title: "다른 정규화 방법들"
tags: [Data augmentation, Early stopping]
categories: [Improving deep neural networks]
---

# 학습 목표
다른 정규화 방법들을 배운다

# 핵심 키워드
데이터 증식(Data augmentation)
조기 종료(Early stopping)

# 학습 내용
* 데이터 증식    
  * 이미지의 경우 더 많은 훈련 데이터를 사용함으로서 더 많은 과대적합을 해결 할 수 있습니다.
  * 보통 이미지를 대칭, 확대, 왜곡 혹은 회전시켜서 새로운 훈련 데이터를 만듭니다.
  * 이런 추가적인 가짜 이미지들은 완전히 새로운 독립적인 샘플을 얻는 것보다 더 많은 정보를 추가해주지는 않지만, 커뮤터적인 비용이 들지 않고 할 수 있다는 장점이 있습니다.
* 조기 종료
  * 훈련세트의 오차는 단조하강함수로 그려질 것입니다.
  * 조기종료에서는 개발 세트의 오차도 그려줍니다.
  * 만약에 개발세트의 오차가 어느 순간부터 하락하지 않고 증가하기 시작하는 것이라면 과대적화가 되는 시점입니다.
  * 따라서, 조기 종료는 신경망이 개발 세트의 오차 저점 부근, 즉 가장 잘 작동하는 점일때 훈련을 멈추는 것입니다.
  * 단점 : 훈련시 훈련 목적인 비용 함수를 최적화 시키는 작업과 과대적합하지 않게 만드는 방법이 있습니다.
  두 작업은 별개의 일이라서 두 개의 다른 방법으로 접근해야합니다. 그러나 조기 종료는 두 가지를 섞어 버립니다.
  그래서 최적의 조건을 찾지 못할 수도 있습니다.
  
# 데이터 증식(Data augmentation)
![image](https://user-images.githubusercontent.com/50114210/65005614-60b0ad00-d93b-11e9-8c32-a14e800ff465.png)   
L2 정규화와 드롭아웃 정규화와 더불어 신경망의 과대적합을 줄이는 다른 기법들을 알아봅시다.    
고양이 분류기를 훈련시키는 경우 더 많은 훈련 데이터가 과대적합을 해결하는데 도움을 줄 수 있지만,    
많은 비용이 들어가거나 불가능한 경우가 있습니다.    
그럴 때 수평 방향으로 뒤집은 이미지를 훈련 세트에 추가시켜 훈련 세트를 늘리는 방법을 사용합니다.    
오른쪽의 이미지도 샘플에 추가하는 것입니다.    
따라서 이미지를 수평 방향으로 뒤집어 훈련 세트를 두 배로 증가시킬 수 있습니다.    
새로운 m개의 독립적인 샘플을 얻는 것보다 이 방법은 중복된 샘플들이 많아져 좋지 않지만,  
새로운 고양이 사진을 더 많이 구하지 않고 적용할 수 있는 방법입니다.   

이 방법 외에도 무작위로 이미지를 편집해 새로운 샘플을 얻을 수도 있습니다.    
여기서는 이미지를 회전시키고 무작위로 확대시켰습니다. 여전히 고양이처럼 보이긴 합니다.   
이미지의 무작위적인 왜곡과 변형을 통해 데이터 세트를 증가시키고 추가적인 가짜 훈련 샘플을 얻을 수 있습니다.    
이런 추가적인 가짜 이미지들은 완전히 새로운 독립적인 고양이 샘플을 얻는 것보다 더 많은 정보를 추가해주지는 않을 것입니다.     
그러나 컴퓨터적인 비용이 들지 않고 할 수 있다는 장점이 있습니다. 데이터를 더 얻을 수 있는 비싸지 않은 방법입니다.    
데이터 증식을 통해 정규화를 하고 과대적합을 줄일 수 있습니다.    
이렇게 합성한 이미지를 통해 사용하는 알고리즘에게 고양이 이미지를 뒤집어도 여전히 고양이라는 것을 학습시킬 수 있습니다.     
확대한 경우에도 여전히 고양이 이미지라는 것을 알 수 있습니다.

###
![image](https://user-images.githubusercontent.com/50114210/65005636-77570400-d93b-11e9-88a8-df760f2722a6.png)  
시각적인 문자 인식의 경우 숫자를 얻어 무작위의 회전과 왜곡을 부여할 수 있습니다.    
이것들을 훈련 세트에 추가해도 여전히 숫자 4를 나타냅니다.    
에를 들기 위해 왜곡이 심한 구불구불한 4를 넣었는데     
실제로는 이렇게 심한 왜곡을 줄 필요없이 미묘하게 왜곡시키면 됩니다.    

# 조기 종료(Early stopping)
![image](https://user-images.githubusercontent.com/50114210/65005666-905fb500-d93b-11e9-87a5-140ec45fe635.png)       
조기 종료라고 부르는 또 다른 기법이 있습니다.     
경사 하강법을 실행하면서 훈련 세트에 대한 분류 오차 즉 훈련 오차를 그리거나 최적화하는 비용함수 J를 그리게 됩니다.     
훈련 오차나 비용함수 J는 다음과 같이 단조 감소하는 형태로 그려져야합니다.     
조기 종료에서는 개발 세트 오차도 함께 그려줍니다.       
이것은 개발 세트의 분류 오차 혹은 개발 세트에서 평가되는 로지스틱 손실 함수라고 볼 수 있습니다.     
여기서 발견할 수 있는 것은 개발 세트 오차가 아래로 내려가다가 이 부분에서 증가하는 것입니다.    
dev셋을 보면서 신경망이 가장 잘 작동하는 곳에서 훈련을 멈춥니다.    
왜 이 방법이 작동할까요? 신경망에서 많은 방복을 실행시키지 않은 경우 매개변수 w는 0에 가깝습니다.    
무작위의 작은 값으로 초기화시켜서 오랜 시간 훈련시키기 전까지 w의 값은 여전히 작습니다.    
반복을 실행할수록 w의 값은 계속 커집니다. 따라서 이 지점에서는 매개변수 w의 값이 훨씬 커진 상태입니다.     
조기 종료 기법에서 반복을 중간에 멈추면 w가 중간 크기의 값을 갖는 상태입니다.    
L2 정규화와 비슷하게 매개변수 w에 대해 더 작은 노름을 갖는 신경망을 선택함으로써 신경망이 덜 과대적합하게 됩니다.    
조기 종료가 의미하는 말처럼 신경망을 조기로 종료합니다.    

### 조기 종료의 단점
![image](https://user-images.githubusercontent.com/50114210/65005699-b1280a80-d93b-11e9-9752-b0f24213b3a7.png)    
조기 종료를 사용하는데 하나의 단점이 있습니다.     
머신러닝 과정은 서로 다른 몇가지 단계로 이루어져 있습니다.     
첫 번째로 비용함수 J를 최적화하는 알고리즘을 원합니다. 경사 하강법처럼 이를 위한 몇 가지 알고리즘이 있습니다.   
비용함수 J를 최적화하고 난 뒤에 과대적합되는 것을 막기 위한 몇가지 도구들이 또 있습니다.   
정규화, 데이터 더 추가하기 등의 방법입니다.    
머신러닝에서 이미 아주 많은 하이퍼파라미터들이 있고 여러 가능한 알고리즘 중 선택하는 것은 매우 복잡합니다.    
따라서 비용함수 J를 최적화하는 하나의 도구 세트만 있다면 머신러닝이 훨씬 더 간단해질 것이라고 생각합니다.    
비용함수 J를 최적화할 때 집중하는 것은 w와 b를 찾는 것입니다.    
J(w, b)가 가능한 작아지는 값을 찾는 것 외에는 신경쓰지 않습니다.    
과대 적합을 막는 것은 완전히 다른 말입니다. 다른 말로는 분산을 줄이는 것이죠.    
이를 위한 별개의 도구들이 필요합니다. 제가 생각하는 조기 종료의 주된 단점은 이 둘을 섞어버린다는 것입니다.    
이 두 문제에 대해 더 이상 독립적으로 작업할 수 없게 됩니다.   
왜냐하면 경사 하강법을 일찍 멈춤으로써 비용함수 J를 최적화하는 것을 멈추게 됩니다.    
비용함수 J를 줄이는 일을 잘 하지 못하게 됩니다. 동시에 과대적합을 막으려고 합니다.    
따라서 두 문제를 해결하기 위해 서로 다른 도구를 사용하는 대신 혼합된 하나의 도구를 사용하게 됩니다.    
그리고 이것은 문제를 더 복잡하게 만듭니다. 조기 종료를 사용하는 것의 대안은 L2 정규화 사용하는 것입니다.    
그럼 가능한 오래 신경망을 훈련시킬 수 있게 됩니다.   
이를 통해 하이퍼파라미터의 탐색 공간이 더 분해하기 쉽고 찾기 쉬워집니다.   
그러나 단점은 정규화 매개변수 람다에 많은 값을 시도해야 한다는 것입니다.   
람다의 많은 값을 대입하는 것은 컴퓨터적으로 비용이 많이 듭니다.      
조기 종료의 진짜 장점은 경사 하강법 과정을 한번만 실행해서 작은 w, 중간 w, 큰 w의 값을 얻게 됩니다.    
많은 값을 시도할 필요없이 말입니다.   

# 아웃트로
단점이 분명하지만 많은 사람들이 조기 종료 기법을 사용합니다.     
저는 L2 정규화를 사용해 람다에 많은 값을 시도하는 방법을 선호합니다. 컴퓨터적으로 감당할 수 있다면요.     
그러나 조기 종료는 명시적으로 많은 값을 람다에 시도하지 않고도 비슷한 효과를 가져올 수 있습니다.    







  
