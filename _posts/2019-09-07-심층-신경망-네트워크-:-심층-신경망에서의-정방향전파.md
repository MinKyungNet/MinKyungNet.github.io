---
layout: post
titie: "심층 신경망 네트워크 : 심층 신경망에서의 정방향전파"
tags: [Foward Propagation, Deep Neural Netowrk]
categories: [Neural Networks and Deep Learning]
---

# 학습 목표
심층 신경망에서 정방향 전파를 어떻게 수행하는지 알 수 있다.

# 핵심 키워드
* 심층 신경망(Deep Neural Network)
* 정방향 전파(Forward propagation)

# 학습 내용
강의를 따라서 이전에 배운 지식을 상기하고, 한 층씩 정방향 전파를 진행해보시길 바랍니다.

# 인트로
지난 비디오에서 l층의 심층 신경망이 무엇인지 알아보았고, 그런 네트워크를 나타내기 위한 표기법을 알아보았습니다.    
이 글에서는 정방향 전파를 어떻게 수행하는지 살펴봅시다.

# 단일 학습 데이터에 대한 정방향 전파
![image](https://user-images.githubusercontent.com/50114210/64477055-e4e39180-d1d1-11e9-933a-70ffe65b32cb.png)    
### 1층 계산
![image](https://user-images.githubusercontent.com/50114210/64477184-448e6c80-d1d3-11e9-9324-cf144f6ec540.png)      
주어진 단일 학습 데이터 x에 대해 첫 번째 층에 대한 활성화를 계산하는 방법입니다.    
첫 번째 층에 대해 z[1]은 W[1] * x + b[1]과 같습니다.
W[1]과 b[1]은 층 1에서 활성화에 영향을 주는 파라미터들입니다.    
여기가 심층 신경망의 층 1입니다. 그리고 해당 층에 대한 활성화를 계산하는데, g(z[1])과 같습니다.    
활성화 함수 g는 층마다 다릅니다. 따라서 층 1에 대한 활성화 함수를 계산하는 것이죠.    
### 2층 계산
![image](https://user-images.githubusercontent.com/50114210/64477149-f9745980-d1d2-11e9-8f9a-824e848b04ff.png)    
층 2는 어떨까요?    
그럼 z[2]는 W[2] * a[1] + b[2]와 같습니다.      
따라서 층 2의 활성화는 가중치 행렬에 층 1의 출력은 곱한 것에 층 2의 편향 벡터를 더한 값입니다.     
그럼 a[2]는 z[2]에 활성화 함수를 적용한 값입니다. 이것이 층 2에 관한 계산입니다.    
이런 식으로 출력 층인 층4에 이를 때까지 계속합니다.    
### 4층 계산
![image](https://user-images.githubusercontent.com/50114210/64477162-17da5500-d1d3-11e9-8e1b-c35bc22b2b4d.png)     
z[4]는 해당 층의 파라미터에 이전 층의 활성화를 곱한 것에 편향 벡터를 더해준 것과 같습니다.    
그리고 비슷하게 a[4]는 g[4](z[4])와 같습니다.    
이것이 추정된 y의 예측값을 계산하는 방법입니다.
### a[0] = x
![image](https://user-images.githubusercontent.com/50114210/64477172-2cb6e880-d1d3-11e9-9fe2-e3d5782a4040.png)     
여기 있는 x는 a[0]과 같습니다. 왜냐하면 입력 요인 벡터 x는 층 0의 활성화이기 때문입니다.    
따라서 x를 지우고 a[0]을 넣습니다.    
기본적으로 이 모든 수식들이 똑같이 생겼죠?     
![image](https://user-images.githubusercontent.com/50114210/64477193-67b91c00-d1d3-11e9-8e41-9af9debaad81.png)    
일반적인 규칙은 z[l]은 W[l] * a[l - 1] + b[l]입니다.    
그리고 해당 층에 대한 활성화는 z[l]에 활성화 함수 g[l]을 적용한 값입니다.    
따라서 이것이 일반적인 정방향 전파의 수식입니다.

# 전체 학습 세트에 대한 벡터화된 정방향 전파
![image](https://user-images.githubusercontent.com/50114210/64477211-a058f580-d1d3-11e9-8f70-286441b8a38a.png)     
수식은 첫 번째 층에 대해 전과 비슷한 방식입니다.   
Z[1]은 W[1] * X + b[1]과 같습니다.   
그리고 A[1]은 g[1](Z[1])과 같습니다.    
X가 A[0]과 같다는 것을 알아두세요. 서로 다른 열에 저장된 학습 데이터를 말합니다.    
X를 지우고 A[0]을 넣습니다. 다음 층에 대해서도 마찬가지로    
Z[2]는 W[2] * A[1] + b[2]와 같습니다.   
A[2]는 g[2](Z[2])와 같습니다.   
Z와 a벡터의 값을 얻어 쌓은 것과 같습니다.    
이런 식으로 m번째 학습 데이터까지하고 이것들을 열에 저장해서 Z라고 부릅니다.    
A도 비슷한 방식입니다. X처럼 모든 학습 데이터는 왼쪽에서 오른쪽으로 저장된 열 벡터의 형태입니다.     
그리고 이 과정은 Y의 예측값인 g(Z[4])즉 A[4]로 끝납니다.    
이것은 수평으로 저장된 모든 학습 데이터에 대한 예측값입니다.    
따라서 정리하기 위해, 여기 있는 표기법을 수정하겠습니다. 소문자 z와 a를 대응되는 대문자로 수정합니다.    
이를 통해 정방향 전파의 벡터화된 버전으로 바꿀 수 있습니다.    
### for문 
![image](https://user-images.githubusercontent.com/50114210/64477207-8e775280-d1d3-11e9-972a-3abae8f5a3ac.png)     
A[0]이 X인 전체 학습 세트에 대해 한 번에 수행할 수 있습니다.   
벡터화 구현을 살펴보면 여기에 반복문을 사용할 것 같습니다.
for l = 1.. 4    
즉 1에서 L까지의 l에 대해 각각의 층에 대해 활성화를 계산해야합니다.    
따라서 여기에 반복문이 필요해 보입니다.    
네트워크를 구현할 때는 명시적인 반복문을 보통 피하고 싶어합니다.   
그러나 명시적 반복문 외에 이 부분을 구현할 수 있는 다른 방법이 없다고 생각합니다.   
따라서 정방향 전파를 구현할 때 층 1, 2, 3, 4각각에 대한 활성화를 계산하는 반복문을 사용하는 것은 괜찮습니다.    
