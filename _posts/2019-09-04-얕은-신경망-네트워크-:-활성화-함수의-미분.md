---
layout: post
title: "얕은 신경망 네트워크 : 활성화 함수의 미분"
tags: [Derivatives]
comments: true
categories: [Neural Networks and Deep Learning]
---

# 학습 목표
* 활성화 함수를 미분할 수 있다.

# 핵심 키워드
* 미분(Derivatives)

# 학습 내용
![image](https://user-images.githubusercontent.com/50114210/64228002-74621980-cf20-11e9-854e-ef59a3ba938b.png)
![image](https://cphinf.pstatic.net/mooc/20180622_108/1529646808652xnTuf_PNG/plot1.png)
---
# 인트로
신경망의 역방향 전파를 구하려면 활성화 함수의 도함수를 구해야합니다. 저희가 고른 활성화 함수와 그 함수의 기울기를 어떻게 구하는지 살펴봅시다.

# Sigmoid
![image](https://user-images.githubusercontent.com/50114210/64228438-cd7e7d00-cf21-11e9-845a-1026fe614fe5.png)

주어진 값 z에 대해 이 함수는 특정한 기울기를 가집니다.
선을 그었을때 삼각형의 높이를 너비로 나눈 값이죠. 
g(z)가 시그모이드 함수일 때 함수의 기울기는 d/dz * g(z)가 됩니다. 
미적분에서 이 값은 z에서의 기울기가 되죠.
<br/>
<br/>
<br/>
![image](https://user-images.githubusercontent.com/50114210/64228485-fe5eb200-cf21-11e9-84ee-701847da071c.png)

미적분을 사용해 시그모이드 함수의 도함수를 어떻게 구하는지 안다면 여기 쓰여있는 공식과 같다는 걸 보일 수 있습니다.
이 값은 g(z) * (1 - g(z))와 같습니다.
<br/>
<br/>
<br/>
![image](https://user-images.githubusercontent.com/50114210/64228499-0c143780-cf22-11e9-96fe-c8b8379f401b.png)

이 공식이 맞는지 한번 확인해보죠
z가 굉장히 큰 값인 10일 때 g(z)는 1에 가까워집니다.
왼쪽의 공식에 따르면 d / dz * g(z)는 1(1-1)과 비슷해지겠죠. 0과 가까워집니다.
z가 커지면 기울기가 0에 가까워지기 때문에 맞는 공식임을 알 수 있습니다.

z가 아주 작은 값인 -10이라면 g(z)는 0에 가까워지고 왼쪽의 공식은 0(1 - 0)에 가까워집니다.

이 값도 0에 가까워지죠. z가 0이라고 가정하면 g(z)는 시그모이드 함수에서 볼 수 있듯이 1 / 2이 됩니다.
따라서 도함수는 1 / 2 * (1 - 1 / 2)인 1 / 4 가 됩니다. z가 0일 때의 기울기 혹은 도함수의 값과 정확히 일치합니다.
<br/>
<br/>
<br/>
![image](https://user-images.githubusercontent.com/50114210/64228530-21896180-cf22-11e9-929b-711b1ae700af.png)

한가지 표기법을 더 소개하면 도함수를 표현하는데 이 식을 쓰는 대신 g'(z)라고 쓸 수 있습니다. 
미적분에서 g'(z)는 입력 변수 z에 대한 g의 도함수를 의미합니다.
신경망에서 a는 g(z)와 같고 이 식과 같습니다.
따라서 이 식은 a(1 - a)로 간소화 할 수 있습니다.
가끔 구현에서 g'(z)가 a(1 - a)라고 되어있는 걸 볼 수도 있을 겁니다.
g'(z)가 여기 쓰인 a(1 - a)와 같기 때문이죠.
이 공식의 장점은 이미 a의 값을 계산했다면 g'의 값을 빠르게 계산할 수 있다는 겁니다.

# Tanh
![image](https://user-images.githubusercontent.com/50114210/64229721-8c886780-cf25-11e9-8e34-1b48a950ff4c.png)
![image](https://user-images.githubusercontent.com/50114210/64229666-6236aa00-cf25-11e9-85a1-4e8edb0414f5.png)

이번엔 tanh활성화 함수를 보죠.
전에 봤던 것처럼 d / dz * g(z)는 z에서 g의 기울기입니다. 
tanh함수의 공식을 살펴보고 미적분을 안다면 도함수를 구해 이 식과 같다는 걸 보일 수 있습니다.
전에 쓴 것처럼 이 식을 g'(z)라고 하겠스니다.
<br/>
<br/>
<br/>
![image](https://user-images.githubusercontent.com/50114210/64229683-6c58a880-cf25-11e9-8dbd-2c9fc36a54f5.png)

이 공식이 말이 된다는 걸 확인해보죠.
z가 10이라면 tanh(z)는 1에 가까워지고 tanh는 -1부터 1까지 값을 가집니다.
이 공식에 따르면 g'(z)는 1 - 1^2인 0에 가까워집니다.
z가 큰 값이면 기울기는 0에 가까워지죠.

z가 매우 작은 값인 -10이라면 tanh(z)는 -1에 가까워집니다.
g'(z)는 1에서 -1의 제곱을 뺀 0에 가까워지죠.

z가 0이라면 tanh(z)는 0이되고 기울기는 1이 됩니다. 실제로도 기울기는 1이죠.
<br/>
<br/>
<br/>
![image](https://user-images.githubusercontent.com/50114210/64229701-78446a80-cf25-11e9-802a-3dd28f34dca4.png)

요약하자면 a가 g(z)인 tanh(z)일 때 도함수인 g'(z)는 1 -a^2이 됩니다.

a의 값을 이미 계산했다면 이 공식을 사용하여 도함수도 빠르게 계산할 수 있습니다.

# ReLU
![image](https://user-images.githubusercontent.com/50114210/64229751-a75adc00-cf25-11e9-8f8c-4049f1a855a9.png)
![image](https://user-images.githubusercontent.com/50114210/64229763-afb31700-cf25-11e9-8095-e03d489afb1d.png)

ReLU에서 g(z)는 max(0, z)가 됩니다.
<br/>
<br/>
<br/>
![image](https://user-images.githubusercontent.com/50114210/64229774-bc376f80-cf25-11e9-9fc4-a4efc89f1bba.png)

따라서 도함수는 z가 0보다 작을 때는 0이고
z가 0보다 크면 1이됩니다.
z가 0일 때는 엄밀하게는 정의되지 않았습니다.
수학적으로 정확하지는 않지만 실제로 z가 0일 때 도함수를 1이라고 해도 문제 없습니다.
0이라고 해도 전혀 문제없죠.
최적화에 익숙하다면 g'는 활성화 함수 g(z)의 서브 경사이기 때문에 경사 하강법이 잘 작동합니다.
z가 정확히 0이 될 확률은 정말 작기 때문에 z가 0일 때 도함수를 뭐라고 하든지 상관없습니다.
따라서 실제로는 g'(z)를 이렇게 구현하죠

# Leaky ReLU
![image](https://user-images.githubusercontent.com/50114210/64229800-d07b6c80-cf25-11e9-8864-afc912497bc9.png)
![image](https://user-images.githubusercontent.com/50114210/64229809-d7a27a80-cf25-11e9-9a36-2a01013b772c.png)

Leaky ReLU에서 g(z)는 max(0.01 * z, z)가 됩니다.
<br/>
<br/>
<br/>
![image](https://user-images.githubusercontent.com/50114210/64229819-dffab580-cf25-11e9-9c76-3eecca740f80.png)

g'(z)는 z가 0보다 작을 때 0.01이 되고
z가 0보다 클 때는 1이됩니다.
여기서도 z가 정확히 0이라면 도함수가 정의되지 않았지만 코드를 쓸 때에는 z가 0일 대 g'을 0.01이나 1둘 중에 아무렇게나 설정해도 괜찮습니다.











