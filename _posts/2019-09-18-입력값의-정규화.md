---
layout: post
title: "입력값의 정규화"
tags: [Normalizing]
categories: [Improving deep neural networks]
---

# 학습 목표
입력을 정규화 할 수 있다.

# 핵심 키워드
정규화(Normalizing)

# 학습 내용
* 정규화 방법
  1. 평균을 0으로 만듭니다.
  ![image](https://user-images.githubusercontent.com/50114210/65094585-85b72580-d9f8-11e9-8a36-03962cd95bbf.png)          
  2. 분산을 1으로 만듭니다.
  ![image](https://user-images.githubusercontent.com/50114210/65094597-923b7e00-d9f8-11e9-90dc-60f27526cf77.png)         
* 테스트 세트 정규화할 때 훈련 데이터에 사용한 u,a를 사용해야합니다.
* 정규화를 통해 비용함수의 모양은 더 둥글고 최적화하기 쉬운 모습이 됩니다. 그로 인해 학습 알고리즘이 빨리 실행됩니다.

# 정규화
정규화는 신경망을 빠르게 학습시킬 수 있는 하나의 기법입니다.    

### 원본 데이터
![image](https://user-images.githubusercontent.com/50114210/65094664-daf33700-d9f8-11e9-9581-e4bc34423d52.png)           
두 개의 입력 특성이 있는 훈련 세트가 있습니다. 입력 특성 x가 2차원입니다.   
그리고 여기 훈련 세트의 산포도가 있습니다. 두 단계에 걸쳐 입력을 정규화하겠습니다.     

### 평균 빼기
![image](https://user-images.githubusercontent.com/50114210/65094732-17bf2e00-d9f9-11e9-9175-2594a917d185.png)         
첫번째는 평균을 빼는 것입니다. 즉 평균을 0으로 만드는 것입니다.    
위의 식처럼 모든 입력을 다 더한 후에 입력의 개수로 나눠서 평균을 구합니다.   
그리고 입력에 평균을 빼줘서 평균이 0이 되도록합니다.    
![image](https://user-images.githubusercontent.com/50114210/65094771-4a692680-d9f9-11e9-9f16-26f1bd743971.png)        

### 분산 정규화
두번째 단계는 분산을 정규화하는 것입니다. 특성 x1이 특성 x2보다 더 큰 분산을 갖고 있습니다.    
![image](https://user-images.githubusercontent.com/50114210/65094811-7ab0c500-d9f9-11e9-84ed-7fc910fa2d81.png)           
**은 요소별 제곱을 나타냅니다. a^2는 각 특성의 분산에 대한 벡터입니다.    
이미 평균을 빼버렸기 때문에 x(i)의 요소별 제곱은 분산입니다.    
각 샘플을 a^2벡터로 나눠주게 되면 아래와 같은 그림이 됩니다.
![image](https://user-images.githubusercontent.com/50114210/65094883-d67b4e00-d9f9-11e9-987b-beffafa01b71.png)       

# 훈련시에 사용했다면 테스트 할 때도 사용하자
하나의 팁을 더 드리자면 훈련데이터를 정규화했다면, 테스트 데이터도 같은 값으로 정규화해야합니다.    
훈련데이터와 테스트 데이터를 다르게 정규화하면 네트워크가 학습한 것과는 다른 입력을 받기 때문에
u와 a가 어떤 값이든 테스트할 때도 같은 방식으로 정규화를 해줘야합니다.     

# 정규화를 왜 하는걸까?    
### 정규화되지 않은 입력 특성
![image](https://user-images.githubusercontent.com/50114210/65095030-7638dc00-d9fa-11e9-8b0f-71e11ea74602.png)        
정규화되지 않은 입력 특성을 사용하면 비용함수가 위와 같은 모습이 됩니다.    
매우 구부러진 활처럼 가늘고 긴 모양의 비용함수가 됩니다.     
우리가 찾으려 하는 최소 값은 중앙에 위치하는데 특성들이 매우 다른 크기를 가지고 있다면,    
예를 들어 특성 x1이 1에서 1000 특성 x2가 0에서 1의 값의 범위라면          
매개볌수에 대한 비율, 값의 범위는 w1과 w2가 굉장히 다른 값을 갖게 될 것입니다.    

### 정규화되지 않은 비용함수
![image](https://user-images.githubusercontent.com/50114210/65095138-f3645100-d9fa-11e9-8e88-c8b572d13946.png)        
위의 예제라면 비용함수는 이처럼 매우 가늘고 긴 모양이 될 것입니다.     
함수의 등고선을 그려보면 이와 같은 함수를 얻습니다.    
정규화하지 않은 비용함수에 경사하강법을 실행한다면 매우 작은 학습률을 사용하게 될 것입니다.     
왜냐하면 최종적으로 최소값에 이르는 길을 찾기 전까지        
앞뒤로 왔다 갔다하기 위해 많은 단계가 필요하기 때문입니다.    

### 정규화한 입력 특성
![image](https://user-images.githubusercontent.com/50114210/65095193-2ad2fd80-d9fb-11e9-98fb-70570a20cc4e.png)               
반면에 특성을 정규화하면 비용함수는 평균적으로 대칭적인 모양을 갖게 됩니다.    

### 정규화한 비용 함수
![image](https://user-images.githubusercontent.com/50114210/65095323-9fa63780-d9fb-11e9-8e35-5e23e988414a.png)
원 모양의 등고선의 경우 어디서 시작하든 경사 하강법은 최소값으로 바로 갈 수 있습니다.     
정규화를 하지 않은 비용함수처럼 왔다갔다 하지 않아도 큰 스텝으로 전진할 수 있습니다.

# 입력 특성이 비슷한 범위일 때 학습이 용이함
실전에서는 w가 높은 차원의 벡터이고 2차원에 그리는 것이 모든 직관을 올바르게 전달하지 않습니다.    
그러나 특성이 비슷한 크기를 가질때         
비용함수가 더 둥글고 최적화하기 쉬운 모습이 된다는 대략적인 직관을 얻을 수 있습니다.    
1에서 1000, 0에서 1이 아닌    
대부분 -1에서 1혹은 서로 비슷한 분산으로 비용함수 J를 최적화하기 쉽고 빠르게 만듭니다.     
예를 들어 x1이 1~1000, x2가 0~1의 범위를 가진다면 최적화가 어려워지겠지만    
x1이 0~1, x2가 -1에서 1, x3가 1에서 2인 경우에 이들은 상당히 비슷한 범위를 가지기 때문에    
아주 다른 범위를 가지는 특성과는 다르게 잘 작동될 것입니다.    
이처럼 너무 다른 범위는 최적화 알고리즘에 방해가 됩니다.    
1. 모든 것을 0의 평균으로 설정하고     
2. 모든 특성을 비슷한 크기로 보장할 수 있는 분산을 설정하면    
학습 알고리즘이 빠르게 실행되는 것을 도울 것입니다.      
따라서 만약 입력 특성이 매우 다른 크기를 갖는다면 특성을 정규화하는 것이 중요합니다.    

# 아웃트로
만약 특성이 비슷한 크기를 갖는다면 이 단계는 그렇게 중요하지 않습니다.    
그러나 이런 정규화는 어떤 해도 가하지 않기 때문에 되도록 하는 것을 추천드립니다.    
이 과정이 알고리즘을 빠르게 훈련하는데 도움을 줄지 확신하지 않더라도요.

