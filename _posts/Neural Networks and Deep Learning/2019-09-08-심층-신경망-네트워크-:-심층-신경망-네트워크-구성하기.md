---
layout: post
title: "심층 신경망 네트워크 : 심층 신경망 네트워크 구성하기"
tags: [Deep Neural Network, Forward Function, Backward Function]
categories: [Neural Networks and Deep Learning]
---
# 학습 목표
정방향 전파와 역방향 전파의 기본 요소를 심층 신경망에서 어떻게 이용할 수 있는지 알 수 있다.

# 핵심 키워드
* 심층 신경망(Deep Neural Network)
* 정방향 함수(Forward Function)
* 역방향 함수(Backward Function)

# 학습 내용
* L번째 층에서 정방향 함수는 이전 층의 활성화 값인 a[L-1]을 입력으로 받고, 다음 층으로 a[L]값을 출력으로 나오게 합니다. 이때 선형결합된 값인 z[L]와 변수 W[L], b[L]값도 캐시로 저장해둡니다.
* L번째 층에서 역방향 함수는 da[L]을 입력으로 받고, da[L-1]을 출력합니다. 이때 업데이트를 위한 dW[l]과 db[L]도 함께 출력합니다. 이들을 계산하기 위해서 정방향 함수때 저장해두었던 캐시를 쓰게 됩니다.


# 인트로
이번 주 초기의 비디오는 지난 몇 주간의 비디오와 마찬가지로 정방향 전파와 역전파의 기본 구성 요소들을 살펴보았습니다.   
심층 신경망을 구현하기 위한 중요한 요소들이었죠. 이제 깊은 망을 만들기 위해 이 요소들을 어떻게 이용할지 알아봅시다.    

# 정방향, 역방향 함수

![image](https://user-images.githubusercontent.com/50114210/64488706-d5be1b80-d285-11e9-807c-89390f181e57.png)    
여기 적은 수의 층을 가진 네트워크가 있습니다. 하나의 층을 골라서 그 층에 집중하여 계산을 해봅시다.    

### 정방향 전파
![image](https://user-images.githubusercontent.com/50114210/64488735-45340b00-d286-11e9-98d5-d34910af3f5d.png)    
층 L에 대해서 매개변수 W[L}과 b{L]이 있습니다.     
그리고 정방향 전파에 대해서는 입력으로 이전 층의 활성화 값인 a[L-1]을 주게 됩니다.    
그럼 출력은 a[L]이 됩니다. 우리가 전에 했던 것입니다.    
z[L]은 W[L] * a[L-1] + b[]과 같습니다. 그럼 a[L]은 g[L](z[L])과 같습니다.     
이것이 입력 a[L-1]에서 출력 a[L]을 얻어내는 방법입니다.   
나중에 사용할 경우를 대비해 z[L]의 값을 저장하는 것이 유용합니다.    
왜냐하면 z[L]의 값을 저장하면 역전파의 단계에서 유용하게 쓰일 수 있기 때문입니다.   

### 역방향 전파
![image](https://user-images.githubusercontent.com/50114210/64488742-5846db00-d286-11e9-88c3-f0c170c9cecb.png)    
그리고 역전파의 단계에서 다시 층 L에 대한 계산에 집중합니다.    
함수를 구현하게 되는데 입력으로 da[L]을 받고 출력으로 da[L-1]이 나옵니다.    
자세히 말하면 입력은 da[L]과 저장해 놓은 z[L]을 사용합니다.   
그리고 da[L-1]과 경사 하강법 학습을 위한 그래디언트를 출력합니다.    
따라서 이것이 정방향과 역방향 단계를 구현하기 위한 기본 구조입니다.    
정방향 함수와 역방향 함수라고도 합니다.     

### 정방향 함수와 역방햠 함수
![image](https://user-images.githubusercontent.com/50114210/64488769-aa87fc00-d286-11e9-935e-249de3ea24bd.png)    
요약하자면 층 L에서 정방향 단계, 전파 혹은 함수가 있습니다.     
입력으로 a[L-1]이 들어가면 출력으로 a[L]이 나옵니다.   
계산을 위해서 W[L]과 b[L]을 사용해야 합니다. 또한 z[L]이 포함된 캐시도 출력됩니다.
그럼 역전파 단계를 사용하는 역방향 함수는 또 다른 함수가 되는데
입력으로 da[L]을 넣으면 출력으로 da[L-1]이 나옵니다.   
따라서 이 활성에 대한 도함수 da[L]을 주면 아떤 도함수를 원할까요?    
이전 층의 활성에 대한 도함수를 계산합니다.    
이 박스에서는 W[L]과 b[L]을 사용합니다. 계산 과정에 따라 dz[L]도 얻습니다.    
그리고 이 역방향 함수는 dW[L] 과 db[L]을 출력합니다.   

# 네트워크 학습의 전반적인 과정

### 정방향
![image](https://user-images.githubusercontent.com/50114210/64488844-677a5880-d287-11e9-8976-7b53794c673b.png)    
이 두 함수를 구현할 수 있다면 신경망의 기본 계산은 다음과 같을 것입니다.    
입력 특성 a[0]을 가져와서 집어넣습니다. 이것이 첫 번째 층의 활성을 계산할 것입니다. a[1]이라고 부릅시다.    
그리고 이것을 위해 W[1]과 b[1]이 필요합니다. 또한 z[1]을 캐시에 저장합니다.     
이 과정이 완료되면 a[1]을 두 번째 층에 집어 넣고 W[2]와 b[2]를 사용해서 다음 층의 값인 a[2]를 계산하게 됩니다.    
최종적으로 a[L]을 출력하게 됩니다. 이 값은 y의 예측값과 같습니다.     
그리고 이 과정에서 모든 층의 z값을 캐시에 저장합니다. 따라서 이것이 정방향 전파 단계입니다.

### 역방향
![image](https://user-images.githubusercontent.com/50114210/64488878-d5bf1b00-d287-11e9-99bd-1f7c8289cf2b.png)     
이제 역전파 단계에 대해 우리가 할 것은 이 반복을 역순으로 하는 것입니다.    
역방향으로 가면서 경사를 계산합니다. 따라서 입력값은 d[L]이고 출력값은 d[L-1]입니다.    
이런식으로 da[3], d[2], d[1]을 계산해 나갑니다.   
역전파의 과정에서 dW[L]과 db[L]을 출력합니다.    
역전파시에도 매개변수들을 사용하게 되는데 W[L], b[L]그리고 나중에 살펴볼 dz[L]도 박스 안에서 계산하게 됩니다.     
따라서 신경망의 하나의 학습 반복은 x값인 a[0]으로 시작해서     
정방향 전파를 따라서 y의 예측값을 계산하고    
이 값으로 da[L]을 계산하고 역전파를 거쳐서 모든 도함수를 얻게 됩니다.     

### 파라미터 업데이트
![image](https://user-images.githubusercontent.com/50114210/64488907-49612800-d288-11e9-9f25-c720addfb160.png)    
따라서 W[L]의 값은 학습률을 곱해 W[L] := W[L] - 학습률 * dW[L]로 업데이트 됩니다.   
각각의 층에 대한 값입니다.   
b에 대해서도 마찬가지로 역전파를 계산해서 이 모든 도함수 값을 알 수 있습니다.    

### 프로그래밍시에 
따라서 신경망에 대한 경사 하강법의 하나의 반복이었습니다.    
넘어가기 전에 구현에 관한 세부사항 하나를 말씀드리겠습니다.    
개념적으로 여기 있는 캐시를 역방향 함수에 대한 z값을 저장하는 곳이라고 생각하는 것이 유용합니다.    
그러나 프로그래밍 예제에서 이것을 실제로 구현할 때 캐시가 매개변수 W[1]과 b[1]의 값을 얻어 역방향 함수에 넣기에 편리한 방법이라는 것을 알게 될 것입니다.    
따라서 프로그래밍 예제에서 실제로 z[L]뿐만 아니라 W[1]과 b[1]도 캐시에 저장하게 될 것입니다.   
따라서 구현의 관점에서 매개변수들의 값을 저장하고 나중에 있을 역전파의 계산에서 필요한 곳에 복사해 사용하는 것이 유용합니다.    

# 전체 과정
![image](https://user-images.githubusercontent.com/50114210/64488958-e1f7a800-d288-11e9-977b-1deb721edf7c.png)    

# 아웃트로
심층 신경망에 영향을 주는 기본 구성 요소들을 알아보았습니다.    
각각의 층에서 정방향 전파 단계와 대응하는 역전파 단계가 있고 한 곳에서 다른 곳으로 전보를 전달하는 캐시가 있습니다.





