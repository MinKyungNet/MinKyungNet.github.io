---
layout: post
title: "깊은 신경망 네트워크 직접 구현해보기"
tags: [Neural Network, Python]
categories: [Neural Networks and Deep Learning]
---
나한테 이런 일이 가능하게 되다니 앤드류 응 교수님 사랑합니다 ㅎ
강의에서는 concate하라고 했는데 그냥 한번에 설정했고
마지막 레이어는 한번에 for문으로 처리하는 방법을 모르겠어서 따로 처리했다.     
오후에는 다른사람 코드 보면서 잘 작성한게 맞는지 확인해봐야겠다.

```python
import numpy as np

def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def d_relu(x):
    return np.where(x > 0, 1, 0)

m = int(input())
X = np.random.rand(2, m)
Y = np.random.randint(2, size = (1, m))
n = [2, 3, 5, 4, 2, 1]
L = len(n)
W = [0 for i in range(L)]
b = [0 for i in range(L)]
Z = [0 for i in range(L)]
A = [0 for i in range(L)]
A[0] = X
dW = [0 for i in range(L)]
db = [0 for i in range(L)]
dZ = [0 for i in range(L)]
dA = [0 for i in range(L)]
learning_rate = 0.05
epoch = 1000
for i in range(1, L):
    W[i] = np.random.randn(n[i], n[i - 1])

for i in range(1, L):
    b[i] = np.random.randn(n[i], m)


for _ in range(epoch):
    for i in range(1, L-1):
        Z[i] = np.dot(W[i], A[i-1]) + b[i]
        A[i] = relu(Z[i])

    Z[L-1] = np.dot(W[L-1], A[L-2]) + b[L-1]
    A[L-1] = sigmoid(Z[L-1])

    dA[L-1] = -(Y / A[L-1]) + np.divide((1 - Y), (1 - A[L-1]))
    dZ[L-1] = dA[L-1] * A[L-1] * (1 - A[L-1])
    dW[L-1] = 1/m * np.dot(dZ[L-1], A[L-2].T)
    db[L-1] = 1/m * np.sum(dZ[L-1], axis=1, keepdims=True)

    for i in range(L-2, 0, -1):
        dA[i] = np.dot(W[i+1].T, dZ[i+1])
        dZ[i] = dA[i] * d_relu(Z[i])
        dW[i] = 1/m * np.dot(dZ[i], A[i-1].T)
        db[i] = 1/m * np.sum(dZ[i], axis=1, keepdims=True)

    dW = np.array(dW)
    db = np.array(db)

    W = W - learning_rate * dW
    b = b - learning_rate * db
    print(A[5])
print(Y)
```
![image](https://user-images.githubusercontent.com/50114210/64488958-e1f7a800-d288-11e9-977b-1deb721edf7c.png)
