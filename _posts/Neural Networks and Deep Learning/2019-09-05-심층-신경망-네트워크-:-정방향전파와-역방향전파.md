---
layout: post
title: "심층 신경망 네트워크 : 정방향전파와 역방향전파"
tags: [Forward Propagation, Back Propagation]
categories: [Neural Networks and Deep Learning]
---

# 학습 목표
정방향전파와 역방향전파의 과정을 다시 전체적으로 복습한다.

# 핵심 키워드
* 정방향 전파(Forward Propagation)
* 역방향 전파(Back Propagation)

# 학습 내용
* l 번째 층에서 정방향전파는 이전 층의 활성화 값인 a[l-1]을 입력으로 받고, 다음 층으로 a[l]값을 출력으로 나오게합니다. 
이 때 선형결합된 값인 z[l]와 변수 W[l], b[l]값도 캐시로 저장해둡니다.

* l 번째 층에서 역방향전파는 da[l]을 입력으로 받고, da[l]을 출력합니다. 이 때 업데이트를 위한 dW[l]와 db[l]도 함께 출력합니다.
이들을 계산하기 위해서 전방향 함수때 저장해두었던 캐시를 쓰게 됩니다.

# 인트로
이전 비디오에서 심층 신경망을 구현하기 위한 기본 구성 요소를 보았습니다. 각 층에 대한 정방향 전파 단계와 대응하는 역전파의 단계를 보았습니다.    
실제로 이 단계들을 어떻게 구현할지 알아봅시다.

# 정방향전파
### 입력과 출력 그리고 수식
![image](https://user-images.githubusercontent.com/50114210/64341309-09e5d200-d023-11e9-9435-2fece93d1ca6.png)    
입력 a[l-1]과 출력 a[l], 캐시된 z[l]이 무엇을 하는지 기억해보세요.    
구현의 관점에서 보면 W[l]과 b[l]을 캐시하는 것입니다. 또 프로그래밍 예제에서 함수 호출을 더 쉽게 만들어 줍니다.     
![image](https://user-images.githubusercontent.com/50114210/64341459-5a5d2f80-d023-11e9-851c-c6f08622e533.png)   
네 번째 함수를 구현하는 방법은 z[l] = W[l] * a[l-1] + b[l] 그리고 a[l]은 z[l]에 활성화 함수를 적용시킨 것과 같습니다.   
이 등식은 이미 익숙해야합니다.  
### 벡터화 구현
![image](https://user-images.githubusercontent.com/50114210/64341487-6943e200-d023-11e9-8315-64f1eafea568.png)   
만약 벡터화 구현을 원한다면 W[l] * A[l-1] + b[l]이런 식으로 해주면 됩니다.    
그리고 A[l]은 z[l]에 적용된 g[l]입니다.    
### 그림으로 나타내보기 
![image](https://user-images.githubusercontent.com/50114210/64341637-bfb12080-d023-11e9-8edd-dc0917a9fac9.png)    
네 번째 단계에 대한 다이어그램을 기억하시나요 정방향 박스의 체인을 말합니다.
A[0]을 넣어서 초기화시켰습니다. X와 같은 값입니다. 첫 번째 것에 대한 입력값은 무엇일까요?    
a[0]은 한 번에 하나씩 할 경우의 학습 데이터에 대한 입력 특성이고 A[0]은 전체 학습 세트를 진행할 때의 입력 특성입니다.    
따라서 체인에서 첫 번째 정방향 함수에 대한 입력값입니다.    
그리고 이 과정을 반복하는 것은 왼쪽에서 오른쪽으로 가는 정방향 전파를 계산할 수 있게 합니다.

# 역방향전파
![image](https://user-images.githubusercontent.com/50114210/64342080-bb393780-d024-11e9-8f56-009d04909ebd.png)    
여기 입력 da[l]과 출력 da[l-1], dW[l], db[l]이 있습니다.   
이것을 계산하기 위한 단계를 작성하도록 하겠습니다.    
![image](https://user-images.githubusercontent.com/50114210/64342103-c5f3cc80-d024-11e9-9369-39e97876edaf.png)    
dz[l]은 da[a]에 g'[l]을 z[l]에 적용시킨 것에 요소별 곱셈을 한 값과 같습니다. 그리고 도함수 dW[l]은 dz[l] * a[l-1]과 같습니다.    
명시적으로 캐시에 저장하지 않았지만 이것 또한 해줘야합니다. 그럼 db[l]은 dz[l]과 같습니다.     
마지막으로 da[l-1]은 W[l]의 전치에 dz[l]을 곱한 것과 같습니다.   
이것의 구체적인 미분 과정을 같이 하지는 않겠습니다. 그러나 여기 da의 정의를 이 곳에 적용시키면 dz[l]을 계산하는 식을 얻게 됩니다.     
dz[l]은 (W[l+1]^T * dz[l+1]) * (g'[l] * z[l])과 같습니다. 복잡한 방정식이라는 것을 압니다.    
지난 주에 했던 역전파에 적었던 식과 같은지 이중으로 확인해보세요 시믕 신경망에서 하나의 은닉층만 사용한 경우에요.    
그리고 이번에는 여기가 요소별 곱셈인 것에 주의하세요. 따라서 역함수를 구현하는데 필요한 것은 이 네가지 식입니다.

# 역방향 전파의 벡터화
![image](https://user-images.githubusercontent.com/50114210/64342339-41ee1480-d025-11e9-8b9b-7223d865cb8a.png)    
첫 번째 줄은 dz[l]은 dA[l]과 g'[l] * z[l]을 요소별 곱셈한 값과 같습니다. 별로 놀랍지 않으실 겁니다.    
dW[l]은 1 / m * dz[l] * A[l-1]^T와 같습니다.
그리고 db[l]은 1 / m * np.sum(dz[l], axis=1, keepdims=True)입니다.    
마지막으로 dA[l-1]은 W[l]^T * dz[l]과 같습니다.    
따라서 여기 da[l]의 값을 입력하고 출력 dW[l], db[l]과 필요한 도함수, 그리고 da[l-1]의 값을 계산할 수 있도록 합니다.    

# 정리
![image](https://user-images.githubusercontent.com/50114210/64342772-30f1d300-d026-11e9-906d-ee606f0e7f4c.png)   
입력 X를 받고 ReLU를 활성화 함수로 갖는 첫 번째 층이 있고 두 번째 층에는 또 다른 ReLU활성화 함수가 있습니다.   
세 번째 층에는 시그모이드 활성화함수르 사용할 수도 있습니다. 바이너리 분류를 하는 경우에 말입니다.   
이 출력은 y의 예측값이 되고 이를 통해 손실을 계산할 수 있습니다.    
그럼 역으로 반복하는 과정을 시작할 수 있습니다.    
여기 도함수를 계산하는 역전파가 있습니다.
dW[3], db[3]을 계산하고, dW[2], db[2]를 계산하고, dW[1], db[1]을 계삽합니다.   
그리고 이것을 계산하는 과정에서 캐시에서 z[1], z[2], z[3]을 옮깁니다.    
그리고 dz[2]를 뒤로 전달하고 da[1]을 전달합니다.    
따라서 이것이 3개의 층을 가진 심층 신경망에서 정방향 전파와 역전파를 구현하는 방법입니다.    
# 역방향반복의 초기값
![image](https://user-images.githubusercontent.com/50114210/64342808-3e0ec200-d026-11e9-96ba-bb57414c1f26.png)   
제가 마지막으로 세부사항 하나를 말씀드리겠습니다.   
정방향 반복은 입력 데이터 X로 초기화합니다.   
역방향 반복은 어떨까요? da[l]입니다.    
로지스틱 회귀에서 바이너리 분류를 할 때 이 값은 -y / a + (1 - y)/ (1 - a)와 같습니다.    
따라서 출력 즉, y의 예측값에 대응되는 손실함수의 도함수는 이것과 같다고 보여질 수 있습니다.   

# 아웃트로
이 수식들은 머신러닝에서 가장 어려운 미적분 중에 하나입니다. 어려워도 계속 도전해보세요.   
이 과정이 다소 추상적이고 어렵게 느껴진다면 프로그래밍 과제를 하고 난뒤에는 개념이 더 견고해질 것입니다.   
