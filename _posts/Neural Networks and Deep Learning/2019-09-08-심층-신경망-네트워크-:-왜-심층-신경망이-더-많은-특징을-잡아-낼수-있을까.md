---
layout: post
title: "심층 신경망 네트워크 : 왜 심층 신경망이 더 많은 특징을 잡아 낼수 있을까요?"
tags: [Deep Neural Network]
categoires: [Neural Networks and Deep Learning]
---

# 학습 목표
왜 깊은 신경망이 더 잘 작동하는지 직관적인 이해를 얻을 수 있다.

# 핵심 키워드
심층 신경망(Deep Neural Network)

# 학습 내용
* 직관 1 : 네트워크가 더 깊어 질 수록, 더 많은 특지을 잡아낼 수가 있습니다. 낮은 층에서는 간단한 특징을 찾아내고, 깊은 층에서는 탐지된 간단한 것들을 함게 모아 복잡한 특징을 찾아낼 수 있습니다
* 직관 2 : 순환 이론에서 따르면, 상대적으로 은닉층의 개수가 작지만 깊은 심층 신경망에서 계산할 수 있는 함수가 있습니다. 그러나 얕은 네트워크로 같은 함수를 계산하려고 하면, 즉 충분한 은닉층이 없다면 기하급수적으로 많은 은닉 유닛이 계산에 필요하게 될 것입니다.
* 순환이론 : 로직 게이트의 서로 다른 게이트에서 어떤 종류의 함수를 계산할 수 있을지에 관한 것입니다.

# 인트로
심층 신경망이 많은 문제를 해결하는데 도움이 된다고 알고 있습니다.    
단지 규모가 큰 신경망이 필요한 것이 아니고, 깊거나 많은 은닉층을 가지고 있어야합니다.    
몇 가지 예제를 통해 깊은 신경망이 잘 작동하는 이유에 대한 직관을 얻어봅시다.    
# 직관 1, 이미지 인식
![image](https://user-images.githubusercontent.com/50114210/64487927-07ca8000-d27c-11e9-984d-e1039f334a82.png)     
심층망 계산이란 무엇일까요? 얼굴 인식이나 감지 같은 시스템을 구축할 때 심층 신경망이 할 수 있는 일들입니다.    
얼굴 사진을 입력값으로 넣으면 심층 신경망의 첫 번째 층은 특성 탐지기나 모서리 탐자기가 될 수 있을 것입니다.    
이 예시에서 약 20개의 은닉층이 이 이미지를 어떻게 계산하는지 살펴보겠습니다.    
20개의 은닉층은 이 작은 네모 상자로 시각화되었습니다.   
### 앞단의 레이어
![image](https://user-images.githubusercontent.com/50114210/64488000-11a0b300-d27d-11e9-820b-8bdc0250f1c2.png)    
예를 들어 맨 왼쪽 이미지들의 작은 칸은 모서리를 알아내는 은닉 유닛을 나타냅니다.    
유닛들은 이미지에서 가로, 세로, 대각선 방향의 모서리가 어디있는지 알아냅니다.     
간단하게 말하면 신경망의 첫 번째 층에서 하는 것은 사진을 보고 모서리가 어디에 있는지 파악하는 것입니다.    
이제 모서리를 형성하기 위해 픽셀을 그룹화함으로써 사진에서 모서리가 어디에 있는지 알아봅시다.    
그럼 감지된 모서리와 그룹화된 모서리를 받아서 얼굴의 일부를 형성할 수 있습니다.    
### 중간의 레이어
![image](https://user-images.githubusercontent.com/50114210/64488004-1c5b4800-d27d-11e9-9518-2c5f3d28ddf2.png)    
예를 들어 어떤 뉴런에서는 눈을 찾을 수 있고, 다른 뉴런에서는 코의 일부를 찾을 수 있습니다.     
따라서 많은 모서리를 한데 모아서 얼굴의 일부를 감지할 수 있습니다.     
### 뒷단의 레이어
![image](https://user-images.githubusercontent.com/50114210/64488008-267d4680-d27d-11e9-9024-25c5f4af7095.png)     
그리고 서로 다른 얼굴의 일부를 최종적으로 모아서 서로 다른 종류의 얼굴을 감지할 수 있게 됩니다.    
따라서 직관적으로 생각하면 신경망의 초기 층에서는 모서리와 같은 간단한 함수를 감지하게 되고,     
그 이후의 신경망의 층에서 이것들을 구성해서 더 복잡한 함수를 학습할 수 있도록 합니다.   
이 시각화의 기술적인 세부 사항을 하나 말씀드리면 모서리 탐지기는 이미지에서 상대적으로 작은 영역을 봅니다.   
얼굴 탐지기는 이미지의 더 넓은 영역을 볼 수 있게 됩니다.    
그러나 중요한 직관은 모서리처럼 간단한 것을 찾고 같이 모아서 더 복잡한 것을 찾고 그것을 또 모아서 더 복잡한 것을 찾는다는 것입니다.    
# 직관 2, 음성 인식
![image](https://user-images.githubusercontent.com/50114210/64488022-56c4e500-d27d-11e9-886e-62ddf53a1e2a.png)
이런 종류의 간단한 것에서 복잡한 것의 계층 표현 혹은 구성 표현은 얼굴 인식뿐만 아닌 다른 종류의 데이터에도 적용됩니다.    
예를 음성 인식 시스템을 구축하는 경우에 음성을 어떠게 시각화할 것인가로 들어보죠.    
음성을 입력으로 줄 때 신경망의 첫 번째 층은 낮은 단계의 음성 파형 특징을 탐지할 것입니다.    
예를 들어 톤이 올라가는지, 내려가는지, 백색 소음, 쉬쉬 소리, c, i, r같은 것을 말이죠.     
이런 식으로 낮은 단계의 파형특징을 추출할 수 있습니다.    
낮은 단계의 파형을 구성하는 것은 소리의 기본 단위 탐지를 학습하는 과정이라고 할 수 있습니다.     
소리의 기본 단위를 찾는 것을 학습하고 그것을 다같이 구성해서 음성의 단어를 인식하는데, 학습하기도 하고 단어를 구성해서 구나 문장을 인식하기도 합니다.     
따라서 여러 개의 은닉층을 갖는 심층 신경망은 낮은 단계에서는 간단한 특징을 학습하게 되고     
그 후 깊은 층에서는 탐지된 간단한 것들을 함께 모아 단어, 구, 무장, 같은 것들을 탐지하게 됩니다.     
이를 통해 음성 인식을 수행할 수 있게 됩니다.    
초기에 층에서 계산되는 것은 이미지에서 모서리를 찾는 것처럼 간단한 것으로 함수로 보여집니다.    
네트워크가 더 깊어질수록 놀라울만큼 복잡한 것들이 가능해집니다.     
# 심층 신경망과 뇌
어떤 사람들은 심층 신경망을 사람의 뇌에 비유하기도 합니다.    
신경 과학자가 말하듯이 사람의 뇌는 모서리같이 눈에 보이는 간단한 것부터 감지하고 이를 모아 더 복잡한 정보를 인식합니다.    
여러분이 보는 얼굴처럼요.     
심층 신경망을 사람의 뇌에 비유하는 것은 가끔 위험해보이긴 합니다.    
그러나 사람의 뇌가 작동하는 방식과 비슷한 것은 사실입니다.    
처음에는 간단한 것을 감지하고 그 정보를 모아 더 복잡한 물체를 인식할 수 있게 되니까요.    
이 또한 딥러닝에서 하나의 느슨한 영감으로 작용해왔습니다.    
# 직관 3
심층 신경망이 잘 작동하는 또 다른 직관은 다음과 같습니다. 이 결과는 회로 이론에서 나왔습니다.      
회로 이론은 로직게이트의 서로 다른 게이트에서 어떤 종류의 함수를 계산할 수 있을지에 관한 것입니다.    
상대적으로 작지만 깊은 심층 신경망에서 계산할 수 있는 함수가 있습니다.    
작다는 것은 은닉층의 개수가 상대적으로 작다는 것을 의미합니다.     
그러나 얕은 네트워크로 같은 함수를 계산하려고 하면 즉, 충분한 은닉층이 없다면 기하급수적으로 많은 은닉 유닛이 계산에 필요하게 될 것입니다.     
### 배타적 논리합 예시
![image](https://user-images.githubusercontent.com/50114210/64488110-b5d72980-d27e-11e9-9620-a5d39265ccab.png)     
하나의 예시를 드리죠.     
여러분이 모든 입력 특성에 대한 배타적 논리합을 계산한다고 해봅시다.    
이런 식으로 x1에서 xn까지요. XOR트리를 그려보면       
x1과 x2, x3, x4의 XOR를 계산하면 기술적으로 and, or, not게이트만 사용한다면 몇개의 층이 필요하게 될 것입니다.     
그러나 상대적으로 작은 회로에서 XOR을 계산할 수 있고 이 처럼 멋진 XOR트리를 만들 수 있게 됩니다.    
그리고 결국에는 출력으로 y에 해당하는 값을 얻게 됩니다.     
y의 예측값은 y와 같고 모든 입력 비트의 배타적 논리합합니다. 따라서 네트워크의 깊이를 계산하면 O(log n)이 됩니다.    
따라서 노드의 개수는 네트워크의 게이트 수와 같습니다. 네트워크가 크지 않게 때문에 그렇게 많은 게이트가 필요하지 않습니다.    
### 한 층을 가진 네트워크로 표현한다면?
![image](https://user-images.githubusercontent.com/50114210/64488140-39911600-d27f-11e9-889b-3248a2a13cf6.png)    
만약 하나의 층을 가진 네트워크로 XOR문제를 해겨랗려면 모든 x는 어떤 은닉 유닛으로 가게되고 y가 출력됩니다.    
그럼 XOR의 함수를 계산하기 위해 이 은닉층은 기하급수적으로 커져야합니다.     
왜냐하면 근본적으로 2^n의 가능한 구성을 모두 철저하게 열거해야 할 필요가 있기 때문입니다.    
1 또는 0으로 배타적 논리합이 나오는 입력 비트의 가능한 조합입니다.    
따라서 비트의 수에 따라 기하급수적으로 큰 은닉층이 필요해집니다.    
정확하게는 2^(N-1)의 은닉유닛이 필요할 것입니다.    
따라서 얕은 네트워크보다 깊은 네트워크에서 더 계산하기 쉬운 수학적인 함수가 있다는 것에 대한 감을 잡으셨기를 바랍니다.    
# 아웃트로
솔직히 말씀드리면 딥러닝이라는 용어는 마케팅적인 부분이 크다고 생각합니다.    
원래는 은닉층이 많은 신경망이라고 불리곤 했으나 '딥러닝'이라는 용어가 좋은 브랜드 효과를 주었죠.     
대중의 상상력을 자극시킨것입니다.    
그러나 홍보나 브랜딩과 별개로 딥러닝을 실제로 잘 됩니다.     
가끔 사람들은 너무 많은 은닉층을 사용하려고 합니다.    
그러나 저는 새로운 문제를 직면하면 로지스틱 회귀부터 시작하고 한 개 혹은 두개의 은닉층을 시도합니다.    
그것을 하이퍼파라미터로 사용하죠. 그들을 조절해 신경망의 알맞은 깊이를 찾습니다.    
그러나 지난 몇 년간 일부 애플리케이션에서 아주 깊은 신경망을 만드는 경향이 있었습니다.    
어떤 경우에는 수 십개의 층이 있기도 합니다. 물론 특정 문제에 대핸 최고 모델이 될 수 있겠죠.    



