---
layout: post
title: "심층 신경망 네트워크 : 행렬의 차원을 알맞게 만들기"
tags: [Matrix, Dimensions]
categories: [Neural Networks and Deep Learning]
---

# 학습 목표
행렬의 차원을 확인하는 방법을 터득한다.

# 핵심 키워드
* 행렬(Matrix)
* 차원(Dimensions)

# 학습 내용
수식을 이용해 W와 b의 벡터가 올바른 차원을 가지는지 확인하고,      
역전파를 구현하는 경우에도 dW[l]의 차원은 W[l]의 차원과 같은지 확인하는 연습을 합시다.

# 인트로
심층 신경망을 구현할 때 제가 코드의 정확성을 확인하기 위해 자주 사용하는 디버깅 툴은   
종이를 한 장 놓고 제가 작업하는 행렬의 차원을 확인하는 것입니다.    

# 단일 학습 데이터로 정방향 전파를 할 때의 차원
![image](https://user-images.githubusercontent.com/50114210/64487173-f0869500-d271-11e9-91c2-b237c68f8434.png)    
L = 5입니다. 입력층은 제외하고 5개의 층이 있습니다. 4개의 은닉층과 1개의 출력층입니다.   
### 1층
![image](https://user-images.githubusercontent.com/50114210/64487215-89b5ab80-d272-11e9-9848-d56bbc6f7318.png)     
정방향 전파를 구현하는 첫 번째 단계는 z[1]은 W[1] * x + b[1]입니다.    
지금 편향 값 b[1]은 무시합시다. 그리고 매개변수 W에 집중합시다.    
이 첫번째 은닉층은 3개의 히든 유닛이 있습니다. 여기 층 0, 1, 2, 3, 4, 5가 있습니다.    
이전 영상의 표기법을 사용하면 n[1], 즉 1층의 히든 유닛의 개수는 3과같습니다.    
여기서 n[2]는 5이고 n[3]은 4이고, n[4]는 2, n[5]는 1입니다.    
그리고 마지막으로 입력층에 대해서 n[0]은 n아래 첨자 x와 같고 이 값은 2입니다.    
이제 z, W, x의 차원을 생각해봅시다.   
z는 이 첫 번째 은닉층에 대한 활성화 벡터입니다. 따라서 z는 (3, 1)행렬입니다. 3차원 벡터입니다.    
따라서 (n[1], 1)차원 벡터로 적겠습니다. 이 경우에는 (3, 1)행렬입니다.    
입력특성 x는 어떨까요? x에는 두 가지 입력특성이 있어서 이 경우에는 (2, 1)행렬입니다. 더 일반적으로 (n[0], 1)입니다.
### Weight의 차원
![image](https://user-images.githubusercontent.com/50114210/64487270-3859ec00-d273-11e9-9042-592ff3f1c3b3.png)     
행렬 W[1]은 (n[0], 1)벡터를 곱했을 때 (n[1], 1)벡터가 나오도록하는 값이어야 합니다.    
따라서 이런 식의 3차원 벡터는 무언가에 2차원 벡터를 곱한 것과 같습니다.    
행렬 곱의 규칙에 따라 이것은 (3, 2)행렬이 되어야 합니다.    
왜냐면 (3, 2) 행렬 곱하기 (2, 1)행렬 혹은 벡터는 (3, 1)벡터가 되기 때문입니다.    
그리고 더 일반적으로 이것은 (n[1], n[0])차원의 행렬이 될 것입니다.    
따라서 W[1]의 차원은 (n[1], n[0])입니다.    
그리고 더 일반적으로 W[l]의 차원은 (n[l], n[l-1])입니다.    
예를 들어 W[2]의 차원은 (n[2], n[1])입니다. (5, 3)
### 2층
![image](https://user-images.githubusercontent.com/50114210/64487284-71925c00-d273-11e9-8aa9-4b1495b5b50b.png)    
왜냐하면 z[2]를 계산할 것인데, n[2]는 5이고 n[1]은 3이기 때문에, W[2] * a[1]은 (5, 3)행렬이 됩니다.    
### 나머지 층
![image](https://user-images.githubusercontent.com/50114210/64487311-c930c780-d273-11e9-9128-42dda684f4e6.png)     
비슷하게 W[3]은 해당 층의 차원과 그 전층의 차원이므로 (4, 5)가 됩니다.    
W[4]는 (2, 4)가 됩니다.    
W[5]는 (1, 2)가 됩니다.   
따라서 W의 일반적인 수식을 보면 해당 행렬의 차원은 (n[l], n[l-1])이 됩니다.
### bias의 차원
![image](https://user-images.githubusercontent.com/50114210/64487336-23ca2380-d274-11e9-93f9-e81d1fd2186d.png)    
z[1]의 차원은 (3, 1)인데 b는 z에 더하는 행렬이기 때문에 (3, 1)행렬이 될 것입니다.     
z[2]는 (5, 1)이므로 b[2]도 (5, 1)가 됩니다.     
따라서 더 일반적인 규칙은 b[l]은 (n[l], 1) 차원의 벡터가 되어야합니다.     
# 단일 학습 데이터로 역방향 전파를 할 때의 차원
dW[l]의 차원은 W[l]의 차원과 같아야합니다.   
![image](https://user-images.githubusercontent.com/50114210/64487410-43ae1700-d275-11e9-9a5a-497102aaa647.png)     
값을 업데이트 해줄 때에 'W := W - 러닝레이트 * dW'의 식을 사용하기 때문이죠.    
마찬가지로 db[l]의 차원은 b[l]과 같아야합니다.    
![image](https://user-images.githubusercontent.com/50114210/64487402-25481b80-d275-11e9-8b5b-64a515717645.png)    
차원 확인이 필요한 또 다른 중요한 값들은 z, x, a[l]입니다.    
z[l]은 g(a[l])과 같기 때문에 a는 z와 같은 차원을 가집니다.
# 벡터화된 정방향 전파의 차원
벡터화된 구현에서도 당연히 W와 dW, b와 db의 차원은 같습니다.    
그러나 z, a, x, b의 차원은 조금 달라집니다.    
단일 데이터에서는 z[1]을 W[1] * x + b[1]과 같다고 했습니다. 
![image](https://user-images.githubusercontent.com/50114210/64487446-d64eb600-d275-11e9-957c-8ae2260afd5a.png)     
z[1]은 (n[1], 1)행렬이고 W[1]은 (n[1], n[0])행렬이고 x는 (n[0], 1)그리고 b은 (n[1], 1)입니다.    
![image](https://user-images.githubusercontent.com/50114210/64487451-e6669580-d275-11e9-9fe2-fc98c6b88fcb.png)     
벡터화된 구현에서 Z[1]은 W[1] * X + b[1]입니다.     
Z[1]은 z[1][1], z[1][2], z[1][3], ... , z[1][m]의 데이터에서 값을 얻게 됩니다.    
얻은 값들을 열에 저장해서 Z[1]의 차원은 (n[1], 1)에서 (n[1], m)이 됩니다.    
W[1]의 차원은 그대로 (n[1], n[0])입니다.    
X는 (n[0], 1)이 아닌 모든 학습데이터가 수평으로 저장되어 (n[0], m)이 됩니다.    
W[1]과 X를 곱하면 Z[1]의 차원이 나온다는 것을 확인할 수 있습니다.    
마지막으로 b[1]은 여전히 (n[1], 1)입니다. 브로드캐스팅이 된다면 (n[1], m)이 되겠죠. 
# 벡터화된 역방향 전파의 차원
![image](https://user-images.githubusercontent.com/50114210/64487512-989e5d00-d276-11e9-92a2-5b2a5f11a999.png)     
W, b, dW, db의 차원도 구해봅시다.    
z[l]과 a[l]은 (n[l], 1)차원이었듯이 Z[l]과 A[l]의 차원은 (n[l], m)입니다.    
l = 0인 특별한 경우에서 즉, A[0]가 훈련 집합의 입력 특성 X와 같고 이 값은 (n[0], m)입니다.    
이것을 역전파에서 구현할 때는 dZ[l]와 dA[l]도 계산하게 됩니다.    
이들 역시 Z, A와 같은 차원을 가지게 됩니다.
# 아웃트로
여러 번의 연습을 통해 다양한 행렬의 차원을 명확히 하는데 익숙해졌습니다.    
심층 신경망의 역전파를 구현할 때 코드를 쭉 보면서 모든 행렬의 차원이 일치하는지 잘 확인하세요.    
나올만한 일부 버그를 삭제하는데 도움이 될 것입니다.    
이 강의의 예제들이 여러분이 작업할 다양한 행렬의 차원을 알아내는데 도움이 됐길 바랍니다.     
심층 신경망을 구현할 때 다양한 행렬과 벡터의 차원을 잘 알면 나올만한 일부 버그를 삭제하는데 도움이 될 것입니다.   
