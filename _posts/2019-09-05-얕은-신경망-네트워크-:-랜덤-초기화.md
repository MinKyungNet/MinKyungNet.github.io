---
layout: post
title: "얕은 신경망 네트워크 : 랜덤 초기화"
tags: [Random, Weight, Initialization]
categories: [Neural Networks and Deep Learning]
---

# 학습 목표
변수를 임의값으로 초기화하는 이유를 알 수 있다.

# 핵심 키워드
가중치 초기화(Weight Initialization)

# 학습 내용
* 신경망에서 w의 초기값을 0으로 설정한 후 경사 하강법을 적용할 경우 올바르게 작동하지 않습니다. dw를 계산했을 때 모든 층이 같은 값을 가지게 되기 때문입니다.

* 따라서 np.random.rand()를 이용해 0이 아닌 랜덤한 값을 부여해줘야 합니다.

# 인트로
신경망을 훈련시킬 때 변수를 임의값으로 초기화하는 것은 중요합니다.    
로지스틱 회귀의 경우 모두 0으로 초기화하여도 괜찮지만 신경망에서 모두 0으로 초기화하고 경사하강법을 적용할 경우 올바르게 동작하지 않을 것입니다.   
이제 그 이유를 살펴보겠습니다.

# 0으로 초기화

### 예시 모델
![image](https://user-images.githubusercontent.com/50114210/64323566-b44bfe00-cfff-11e9-8dc5-320d19c931b1.png)    
여기에서는 두 개의 입력 특성을 가지는데요. 따라서 n[0]은 2가 됩니다.    
그리고 두 개의 은닉층이 있고 n[1]은 2의 값을 가집니다. 이 은닉층에 관한 행렬 w[1]은 (2, 2)행렬이 될 것입니다.    
### 0으로 초기화
![image](https://user-images.githubusercontent.com/50114210/64323651-d80f4400-cfff-11e9-9073-46da253dbd79.png)    
이 값을 모두 0으로 초기화했다고 해보겠습니다. b[1]또한 0으로 초기화하겠습니다.   
b를 0으로 초기화하는 것은 실제로 괜찮지만 w는 그렇지 않습니다. 0으로 초기화할 시에는 문제가 될 수 있죠.     
여기서 발생하는 문제는 어떤 샘플의 경우에도 a[1][1]과 a[1][2]가 같은 값을 가진다는 것입니다.    
따라서 이 두개의 활성이 같은 것이 됩니다. 두 은닉 유닛 모두 같은 함수를 계산하기 때문이죠.   
### 역전파를 계산할 때도 발생하는 문제
![image](https://user-images.githubusercontent.com/50114210/64323690-ebbaaa80-cfff-11e9-964b-b0c7bc6353ae.png)   
그리고 역전파를 계산할 때 dz[1][1]과 dz[1][2]또한 같은 결과를 가지게 됩니다. 대칭적인 결과를 가지는 것이죠.     
두 은닉 유닛이 같은 값으로 초기화되기 때문에 가중치의 결과값이 항상 같다는 것이죠. w[2]는 [0, 0]이 될 것입니다.     
이 신경망을 이렇게 초기화하는 경우에 위의 유닛과 아래의 유닛은 완전히 같은 것이 됩니다.   
따라서 이것을 완전 대칭이라고 말할 수 있습니다. 완전히 같은 함수를 계산한다는 것이죠.
### 수학적 귀납법을 사용한 증명
수학적 귀납법을 사용하면 각 훈련의 반복마다 두 은닉 유닛은 항상 같은 함수를 계산한다는 것을 알 수 있습니다.    
dw의 행렬은 이런 형태이고 각 열이 모두 같은 값을 가진다고 합시다.    
이제 가중치를 계산하면 w[1] w[1] - 러닝레이트 * dw가 됩니다.    
그러면 각 반복 이후의 w[1]은 첫 번째 열이 두번째 열과 같아지게됩니다.    
따라서 수학적 귀납법에 의해 w의 값을 모두 0으로 초기화할 경우 두 은닉 유닛이 같은 함수를 계산하는 것으로 시작하기 때문에 두 은닉 유닛이 출력 유닛에 항상 같은 영향을 주게 되고, 첫 반복 이후에 같은 상태가 계속해서 반복됩니다.   
두 은닉 유닛은 계속 대칭적이므로 수학적 귀납에 의해서 두 번째 ,세 번째, ... 반복에 대해서 신경망이 얼마나 많은 훈련을 하는지에 상관없이 두 은닉 유닛은 항상 같은 함수를 가지는 것이죠.     
따라서 이 경우에 은닉 유닛이 실제로 한 개라고 할 수 있습니다. 항상 같은 것을 계산하기 때문이죠.    
### 큰 규모의 신경망에서도 동일하게 적용되는 초기화
![image](https://user-images.githubusercontent.com/50114210/64324203-dc882c80-d000-11e9-9043-06b775c501ed.png)    
또한 더 큰 규모를 가지는 신경망의 경우를 보면 세 개의 입력 특성이 있고 매우 많은 은닉 유닛이 있는 경우에도 비슷한 논리가 적용됩니다.    
모든 값을 0으로 초기화한다면 모든 은닉 유닛은 대칭이 되고 경사 하강법을 얼마나 적용시키는지에 상관 없이 모든 유닛은 항상 같은 함수를 계산하게 될 것입니다.    
따라서 은닉층의 많은 노드들이 쓸모 없게 되는 것이죠. 다른 함수를 계산하기 위한 각각 다른 유닛이 필요하기 때문입니다.    
이것의 해결 방법은 변수를 임의로 초기화하는 것입니다.

# 랜덤 

### 실제로 구현하는 방법
![image](https://user-images.githubusercontent.com/50114210/64325542-54efed00-d003-11e9-954f-8de56f77b8d8.png)     
w[1]을 np.random.randn으로 설정합니다. 이 함수는 가우시안 랜덤 변수를 생성합니다. (2, 2)행렬로 설정합니다.    
일반적으로 이 값에 0.01과 같은 굉장히 작은 수를 곱해줍니다. 따라서 굉장히 작은 임의의 수를 초기값으로 만들어낼 수 있습니다.    
우리가 다루고 있는 문제를 대칭회피 문제라고 부르는데 b[1]의 경우 대칭회피 문제를 가지지 않습니다. 따라서 b[1]을 0으로 초기화하는 것은 괜찮습니다. 
w가 이미 임의의 값으로 초기화되었기 때문에 다른 은닉 유닛에서 계산은 다른 결과를 만들어 낼 것이고 여기엔 더 이상 대칭 회피 문제가 존재하지 않기 때문이죠.     
비슷한 방법으로 w[2]도 임의의 값으로 초기화할 수 있습니다. b[2]도 마찬가지로 0으로 초기화하면 됩니다.
### 0.01은 어디에서 나온 값일까?
![image](https://user-images.githubusercontent.com/50114210/64325657-87014f00-d003-11e9-917a-a171cce2d3ed.png)   
그렇다면 0.01은 어떻게 나온 값일까요? 왜 100이나 10000같은 숫자를 사용하지 않는 것일까요?    
여기서는 가중치의 초기값을 매우 작은 값으로 정하는 것이 좋습니다.    
![image](https://user-images.githubusercontent.com/50114210/64325708-9da7a600-d003-11e9-8e79-1e2b1164465c.png)    
만약 tanh함수 또는 시그모이드 활성함수를 사용하거나 출력층에서 시그모이드 형태를 사용한다고 했을때 가중치가 너무 큰 값을 가지는 경우 활성값을 계산하면 z[1]은 w[1] * x + b[1]이고 활성홤수 a[1]은 z[1]에 대한 값이됩니다.      
w가 큰 값을 가지는 경우에 z도 굉장히 큰 값을 가지거나 몇몇 값이 굉장히 크거나 굉장히 작은 상태가 될 수 있습니다.    
tanh나 시그모이드 함수에서 이 두꺼운 부분은 경사의 기울기가 매우 낮기 때문에 경사 하강법 또한 매우 느리게 적용됩니다.    
따라서 학습 속도가 느려지게 되는 것이죠. 결론적으로 w가 너무 큰 값을 가지면 매우 큰 값의 z를 이용해 훈련을 시작하게 되고 tanh나 시그모이드 활성 함수가 너무 큰 값을 가지게 되므로학습의 속도가 느려진다는 것입니다.    
시그모이드 또는 tanh활성 함수를 신경망에 적용하지 않는 경우에는 이 문제는 적어지겠지만 이진 분류의 경우에는 출력 유닛이 시그모이드 함수이므로 이 때는 초기변수들이 너무 큰 값을 가지지 않도록 해야합니다.      
이것이 바로 0.01을 곱하는 이유입니다. 굳이 0.01이 아니여도 다른 작은 수를 곱해도 괜찮습니다.    
w[2]에 대해서도 똑같이 할 수 있죠 np.random.randn((1, 2)) * 0.01이 될 것입니다.
한 개의 은닉층을 가지는 신경망을 훈련시킬 때 적은 은닉층을 가진 상대적으로 얕은 신경망에서는 0.01을 곱하는 것도 괜찮지만    
매우 깊은 깊이의 신경망을 훈련시키는 경우에는 0.01과는 다른 수를 선택할 수도 있을 것입니다.
