---
layout: post
title: "왜 사람 수준의 성능을 봐야할까요?"
tags: [human level performance, bayes optimal error]
categories: [Structuring Machine Learning Projects]
---

# 학습 목표
머신러닝의 성능을 측정하는 다양한 방법을 알아본다.

# 핵심 키워드
* 사람 수준의 성능(human level performance)
* 베이지안 최적오차(bayes optimal error)

# 학습 내용
* 베이지안 최적 오차는 모델의 이론상 가능한 최저의 오차 값입니다. 과대적합이 되지 않는 이상 이 값을 뒤어 넘을 수는 없습니다.
* 많은 머신러닝 모델들이 사람 수준의 성능을 빠르게 뛰어 넘지만, 베이지안 최적 오차까지 줄이는 데는 시간이 많이 소요됩니다.
* 이유는 두가지 입니다.
  * 첫째, 사람 수준의 오차와 베이지안 최적오차간 차이가 크게 안나는 경우가 있습니다.
  * 둘째, 사람 수준의 성능이 나오지 않을 때 사용하는 성능 향상 기법을 쓸 수 없기 때문입니다.
  
# 인트로
지난 몇 년 동안 많은 머신러닝 연구팀들이 머신러닝 시스템과 사람 수준의 성능을 비교했습니다.           
왜 그랬을까요? 저는 크게 두 가지 이유가 있다고 생각합니다.       
첫째는 딥러닝의 발전으로 인해 머신러닝 알고리즘이 좋은 성능을 보였습니다.          
따라서 머신러닝 알고리즘을 다양한 분야에 적용할 수 있게 되었고       
사람 수준의 성능과 경쟁 가능한 수준에 이르렀죠.           
둘째는 사람이 할 수 있는 것에 대한 머신러닝 시스템을 디자인하고 만드는 것이 더 효율적이기 때문입니다.              
이러한 상황 하에서 사람 수준의 성능과 비교하거나 그에 도전하는 것이 자연스러워졌습니다.        
무슨 의미인지 몇 가지 예시를 살펴보죠.         

# 머신러닝의 정확도
![image](https://user-images.githubusercontent.com/50114210/68088582-fa103000-fea3-11e9-8bd7-c75d95cd147d.png)      
여러분께서 시간을 들여온 수많은 머신러닝 태스크들을 생각해봅시다.          
x축의 시간은 수개월 혹은 수년이 될 수도 있겠죠.         
어떤 팀이나 연구 커뮤니티가 문제를 풀기 위해 들이는 시간입니다.           
사람 성능에 근접해지는 과정은 상대적으로 진행 속도가 빠릅니다.      
하지만 알고리즘이 사람 성능을 뛰어넘은 이후 정확도의 측면에서 진행 속도는 느려집니다.       
다소 좋아지기는 하겠죠. 사람 성능을 뛰어넘은 후에도 점점 좋아집니다.          
다만 정확도가 얼마나 빨라지느냐를 뜻하는 기울기는 점점 낮아집니다.         

# 베이지안 최적 오차
목표는 이론적으로 최적의 성능에 도달하는 것이죠.           
더 큰 모델이나 많은 데이터를 이용해 시간에 걸쳐 알고리즘을 학습시키면서               
성능은 어떤 이론적 한계를 결코 뛰어넘지 못합니다.          
베이지안 최적 오차라고 부르는 값이죠.            
베이지안 최적 오차는 가능한 최저의 오차 값이라고 보면 됩니다.         
즉 어떤 x에서 y로의 함수도 이 정확도 값을 넘을 수 없는 것지요.            
예를 들어 음성 인식의 경우에 x가 소리 파일이라고 합시다.          
어떤 소리는 너무 잡음이 심해서 정확히 무슨 문장인지 알아낼 수 없을 겁니다.         
즉 최고의 정확도 값이 100%가 될 수 없죠.               
또는 고양이 인지의 경우에 어떤 이미지는 너무 흐려서       
그것이 고양이인지 아닌지 절대 구분하지 못 할 수도 있습니다.       
즉 최고의 정확도 값은 100%가 될 수 없습니다.           
베이지안 최적 오차 또는 베이지안 오차는 x에서 y로의 이론적으로 최적의 함수입니다.            
더 이상의 성능을 낼 수는 없죠.         
따라서 여러분이 문제를 풀기 위해 몇 년을 썼던지 간에      
베이지안 에러, 베이지안 최적 오차를 넘지 못하는 것은 전혀 놀랄 일이 아닙니다.          
그리고 사람 성능을 넘기기까지는 아주 빠른 속도로 정확도가 상승하죠.            
사람 성능을 넘어서면 느려지기 시작합니다.         

# 베이지안 최적 오차와 사람 성능은 대부분 차이가 크게 나지 않음 
![image](https://user-images.githubusercontent.com/50114210/68088602-24fa8400-fea4-11e9-99b2-98b6a8adb7ac.png)           
사람 성능을 넘어서면 진행 속도가 느려지는 이유로 크게 두 가지가 있다고 생각하는데요.          
우선 대부분의 태스크에서 사람 성능은 베이지안 최적 오차와 크게 차이나지 않습니다.           
사람들은 사진을 보고 고양이 여부를 구분하거나 소리를 듣고 글로 옮겨적는 과정에 익숙하니까요.             
즉 사람 성능을 뛰어넘은 뒤에는 더 성능시킬만한 여유가 없을 가능성이 높습니다.            

# 사람보다 성능이 낮을 때
다음으로 사람의 성능보다 여러분 알고리즘의 성능이 낮다면 사람 성능을 뛰어넘은 뒤보다            
더 쉽게 특정 방법을 이용해서 성능을 높일 수 있습니다.        
여기에 정리가 되어 있는데요. 사람이 잘 하는 태스크에 대해서 말이죠.           
예컨대 사진을 보고 물체를 인지하거나 소리를 듣거나 글을 읽는 것 말이에요.          
이런 태스크에 대해서 머신러닝 알고리즘이           

### 라벨링
![image](https://user-images.githubusercontent.com/50114210/68088609-2deb5580-fea4-11e9-8552-784057e817b2.png)      
사람보다 더 나쁜 성능을 보인다면 사람이 라벨링을 할 수 있습니다.          
사람들을 고용한 뒤 예시들을 라벨링해달라고 부탁하는 거죠.           
이렇게 학습 알고리즘에 더 많은 데이터를 학습시킬 수 있는 겁니다.          

### 수동 오차 분석
![image](https://user-images.githubusercontent.com/50114210/68088613-3cd20800-fea4-11e9-922f-668ec83bceb9.png)       
다음 주에 이야기해볼 것은 수동 오차 분석입니다.            
여러분의 알고리즘이 사람보다 더 나쁜 성능을 보일 때     
사람들에게 알고리즘이 틀린 예시를 봐달라고 부탁하는 겁니다.      
그렇게 사람은 왜 맞고 알고리즘은 왜 틀리는지 통찰을 얻는 것이죠.           
다음 주에 이 방법이 알고리즘의 성능을 어떻게 높이는지 살펴볼거고요.         

### 편향과 분산의 분석
![image](https://user-images.githubusercontent.com/50114210/68088617-452a4300-fea4-11e9-8e6e-c544852a915e.png)       
그리고 우리가 얘기했던 대로 편향과 분산에 대해 더 좋은 분석을 할 수 있습니다.            
즉 여러분의 알고리즘이 사람보다 못 할 때        
이런 도구들을 이용해서 알고리즘을 향상시킬 수 있는 것이죠.        
하지만 사람보다 더 나은 성능을 보이게 되면 이 세 도구들은 적용하기 어려워집니다.        
이것이 사람이 잘 하는 태스크에 대하여 사람 성능과 비교하는 것이           
왜 중요한지를 말해주기도 하겠군요.          

# 아웃트로
머신러닝 알고리즘이 사람이 잘하는 일을 따라잡고               
나아가 사람보다 더 나은 성능을 보일 수 있는 이유이기도 합니다.         
여러분이 편향과 분산에 대해 잘 알고 있더라도
사람이 그 태스크를 얼마나 잘 하는지를 아는 것도
편향과 분산을 얼마나 줄여야 하는지 이해하는데 큰 도움이 됩니다.          
