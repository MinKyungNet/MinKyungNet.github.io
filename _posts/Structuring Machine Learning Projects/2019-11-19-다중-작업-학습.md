---
layout: post
title: "다중 작업 학습"
tags: [multi-task learning, low-level feature]
categories: [Structuring Machine Learning Projects]
---

# 학습 목표
다중 작업 학습을 배운다.

<br>

# 핵심 키워드
* 다중 작업 학습(multi-task learning)
* 저레벨 특성(low-level feature)

<br>

# 학습 내용
* 다중 작업 학습은 하나의 신경망이 여러 작업을 동시에 할 수 있도록 학습하는 것입니다.
* 이미지 다중 분류 학습은 신경망 초기 특성들은 여러 물체에서 공유가 가능하기 때문에, 하나의 신경망을 학습시키는 것이 여러 신경망을 개별 학습시키는 것 보다 효율적입니다.
* 다중 작업 학습은 아래의 상황에서 많이 쓰입니다.
  - 여러 문제들의 하나의 저레벨 특성을 공유할 때
  - 데이터가 비슷할 때(항상 만족하는 것은 아닙니다.)
  - 거대한 작업 세트들을 하나의 큰 신경망으로 한번에 학습 시키려고 할 때
* 다중 작업 학습보다는 전이 학습이 더 많이 쓰이고 있습니다.

<br>

# 인트로
전이 학습에서는 순차적인 과정을 거치는데 작업 A로부터 학습하여 작업 B로 전이합니다.         
하지만 다중 작업 학습에서는 한 신경망이 여러 작업을 동시에 할 수 있도록 합니다.        
각 작업들이 다른 나머지 작업들에게 도움을 줍니다. 예시를 보도록 하겠습니다.        

<br>

# 다중 작업 학습 예시
![image](https://user-images.githubusercontent.com/50114210/69111082-ce09c700-0abf-11ea-9ab5-d527a994de88.png)         
자율 주행 자동차를 만드는 상황을 보겠습니다. 이 자율 주행 자동차는 다양한 것들을 감지해야 합니다.           
보행자, 다른 자동차, 정지 표지판, 그리고 신호등까지 감지해야 합니다.         
그리고 이외의 것들도 다른 것도 있겠죠. 예를 들어 이 이미지를 보면 왼쪽엔 정지 표지판이 있고             
자동차가 있지만 보행자나 신호등은 보이지 않습니다.         
이 사진을 입력 샘플 x^[i]라고 한다면 y^[i] 하나 레이블만을 가지는 것이 아니라 총 네 개의 레이블을 가지게 됩니다.           
이 예시에서는 보행자는 없고 자동차가 한 대, 정지 표지판 한 개, 그리고 신호등은 없죠.         
그리고 이외의 것들을 감지하려고 한다면 y^[i]는 더 큰 차원을 가지게 됩니다.         

<br>

# 훈련 세트 레이블의 차원
![image](https://user-images.githubusercontent.com/50114210/69111144-fabdde80-0abf-11ea-82b4-9f0ee1e7a8f7.png)
따라서 y^[i]은 (4, 1)차원의 벡터가 됩니다.         
전제척으로 훈련 세트 레이블을 보면 이전과 비슷하게 이렇게 훈련 세트 레이블을 가로로 쌓아 올립니다.        
y^[1]부터 y^[m]까지 입니다 그리고 각 y^[i]는 (4, 1)차원 벡터로 이루어집니다.        
따라서 이것들은 각각 열 벡터가 되죠 따라서 행렬 Y는 (4, m)차원의 행렬이 됩니다.      
이전에는 Y는 한 가지 숫자로만 이루어진 (1, m) 행렬이었죠.          

<br>

# 신경망의 모양
![image](https://user-images.githubusercontent.com/50114210/69111222-33f64e80-0ac0-11ea-8391-29b750a2902b.png)       
이제 Y의 값들을 예측할 수 있도록 신경망을 훈련시켜야 합니다.          
이 신경망은 입력 x를 가지고 4개의 값을 가지는 y를 출력으로 가집니다.        
여기 출력층을 보시면 4개의 노드를 가지는 것을 알 수 있죠.         
첫 번째 노드는 사진에 보행자가 있는지를 예측하고 두 번째 출력은 자동차가 존재하는지          
이것은 정지 표지판이 존재하는지 그리고 이것은 신호등이 존재하는지를 예측합니다.    
따라서 y의 예측값은 4차원의 값이 됩니다.          

<br>

# 손실 함수
![image](https://user-images.githubusercontent.com/50114210/69111402-d57da000-0ac0-11ea-938b-6b1c9b3cce87.png)       
이제 신경망을 훈련하기 위해서 신경망의 손실을 정의해야 합니다.           
(4, 1)차원을 가지는 y의 예측값이 주어졌을 때 손실값은 전체 훈련 세트에 대한 평균이 됩니다.         
i는 1부터 m까지, j는 1부터 4까지에 대해서 각 예측값의 손실에 대한 합을 m으로 나눈 값이 됩니다.          
보행자, 자동차, 정지 표지판, 신호등 이 네 가지 구성 요소를 더하는 것이죠.          
그리고 이 L 값은 일반적인 로지스틱 손실이 됩니다.        
이것을 적어보자면 -y_j^[i] log ŷ_j^[i]에 (1-y_j^[i]) log(1-ŷ_j^[i])를 뺀 것이 됩니다.        

<br>

# 이전의 소프트맥스 와의 차이점
![image](https://user-images.githubusercontent.com/50114210/69111450-f514c880-0ac0-11ea-91db-6b4c759a1c61.png)     
이전의 이진 분류와의 큰 차이점은 j가 1부터 4까지일 때의 값들을 모두 더한다는 것입니다.     
그리고 이것과 소프트맥스 회귀 사이의 차이점은 소프트맥스가 하나의 샘플에 하나의 레이블을 지정했다면                
이것은 **한 이미지가 여러 개의 레이블**을 가지기도 합니다.          
따라서 각각의 이미지가 각각 보행자, 자동차, 정지 표지판, 신호등만의 이미지가 아니라는 것이죠.         
각 이미지에서 보행자, 자동차,  정지 표지판, 신호등 같은           
여러 물체가 같은 이미지 안에서 나타날 수 있는지 확인하는 것입니다.      
실제로 이전 슬라이드의 이미지에서 자동차와 정지 표지판이 함께 있었죠. 하지만 보행자와 신호등은 없었습니다.       
따라서 한 이미지에 하나의 레이블만 지정하는 것이 아니고             
한 이미지에 각 라벨에 해당하는 물체가  존재하는지 보게 됩니다.           
제가 말씀드린 것이 한 이미지가 여러 개의 레이블을 가지는 경우입니다.         

<br>

# 성능을 높여주는 다중 작업 
**신경망을 훈련시킬 때 비용 함수를 최소화하기 위해서 다중 작업 학습을 이용할 수 있습니다.**       
여러분은 지금 이미지를 인식하는 하나의 신경망을 만들고 있고 네 개의 문제를 해결하고자 하고 있습니다.         
그리고 각 이미지가 네 개의 물체들을 포함하고 있죠.        
시도할 수 있는 다른 방법은 분리된 네 개의 신경망을 훈련하는 것입니다.       
네 가지 작업에 대해 한 신경망을 훈련하는 것 대신에 말이죠.       
**신경망의 초기 특성들은 여러 물체에 대해서 공유될 수 있기 때문에           
네 개의 물체에 대해 한 신경망을 훈련시키는 것이 더 좋은 성능을 보입니다.**            
네 작업에 대해 분리된 신경망을 훈련하는 것에 비해말이죠.        
따라서 이것이 다중 작업 학습의 강점입니다.             

<br>

# 몇 가지 물체에 대해서만 레이블이 있는 경우
![image](https://user-images.githubusercontent.com/50114210/69111596-5c327d00-0ac1-11ea-9d88-9d2d41ae1966.png)          
지금까지 이 알고리즘에서는 모든 이미지가 각각의 모든 레이블을 가지는 것처럼 설명했습니다.         
하지만 **다중 작업 학습은 몇몇 이미지가 몇 가지 물체에 대한 레이블만을 가질 때에도 적용됩니다.**         
첫 번째 학습 샘플에서 레이블이 보행자는 있고 자동차는 없다고 표시했을 때           
정지 표지판과 신호등에 대해서는 있는지 없는지 표시하지 않았습니다.       
두 번째 샘플에서는 보행자가 자동차가 있다고 표시하고               
이전과 마찬가지로 이미지를 보고 정지 표지판과 신호등의 유무는 표시하지 않았습니다.          
몇몇 샘플은 모든 레이블을 표시하기도 하고 또 다른 샘플들에서는 자동차의 유무만 표시할 수도 있습니다.             
따라서 다른 부분은 이렇게 물음표로 표시되죠.          
이러한 데이터 세트를 이용해서도 동시에 작업을 할 수 있는 학습 알고리즘을 훈련시킬 수 있습니다.          
몇몇 이미지가 레이블의 특정 부분만 표시하고 있다고 하더라도 말이죠.        
그리고 다른 부분은들 물음표로 표시되어 있더라도 괜찮습니다.        
몇몇 레이블이 물음표인 상태에서 이 알고리즘을 훈련시키는 방법은              
j가 1부터 4까지인 경우에 대해 합을 계산할 때 0이나 1의 값을 가지는 j값에 대해서만 덧셈을 하는 것입니다.            
따라서 여기 물음표가 있다면 덧셈 과정에서는 이 부분을 무시하고           
덧셈 가능한 값을 가진 부분에서만 덧셈을 계산하면 되는 것입니다.         
이렇게 이런 형태를 가지는 데이터 세트를 사용할 수 있습니다.             

<br>

# 다중 작업 학습의 세가지 조건
다중 작업 학습을 적용하기 위해서 일반적으로 세 가지 조건을 만족해야 합니다.           

<br>

# 낮은 수준의 특성을 공유
![image](https://user-images.githubusercontent.com/50114210/69111707-b7fd0600-0ac1-11ea-83ec-3a46e852494d.png)           
하나는 **낮은 수준의 특성을 공유할 때 이익이 되는 작업**들을 학습하는 경우입니다.          
자율 주행 예시의 경우에 신호등과 자동차, 보행자를 인식하는 것이             
정지 표지판을 인식하는데 도움이 되는 비슷한 특성을 가지고 있습니다.      
왜냐하면 이것들은 모두 도로에 관한 특성을 가지고 있기 때문입니다.         

<br>

# 각 작업이 가지는 데이터의 양이 비슷해야 함
![image](https://user-images.githubusercontent.com/50114210/69111759-dc58e280-0ac1-11ea-9616-1053268035ae.png)        
두 번째는 엄격한 규칙은 아니기 때문에 항상 만족하지 않는 경우도 있는데      
많은 성공적인 다중 작업 학습은 일반적으로 **각 작업이 가지는 데이터의 양이 상당히 비슷**합니다.      
전이 학습에서 배운 것을 다시 기억해보자면 작업 A에서 학습한 것을 작업 B로 전이시켰죠.           
작업 A에서 백만 개의 샘플이 있을 때 작업 B에서는 천 개의 샘플이 있었습니다.       
따라서 이 백만 개의 샘플로부터 학습한 모든 지식이 작업 B에서 훨씬 적은 데이터를 활용할 때 도움이 되었습니다.             
        
<br>

### 데이터 간의 지식을 공유
![image](https://user-images.githubusercontent.com/50114210/69111803-03171900-0ac2-11ea-8b63-d50df8c5a362.png)           
그렇다면 다중 작업 학습은 어떨까요? 다중 작업 학습에서는 보통 두 개보다 많은 작업을 가집니다. 
이전 예시에서 4개의 작업이 있었죠. 여기서는 백 개의 작업이 있다고 해보겠습니다.       
동시에 백 개의 유형을 가지는 물체를 인식하는 다중 작업 학습을 하면            
각 작업이 천 개의 샘플을 가질 때 어떤 점을 발견할 수 있을까요?     
100번째 작업 한 가지의 성능에만 초점을 맞춰보겠습니다. A_100이라고 해보겠습니다.           
이 마지막 작업만을 독립적으로 훈련시킨다면 단지 천 개의 샘플만을 가지고 이 작업을 훈련할 수 있습니다.         
하지만 나머지 99개의 작업의 훈련에서 총 99,000개의 샘플이 있기 때문에          
이것에 지식을 제공해서 크게 증가시킬 수 있습니다.        
작업 A_100이 가지는 상대적으로 적은 1000개에 비해서 말이죠.          
대칭적으로 다른 99개의 작업 또한 여기 100개의 **다른 작업들로부터 데이터 또는 지식들을 제공받습니다.**         
두 번째 규칙은 엄격하게 지켜져야 하는 규칙은 아닙니다.           
하지만 어떤 한 작업에만 초점을 맞추어 봤을 때 다중 작업에서 큰 효과를 얻기 위해서는            
다른 작업들이 한 작업에 비해서 훨씬 많은 데이터를 가져야 합니다.         
이것을 만족시키는 방법은 이 예시처럼 많은 작업을 가지고 각 작업이 비슷한 양의 데이터를 가지는 것입니다.           
가장 중요한 것은 이미 한 작업에 대해서 천 개의 샘플을 가지고 있고             
다른 작업에는 천 개의 샘플보다 훨씬 많은 데이터를 가지는 것입니다.        
마지막 작업에서 좋은 결과를 얻도록 다른 작업이 도움이 되기 위해서 말이죠.            

<br>

# 충분히 큰 신경망
![image](https://user-images.githubusercontent.com/50114210/69111934-5ab58480-0ac2-11ea-971d-9861145ac35b.png)            
마지막으로 다중 작업 학습을 잘 적용하기 위해서 모든 작업에 대해 충분히 큰 신경망을 훈련할 수 있어야 합니다.           
다중 작업 학습의 대안으로 각 신경망을 각 작업에 분리해서 훈련할 수 있습니다.         
보행자, 자동차, 정지 표지판, 신호등 감지에 대해 한 신경망을 훈련하는 것 대신             
보행자 감지에 한 신경망을 훈련하고 한 신경망은 자동차 감지에,               
그리고 다른 신경망은 정지 표지판 감지, 또 다른 신경망은 신호등 감지에 훈련하는 것이죠.            
Rich Caruana라는 이름의 한 연구자는 몇 년전에 다중 작업 학습이 분리된 신경망을                   
학습하는 것보다 성능이 낮은 경우는 신경망이 충분하게 크지 않은 경우인 것을 발견했습니다.        
충분히 큰 신경망에서 훈련하는 경우에는 다중 작업 학습은 확실히 낮은 성능을 보이는 것은 매우 드물다는 것입니다.          
실제로 각 작업을 독립적인 신경망에서 훈련하는 것보다 더 나은 성능을 가지는데 도움이 됩니다.            

<br>

# 아웃트로
여기까지가 다중 작업 학습에 대한 것들이었습니다.          
실제로는 다중 작업 학습은 전이 학습에 비해 훨씬 적게 사용됩니다.         
저는 전이 학습을 사용한 경우를 많이 봐왔는데                  
적은 데이터를 가지는 문제가 있다고 할 때 많은 데이터를 가지는 문제에 연관시켜서          
학습한 것들을 새로운 문제로 전이시키는 경우입니다.         
하지만 다중 작업 학습은 더 드물게 나타나는데
사용하고자 하는 거대한 작업 세트가 있을 때 이 모든 작업들을 동시에 훈련하고자 할 때 사용합니다.            
한 예시로 컴퓨터 비전의 경우 물체를 감지할 때 다중 작업 학습을 사용한 경우를 많이 보았습니다.            
하나의 신경망이 동시에 많은 물체들을 감지하는 것이        
다른 신경망이 각 물체를 감지하는 것보다 더 잘 작동하는 것이죠.       
평균적으로는 다중 작업 학습에 비해 전이 학습이 많이 사용된다고 할 수 있습니다.           
하지만 두 방법 모두 여러분이 사용할 수 있는 유용한 방법입니다.           
요약하자면 다중 작업 학습은 한 신경망으로 많은 작업들을 훈련할 수 있게 해줍니다.          
그리고 이것은 독립적으로 작업을 수행하는 것보다 나은 성능을 보여줍니다.         
한 가지 알아두셔야 할 것은 실제로는 전이 학습이 다중 작업 학습보다 훨씬 많이 사용된다는 것입니다.        
머신 러닝 문제를 해결하고자 하는데 상대적으로 적은 데이터 세트를 가지고 있다면 전이 학습이 도움이 될 수 있습니다.          
훨씬 큰 데이터 세트를 가지는 연관된 문제를 찾는다면 말이죠.        
거기서 신경망을 훈련시킨 후에 이것을 적은 데이터를 가진 문제로 전이시키면 됩니다.        
따라서 전이 학습은 오늘날 굉장히 많이 사용되고 있습니다.       
또한 다중 작업 학습을 사용하는 응용 프로그램도 있는데             
다중 작업 학습은 전이 학습에 비해 훨씬 적게 사용됩니다.            
컴퓨터 비전에서 물체 인식의 경우는 예외인데요.         
수 많은 다른 물체들을 감지하기 위해 신경망을 훈련하는 경우입니다.               
이것이 신경망을 분리해서 훈련하는 것보다 나은 결과를 보입니다.          
하지만 다중 작업 학습과 전이 학습이 비슷하게 사용되는 것처럼 보여도          
실제로는 다중 작업 학습보다 전이 학습이 많이 사용됩니다.         
아마 수 많은 다른 작업들을 한 신경망에서 훈련할 수 있도록 설정하는 것이 어렵기 때문일 것으로 생각합니다.           
그리고 컴퓨터 비전의 물체 인식 예시는 가장 알기 쉬운 예외이죠.         
