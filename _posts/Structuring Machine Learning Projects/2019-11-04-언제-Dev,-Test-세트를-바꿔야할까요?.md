---
layout: post
title: "언제 Dev/Test 세트를 바꿔야할까요?"
tags: [evaluation metric, dev set, test set]
categories: [Structuring Machine Learning Projects]
---

# 학습 목표
원하는 결과물을 얻지 못했을 때 대처하는 방법을 배운다.

# 핵심 키워드
* 평가 척도(evaluation metric)
* 개발 세트(dev set)
* 시험 세트(test set)

# 학습 내용
* 머신러닝은 크게 두 단계로 나뉘어져 있습니다.
  1. 모델을 평가할 척도를 설정합니다.
  2. 해당 척도를 기준으로 좋은 성능을 이끌어 내는 것입니다.

* 모델의 평가 척도가 좋더라도 실제 어플리케이션에서 좋은 성과를 얻지 못하거나 원하는 결과물을 얻지 못하면 두가지를 살펴 볼 수 있습니다.
* 첫째, 평가 척도를 바꾸는 것입니다.
- 원하지 않는 결과에 가중치를 크게 줘서 손실값을 나쁘게 만듭니다.
- 기존:          

![image](https://user-images.githubusercontent.com/50114210/68087822-6c7d1200-fe9c-11e9-9f6f-ce19712eaf85.png)          

- 가중치 부여후:        
    
![image](https://user-images.githubusercontent.com/50114210/68087833-7bfc5b00-fe9c-11e9-83d2-eafec06fd97e.png)      
  * 둘째, 개발 및 시험 세트를 실제 앱에서 사용되는 데이터로 바꾸는 것입니다.

# 인트로
개발 세트와 평가 척도를 정하는 것이     
팀이 겨눠야 할 과녁을 정하는 것과 비슷하다는 것을 보았습니다.          
하지만 프로젝트 도중에 과녁의 위치가 잘못되었다는 것을 깨달을 수도 있죠.            
그런 경우에는 과녁을 옮기셔야 합니다. 예시를 한번 살펴볼까요?

# 고양이 분류기
![image](https://user-images.githubusercontent.com/50114210/68088101-ffb74700-fe9e-11e9-9a73-4b03171a9df0.png)         
여러분이 고양이를 사랑하는 사람들을 위해       
여러 고양이 사진을 찾을 수 있도록 고양이 분류기를 만든다고 합시다.         
그리고 척도로는 분류 오차를 사용하기로 했습니다.      
알고리즘 A와 B의 오차는 각각 3%, 5%입니다. 알고리즘 A가 더 나아보이는군요.         
그런데 여러분이 이 알고리즘을 쓰던 중에
모종의 이유로 알고리즘 A가 야한 사진을 분류했다고 칩시다.          
만약 여러분이 알고리즘 A를 최종적으로 선보였다면 사용자들은 더 많은 고양이 사진을 볼 수 있겠죠.           
고양이를 구별하는 데 오차가 3%니까요.       

# 문제
하지만 사용자들에게 야한 사진도 보여줍니다.        
여러분 회사의 입장에서나 사용자의 입장에서 절대 쓸 수 없는 알고리즘인 거죠.       
반대로 5%의 오차를 보이는 알고리즘 B는     
조금 더 많은 사진을 잘못 분류하지만 야한 사진을 분류하지는 않습니다.         
여러분의 회사 입장에서나 사용자가 받아들일 수 있는지의 측면에서 알고리즘 B가 훨씬 낫습니다.          
왜냐하면 야한 사진을 용인하지 않으니까요.          
이 문제에서 A는 평가 척도가 3%로써 좋은 성능을 보이고 있습니다.        
하지만 더 나쁜 알고리즘이 된 상황인거죠.       
이런 경우 평가 척도와 개발 세트는 척도인 오차가 더 낮은 알고리즘 A를 선호합니다.        
그러나 여러분 또는 사용자의 입장에서는 야한 사진을 용인하지 않는 알고리즘 B를 선호하겠죠.       
이런 경우 평가 척도가 매기는 순위는 더 이상 쓸모 없습니다.      
알고리즘 간의 순위가 알고리즘 A가 낫다고 잘못 예측된 거죠.         
평가 척도를 바꿔야 한다는 신호입니다.       
개발 세트나 시험 세트를 바꿔야할 수도 있구요.       

# 기존의 분류 오차
![image](https://user-images.githubusercontent.com/50114210/68088109-15c50780-fe9f-11e9-8ea7-0018185e42cb.png)       
여기서 사용한 분류 오차 척도는 이렇게 적을 수 있습니다.          
1/(개발 세트의 예시 숫자, m_dev)에 i=1부터 m_dev까지     
개발 세트의 i번째 예시의 예측이 i번째 실제 라벨과 일치하는지를 나타내는        
인디케이터를 합한 것을 곱하면 됩니다.       
여기서 예측값을 나타내기 위해 이런 표현을 사용한 적이 없죠?         
이 값은 0 또는 1이 될테고요.       
이 인티케이터 함수는 안쪽에 있는 식이 참이 되는 예시의 개수를 셉니다.       
즉 잘못 분류된 예시의 숫자를 세는 거죠.      
이 평가 척도의 문제는 야한 사진과 야하지 않은 사진을 동등하게 본다는 겁니다.           
하지만 여러분은 야한 사진에 라벨이 제대로 붙여지길 원하죠.         
만약 야한 사진을 고양이 사진으로 인식해서 아무런 의심없는 사용자에게 보여준다면
영문도 모른채 야한 사진을 본 사용자는 매우 불쾌하게 생각할 겁니다.         

# 가중치를 부여한 분류 오차
이 평가 척도를 바꾸는 방법 중 하나는 여기에 w^(i)라는 가중항을 추가하는 거죠.        
w^(i)는 x^(i)가 야한 사진이 아닌 경우 1이고        
x^(i)가 야한 사진일 경우 10이나 100처럼 큰 숫자가 됩니다.           
이로써 예시가 야한 사진일 경우 더 큰 가중치를 주는 거죠.          
따라서 알고리즘이 고양이를 잘못 분류했을 때보다              
야한 사진을 잘못 분류했을 때 오차는 훨씬 커집니다.           
이 예시에서는 야한 사진을 올바로 분류하는 데 열 배의 가중치를 줬습니다.           
그리고 이 정규화 항을 고치자면 w^(i)의 합이 되겠죠.          
그러면 오차는 여전히 0과 1 사이의 값이 될 겁니다.          
이 가중치의 값은 중요하지 않습니다.          
실제로 이 가중치를 구현하기 위해서는 개발, 시험 세트 안에 있는 야한 사진을 살펴봐야 합니다.          
그리고 나서 가중치 함수를 구현할 수 있죠.         
하지만 더욱 중요한 것은 이렇게 평가 척도를 찾았다 하더라도        
무엇이 더 나은 알고리즘인지 순위를 알려주지 못 할 때는 어떻게 해야할까요?           

# 새로운 평가 척도
그러면 새로운 평가 척도를 고민해봐야 합니다.           
새로운 평가 척도를 정의하는 것이 한 가지 가능한 방법인 것이죠.         
평가 척도의 목표는 분류기 두 개에 대해서            
어플리케이션에 무엇이 더 적합한지를 정확히 알려주는 것입니다.          
이 영상의 목표를 생각해보면 어떻게 새로운 척도를 정의할지는           
너무 고민하지 않으셔도 됩니다.          
중요한 것은 여러분의 평가 척도가 마음에 들지 않을 때
그 평가 척도를 계속 붙들고 있지 말라는 겁니다.              
대신 여러분의 선호에 잘 들어맞는 새로운 척도를 정의하세요.       
여기서 선호란 무엇이 더 나은 알고리즘이냐를 말하겠죠.           
주목할 점은 지금까지 우리가 분류기를 평가하기 위한 척도를          
어떻게 정의할 것인가를 두고 논의했다는 겁니다.          
야한 사진 등 다양한 수준에서의 성능을 고려해서           
모든 분류기의 순위를 매길 수 있도록 평가 척도를 정의했다는 거죠.        
이것은 직교화의 예시이기도 합니다.       
머신러닝 문제를 받아서 서로 다른 단계로 잘게 쪼개야 합니다.             
하나의 버튼, 즉 하나의 단계는 척도를 어떻게 정의할까 하는 것입니다.            
여러분이 뭘 원하는지가 중요한 단계죠.         
그리고 각 척도에서 좋은 성능을 내기 위한 방법을 따로 고민할 겁니다.         

# 머신러닝의 첫 단계 : 과녁 놓기
![image](https://user-images.githubusercontent.com/50114210/68088123-33926c80-fe9f-11e9-9cbe-315248ee12af.png)      
제 생각에는 머신러닝이 크게 두 단계로 나눠져 있다고 생각합니다.         
과녁 비유를 빌리자면 첫 단계는 과녁을 놓는 겁니다.        
목표가 어딘지 정의하는 것입니다. 완전히 독립적인 단계죠.          
과녁을 어디에 둘지 조정할 수 있는 하나의 버튼인 셈이죠.          
그리고 또 완전히 분리된 문제, 즉 버튼으로
좋은 성능을 내거나 과녁을 잘 조준하도록 조정할 수 있습니다.         
또는 과녁을 향해 어떻게 쏠지도 해당하겠죠.          
즉 척도를 정의하는 게 첫 단계이고         

# 머신러닝의 두번째 단계 : 과녁 맞추기
![image](https://user-images.githubusercontent.com/50114210/68088133-558bef00-fe9f-11e9-89b1-18ea7d82ae27.png)      
나머지는 두 번째 단계입니다.            
과녁을 맞춘다고 했을 때 여러분의 학습 알고리즘은          
이렇게 생긴 비용 함수를 최적화할 겁니다.          
학습 세트에 대해서 이런 손실을 최소화하는 거죠.           
또 가능한 경우의 수는 이 가중치를 포함시키고         
동시에 정규화 상수도 바꾸는 것입니다.           
w^(i)의 합으로 바꿔야겠죠? 여기서 J를 어떻게 정의하느냐는 중요하지 않아요.           
직교화가 무엇인가를 이해하는 게 중요하죠.          
과녁을 놓는 것이 한 단계 과녁을 조준하고 발사하는 것이 또 다른 단계입니다.        
따로따로 해야하는 거죠. 다시 말하면 척도를 정의하는 것이 한 단계고요.           
척도를 정의한 그 다음에 그 척도에 대해 좋은 성능을 내는 방법을 찾는 거죠.        
그 과정에서 신경망이 최적화하는 비용 함수 J가 바뀔 수도 있습니다.       

# 현실의 분포가 dev, test 셋과 다를 때
![image](https://user-images.githubusercontent.com/50114210/68088140-69375580-fe9f-11e9-8ee4-271b57a4209d.png)      
더 나아가기 전에 다른 예시를 하나 더 살펴봅시다.           
두 개의 고양이 분류기 A, B가 개발 세트에서 계산한 결과            
각각 3%, 5%의 오차를 보입니다.        
또는 인터넷에서 다운로드한 이미지인 시험 세트에서 계산했을 수도 있고요.            
물론 고화질이고 모양이 맞는 이미지겠죠.         
하지만 알고리즘을 상품에 적용시켰을 때 알고리즘 B가 더 좋은 성능을 보일 수도 있습니다.         
개발 세트에서는 A가 좋은 성능을 보이지만요.         
또 여러분은 인터넷에서 다운로드받은 아주 고화질의 이미지를 써서 학습시키지만              
이걸 모바일 어플리케이션에 쓰면 사용자들은 모든 종류의 사진을 올립니다.        
사진 크기가 작거나 고양이의 부분만 포함되어 있거나             
웃긴 표정을 짓고 있는 고양이 불투명한 사진 등 말이에요.    
이런 경우에 알고리즘을 시험해봤더니 알고리즘 B가 더 좋은 성능을 보인 거죠.          
이건 평가 척도와 개발, 시험 세트가 실패하는 또다른 예시입니다.            
여기서는 아주 선명한 이미지로 구성된 개발, 시험 세트로 평가했다는게 문제입니다.          
하지만 사용자들은 그들이 업로드한 사진에 대해 알고리즘이 잘 작동하길 바라죠.          
초보자가 찍은 것 같은 사진이나 흐릿하고 크기가 맞지 않은 사진 말이에요.         


# 현실에서 잘 적용되지 않을 경우
![image](https://user-images.githubusercontent.com/50114210/68088149-866c2400-fe9f-11e9-962e-d422f209e052.png)        
여기서 가이드라인은 여러분의 척도와 개발 세트 또는 개발 및 시험 세트에서 좋은 성능을 보이지만          
실제 어플리케이션에서는 그렇지 않을 경우 척도나 개발, 시험 세트를 바꾸는 걸 고려해봐야 합니다.      
즉 다시 말해서 개발, 시험 세트가 고화질의 사진만 포함하고 있고         
이 개발, 시험 세트에서의 평가가 실제 앱이 저화질의 사진을 다루기 때문에         
실제 앱에서의 성능을 전혀 반영하지 못한다면 개발, 시험 세트를 바꿀 절호의 타이밍입니다.         
진짜로 좋은 성능을 내어야 할 데이터 타입을 반영하도록 바꾸는 거죠.                   
전체적인 가이드라인은 현재의 척도와 평가에 쓰이는 데이터가                   
실제 성능 및 중요하게 생각하는 부분을 반영하지 못한다면         
알고리즘이 좋은 성능을 내야 하는 부분이 포함되도록                
척도와 개발, 시험 세트를 바꿔야 합니다.          

# 이런 과정은 반복의 속도를 높여준다
알고리즘 A와 B 중 나은 걸 결정할 때 척도와 개발, 시험 세트가 있으면 훨씬 빨리 결정을 내릴 수 있습니다.            
여러분이나 팀이 반복하는 과정의 속도를 높일 수 있는 것이죠.         
따라서 완벽한 평가 척도나 개발 세트를 정의하지 못 하더라도             
뭐라도 빨리 정의해서 팀의 반복 과정의 속도를 높이는데 쓰기를 추천합니다.        
한편 계속 반복하면서 별로인 걸 깨닫고 더 좋은 아이디어를 갖게 되었다면 바로 바꾸세요.          
하지만 평가 척도나 개발 세트없이 오래 실행하는 것은 대부분의 팀에게 추천하지 않습니다.          
왜냐하면 반복 과정이나 알고리즘 개선에서 효율성을 낮출 수 있거든요.         

# 아웃트로
평가 척도와 개발, 시험 세트를 언제 바꿀지 관련한 내용은 여기까지 입니다.            
이 가이드라인을 갖고 여러분의 팀이 잘 정의된 목표를 향해        
효율적인 반복 과정을 거쳐 성능을 개선시키기를 바라겠습니다.      
