---
layout: post
title: "모델 퍼포먼스 향상시키기"
tags: [avoidable bias, variance, orthogonalization]
categories: [Structuring Machine Learning Projects]
---

# 학습 목표
모델의 성능을 향상시키는 방법을 알아본다.

# 핵심 키워드
* 회피 가능 편향(avoidable bias)
* 분산(variance)
* 직교화(orthogonalization)

# 학습 내용
* 지도 학습 알고리즘이 잘 작동할 수 있도록 한다는 것은 아래의 두 과정을 거칩니다.
  - 첫째, 훈련세트에 잘 들어 맞아야합니다. 즉, 회피 가능 편향을 줄이는 것입니다.
  - 둘째, 개발 및 시험 세트에서도 좋은 성능을 내도록 일반화합니다. 즉, 분산이 낮아야합니다.
* 직교화를 떠올려 보면, 몇가지 방법으로 회피 가능 편향과 분산을 각각 줄일 수 있습니다.
  - 회피 가능 편향:
    - 더 큰 모델로 훈련 시킵니다.
    - 훈련을 더 오래 시키거나 더 나은 알고리즘으로 최적화를 합니다.
    - 다른 신경망 구조를 만들거나 최적의 하이퍼파라미터를 찾습니다.
  - 분산
    - 더 많은 데이터를 사용합니다.
    - 정규화를 진행합니다.
    - 다른 신경망 구조를 만들거나 최적의 하이퍼파라미터를 찾습니다.

# 인트로
여러분은 직교화 학습, 개발 세트를 어떻게 만드는지
베이지안 오차의 추정치로서 사람 수준의 성능 및 회피 가능 편향과 분산을 어떻게 추정하는지 배웠습니다.          
이걸 모두 모아서 학습 알고리즘의 성능을 향상시키기 위한 가이드라인을 만들어봅시다.         

# 두 단계의 인공지능 개발 과정
### 편향 줄이기
![image](https://user-images.githubusercontent.com/50114210/68737301-5f8fb980-0626-11ea-871e-348a8caedbc5.png)       
제 생각에는 지도 학습 알고리즘이 잘 작동할 수 있도록 한다는 것은 두 가지 일을 하리라는 바람입니다.           
하나는 학습 세트에 잘 맞아들어가는 것이죠.         
대략 낮은 회피 가능 편향을 달성하고 싶다고 볼 수 있습니다.         

### 분산 줄이기
![image](https://user-images.githubusercontent.com/50114210/68737319-70d8c600-0626-11ea-8e98-8413b900a93f.png)      
둘째는 학습 세트에서 잘 한다는 가정 하에 개발 및 시험 세트에 대해서도         
일반적으로 잘하도록 일반화하는 겁니다. 즉 분산이 나쁘지 않아야 하죠.            
여기에서 직교화를 다시 떠올려보자면 어떤 버튼을 조작하여 회피 가능 편향을 줄일 수 있습니다.           
예컨대 더 큰 신경망을 학습시키거나 더 오래 학습시킬 수 있겠죠.        
또한 다른 방법을 이용하면 분산 문제를 해결할 수 있습니다.        
정규화나 더 많은 학습 데이터를 사용해서 말이에요.           

# 주목 해야할 값
![image](https://user-images.githubusercontent.com/50114210/68737457-d75de400-0626-11ea-9652-ab0189124ecf.png)      
지난 영상들에서 봤던 과정을 요약하자면 만약 머신러닝 시스템의 성능을 향상시키고 싶은 경우         
베이지안 오차의 추정치와 학습 오차의 차이에 주목하기를 권합니다.         
회피 가능 편향이 얼마인지 가늠할 수 있죠.         
즉 학습 세트에서 얼마나 더 잘 할 수 있는지를 알 수 있는 것입니다.         
개발 오차와 학습 오차의 차이를 보면 여러분이 갖고 있는 분산 문제를 가늠해볼 수 있습니다.          
즉 여러분의 성능이 개발 세트에도 일반적으로 적용되게 하는 것이 얼마나 어려운지를 말해주죠.         
물론 개발세트에 대해서 학습되지는 않았습니다

### 편향을 줄이는 법  
![image](https://user-images.githubusercontent.com/50114210/68737523-08d6af80-0627-11ea-8e03-10b787e36170.png)             
어떤 이유든지 회피 가능 편향을 줄이고 싶다면 더 큰 모델을 학습시켜서 학습 세트에 대해          
더 좋은 성능을 보이거나 더 오랫동안 모멘텀, RMSprop,  Adam처럼            
더 나은 알고리즘을 사용하는 등의 방법을 사용할 수 있을 겁니다.              
다른 방법으로는 더 나은 신경망 구조나 하이퍼파라미터 값을 찾을 수도 있습니다.           
여기에는 활성함수를 바꾸거나 층 또는 은닉층의 수를 바꾸는 것도 포함됩니다.         
이 방법을 사용하면 모델의 크기를 증가시켜서 다른 모델을 시험해볼 수도 있습니다.         
그 예로 순환신경망이나 합성곱 신경망이 있습니다.          
물론 새로운 신경망 구조가 여러분의 학습 세트에 더 적합할지를 미리 말하기는 어렵습니다.        
하지만 때때로 더 나은 구조를 사용하면 더 나은 결과를 얻을 수도 있죠.         

### 분산을 줄이는 법
![image](https://user-images.githubusercontent.com/50114210/68737545-1ab85280-0627-11ea-87a1-2e75cf4dd321.png)          
만약 분산이 문제라고 생각했다면 여러분은 다음 방법을 사용할 수 있습니다.         
더 많은 데이터로 학습시켜서 아직 알고리즘이 학습하지 않은 개발 세트로 쉽게 일반화를 할 수도 있고요.        
L2 정규화처럼 정규화를 시도해보거나 드롭아웃       
데이터 확대 또한 다양한 신경망의 구조나 하이퍼파라미터 값을 바꿔볼 수도 있습니다.           
이를 통해 여러분의 문제에 더 적합한 신경망 구조를 찾을 수 있는 거죠.         

# 아웃트로
제 생각에 편향, 즉 회피 가능 편향과 분산은 쉽게 배워지지만 익히기는 어려운 내용인 것 같습니다.       
만약 여러분이 이번 주에 배웠던 개념을 잘 적용할 수 있다면         
더 효율적이고 체계적으로 그리고 전략적으로 다른 머신러닝 팀에 비해 성과를 낼 수 있습니다.        
머신러닝 시스템의 성능을 향상시킨다는 측면에서 말이죠.        
