---
layout: post
title: "단일 실수 평가 기준"
tags: [a single real number evaluation metric, precision, recall, F-1 Score]
categories: [Structuring Machine Learning Projects]
---

# 학습 목표
단일 평가 기준을 학습한다.

# 핵심 키워드
* 단일 실수 평가 기준(a single real number evaluation metric)
* 정밀도(Precision)
* 재현율(Recall)
* F-1 지수(F-1 Score)

# 학습 내용
* 정밀도는 모델이 분류한 정답 중에 진짜 정답이 얼만큼 있는지를 측정합니다.
* 재현율은 실제 정답 중에 모델이 정답을 얼만큼 분류했는지를 측정합니다.
* 정밀도와 재현율사이에는 트레이드 오프 관계가 있습니다.
* 두 가지 기준을 동시에 쓰면 어떤 모델이 좋은 모델인지 선택하기 어렵습니다. 따라서, 정밀도와 재현율의 조화평균인 F-1지수를 사용합니다.
  - F-1 Score = 2 / (Pricision^-1 + 1/Recall^-1)
* 개발세트와 하나의 정량적 평가기준을 사용하게 되면 더 빠른 모델선택이 가능합니다. 이를 통해 알고리즘 개선의 순환 속도를 향사시킵니다.

# 인트로
여러분이 하이퍼파라미터를 조정하거나 여러 학습 알고리즘을 시도해보고            
서로 다른 머신러닝 시스템을 만들면서 실수로 된 평가 기준이 하나가 있다면           
훨씬 과정이 빨라질 거라는 것을 알게 되었을 겁니다.       
여러분의 마지막 아이디어에 대해 성능을 더욱 빠르게 비교할 수 있을 겁니다.         
따라서 머신러닝 프로젝트를 시작하는 팀에게         
문제를 풀기 쉽도록 실수 평가 기준을 하나 만들라고 추천합니다.               

# 예시 
![image](https://user-images.githubusercontent.com/50114210/68031173-723cf100-fcfe-11e9-8f24-56a2e15c5383.png)           
제가 머신러닝을 적용하는 과정이 경험적이라고 했던 것 기억하나요?        
아이디어를 갖고 코딩을 한 뒤에 실험을 통해서 성과를 관찰하죠.          
그리고 실험의 결과를 토대로 아이디어를 개선합니다.          
이 과정을 순환하며 알고리즘을 개선하는 거죠.            

# 고양이 분류기
![image](https://user-images.githubusercontent.com/50114210/68031240-9698cd80-fcfe-11e9-9ab5-e1007b2718cc.png)        
여러분의 고양이 분류기를 생각해봅시다. 어떤 분류기 A가 있고요.          
하이퍼파라미터나 학습 세트 등을 바꿔서 새 분류기 B도 학습시켰습니다.         
분류기의 성능을 평가하는 합리적인 방법 중 하나는 여기 정밀도와 재현율을 보는 것입니다.         
정밀도와 재현율이 정확히 무엇인가는 이 예시에 있어서 그렇게 중요하진 않습니다.          

# 정밀도
![image](https://user-images.githubusercontent.com/50114210/68031331-cfd13d80-fcfe-11e9-9344-c7aa940d44cc.png)     
간략하게 정밀도는 분류기가 고양이로 인식한 예시 중 진짜 고양이가 몇 퍼센트냐는 겁니다.           
만약 분류기 A의 정밀도가 95%라면 분류기 A가 고양이라고 말한 것은 95%의 확률로 진짜 고양이라는 것이죠.         

# 재현률
![image](https://user-images.githubusercontent.com/50114210/68031396-eb3c4880-fcfe-11e9-86f9-43103d3352ed.png)       
재현율은 실제 고양이의 사진 중에서 분류기에 의해서 몇 퍼센트가 정확히 인식되었느냐는 겁니다.          
즉 실제 고양이 사진이 정확히 인식된 퍼센트 비율이죠.       
만약 분류기 A가 90%의 재현율을 보였다면        
개발 세트의 고양이 사진 전체 중 분류기 A는 90%를 정확하게 판별한 겁니다.          
정밀도와 재현율의 정의에 너무 신경쓰지 마세요.        


참고로 정밀도와 재현율은 종종 트레이드오프 관계에 있다는 것이 알려져있습니다.         
하지만 여러분은 둘 다 중요하게 생각하겠죠.                         
분류기가 고양이로 분류했으면 실제 고양이일 확률이 높고      
동시에 실제 고양이 사진 중에서 많은 사진을 고양이로 판별했으면 할 테니까요.    
따라서 정밀도와 재현율을 사용해 분류기를 평가하는 것은 합리적입니다.       
정밀도와 재현율을 평가 기준으로 사용할 때의 문제점은           
분류기 A가 여기처럼 재현율이 좋고 분류기 B가 높은 정밀도를 보일 때     
무엇이 더 좋은지 확신할 수 없다는 겁니다.        
만약 수많은 하이퍼파라미터와 수많은 아이디어를 시도한다면 단 두 개의 분류기가 아니라           
수많은 분류기를 빨리 시도해보고 최상의 것을 빨리 골라내기 위해서 저 순환을 계속 반복하겠죠.          

# F-1 지수
![image](https://user-images.githubusercontent.com/50114210/68031637-67cf2700-fcff-11e9-81fc-aa6f0c7a4584.png)         
하지만 두 평가 기준을 써서는 둘 중에 하나, 열 중에 하나를 빠르게 골라내기가 어렵습니다.               
그래서 추천드리는 것은 분류기를 선택할 때 정밀도와 재현율           
두 숫자를 쓰는 것 보다 정밀도와 재현율을 결합한 새로운 평가기준을 대신 정의하라는 거죠.         
머신러닝 논문에서 정밀도와 재현율을 결합할 때 주로 쓰는 방법은 F1 지수입니다.            
F1 지수가 뭔지는 그렇게 중요하지 않습니다.          
대략적으로 정밀도와 재현율의 평균으로 생각해도 됩니다.         
실제 F1 지수의 정의는 2/(1/P+1/R)입니다.          
수학에서는 정밀도 P와 재현율 R의 조화평균이라고 부릅니다.      
대략 정밀도와 재현율의 평균 정도로 생각하면 됩니다.        
산술평균 대신 이 공식처럼 조화평균을 사용한 거죠.          
정밀도와 재현율의 트레이드오프를 고려하면 장점이 여럿 있습니다.         
이 예시를 보면 분류기 A가 더 높은 F1 지수를 보이고 있죠?         
F1 지수가 정밀도와 재현율을 합리적으로 결합했다고 가정하면             
분류기 B 대신 분류기 A를 빠르게 선택할 수 있습니다.          

# 실수 평가 기준
![image](https://user-images.githubusercontent.com/50114210/68031714-8c2b0380-fcff-11e9-823e-f11f17807c6d.png)     
제가 모든 머신러닝 팀을 보면서 깨달은 것은                 
정밀도와 재현율을 계산할 때 쓰는 개발 세트와 하나의 정량적 평가 기준을 가진다면                  
하나의 실수 평가 기준이라고도 부르는데요.      
분류기 A와 분류기 B 중에 무엇이 나은지 빠르게 선택할 수 있다는 겁니다.       
따라서 개발 세트와 하나의 정량적 평가 기준이 있으면 더욱 빨리 순환할 수 있죠.         
머신러닝 알고리즘을 개선시키는 이 순환 과정의 속도를 높이는 겁니다.         

# 다른 예시
![image](https://user-images.githubusercontent.com/50114210/68031878-ddd38e00-fcff-11e9-9a1a-b40f41ee29e2.png)        
다른 예시를 살펴볼까요?     
이제 네 개 지역에서 고양이를 사랑하는 사람들을 위해 고양이 애플리케이션을 만든다고 합시다.        
미국, 중국, 인도와 나머지 지역들이죠 그리고 두 개의 분류기가        
지역에 따라 서로 다른 오차를 보이고 있습니다 .          
알고리즘 A는 미국 사용자들이 제출한 사진에서 3%의 오차를 보이는 식이죠.          
여기에서 분류기가 서로 다른 마커, 지역에서 얼마나 성과를 내는지 분석하는 것은 합리적입니다.         
그러나 네 개의 숫자를 한꺼번에 고려해서         
알고리즘 A와 B 중 무엇이 나은지를 빠르게 결정하기는 힘듭니다.           
만약 여러 개의 분류기를 시험한다면 이 많은 숫자들을 살펴보고 하나를 고르기란 역시 어렵습니다.       
따라서 이 예시에서 추천드리는 것은 네 개의 지역에서의 성능을 분석하는 것에서 나아가 평균을 계산하는 것입니다.        
평균 성능이 하나의 실수 평가 기준으로써 합리적이라는 가정 하에 말이죠.         
그러면 평균을 계산함으로써 빠르게 알고리즘 C가 가장 낮은 평균 오차를 보이니          
순환 과정에서 하나를 골라야 할 때 알고리즘 C를 선택할 수 있는 것이죠.        
여러분이 머신러닝을 다룰 때에는 아이디어를 생각한 뒤         
구현 및 시도해보고 아이디어가 잘 작동하는지 알고싶을 겁니다.     

# 아웃트로
이 영상에서 본 것은 하나의 정량적 평가 기준을 가짐으로써
여러분이나 속한 팀이 결정을 내릴 때 효율성을 향상시킬 수 있다는 겁니다.        

