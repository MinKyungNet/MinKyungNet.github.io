---
layout: post
title: "회피 가능 편향"
tags: [human level error, base optimal error, training error, dev error, avoidable bias, variance]
categories: [Structuring Machine Learning Projects]
---

# 학습 목표
회피 가능 편향을 배운다

# 핵심 키워드
* 사람 수준의 오차(human level error)
* 베이지안 최적 오차(base optimal error)
* 훈련 오차(training error)
* 개발 오차(dev error)
* 회피 가능 편향(avoidable bias)
* 분산(variance)

# 학습 내용
* 사람 수준의 오차와 베이지안 최적오차는 차이가 많이 나지 않는 것으로 가정하겠습니다. 따라서 사람 수준의 오차를 베이지안 최적 오차로 추정하겠습니다.
* 베이지안 최적오차와 훈련오차의 차이를 회피 가능 편향이라고 합니다. 회피 가능 편향은 더 이상 낮아 질 수 없는 편향 또는 최소 오차가 있다는 뜻입니다. 즉, 이 값이 클 수록 아직 모델이 충분히 룬련이 안 된 것입니다. 더 큰 신경망이나 경사하아법을 더 오래 실행해서 회피 가능 편향을 줄이도록 합니다.
* 훈련오차와 개발오차의 차이를 분산이라고 합니다.

# 인트로
학습 세트에 대해 학습 알고리즘이 좋은 성능을 내는 것에 대해 얘기했었는데요.       
하지만 너무 뛰어난 것은 원치 않습니다.          
사람의 성능이 어느 정도인지를 알면 학습 세트에 대해 알고리즘의 적절한 성능을 가늠할 수 있죠.           
무슨 말인지 설명해보죠.         

# 첫번째 예시
![image](https://user-images.githubusercontent.com/50114210/68123359-0fdb2f00-ff50-11e9-8c93-6ee72ee2ec23.png)         
고양이 분류 문제를 많이 봤었는데요.        
어떤 그림이 주어졌을 때 사람은 거의 정확하게 판별한다고 합시다.        
즉 사람의 오차는 1%라고 둘 수 있겠죠.         
학습 알고리즘의 학습 오차가 8%이고 개발 오차가 10%인 경우에        
아마 학습 세트에 대해서 더 좋은 성능을 내려 했을 것입니다.      
여기에서 알고리즘이 학습 세트에 대해 보이는 성능과      
사람이 얼마나 잘하느냐의 큰 차이는 아직 알고리즘이 학습 세트에        
충분히 들어맞지 않았음을 시사합니다.         
여기에서 편향과 분산을 줄이는 도구를 사용할 때        
편향을 줄이는 데 중점을 두라고 할 것 같습니다.           
즉 더 큰 신경망을 학습시키거나 경사하강법을 더 오래동안 실행해서       
학습 세트에 대해 더 좋은 성능을 보이는 거죠.         

# 두번째 예시
이제 학습 오차와 개발 오차가 같고 사람의 오차가 1%가 아니라고 합시다.         
다른 어플리케이션이나 데이터에 대해서 사람의 오차가 7.5%라고 해보죠.         
사진이 너무 흐려서 사람조차도 고양이인지 정확히 구분할 수 없는거죠.         
그런데 사람은 주로 사진을 보고 고양이 여부를 잘 구분해내기 때문에         
다소 부자연스러운 예시이긴 합니다.       
하지만 이 예시 상황을 고려해서 데이터 세트의 사진이 너무 흐리거나        
해상도가 너무 낮다고 합시다.         
그래서 사람의 오차가 7.5%나 되는 거죠.        
학습 오차와 개발 오차가 다른 예시와 같지만       
학습 세트에 대해 좋은 성능을 보이는 것 같습니다.       
사람 수준의 성능에 약간 못 미치니까요.            
두 번째 예시에서 여러분은 학습 알고리즘의 분산을 줄이기 위해 애써야 할 겁니다.         
예컨대 정규화를 통해서 개발 오차와 학습 오차의 차이를 줄이는 거죠.        

# 베이지안 오차에 따라 달라지는 목표
![image](https://user-images.githubusercontent.com/50114210/68123388-22556880-ff50-11e9-89de-74439c5bf8b0.png)           
편향과 분산에 대해 강의 초반에서 다룰 때 우리는 태스크의 베이지안 오차가       
0에 가깝다고 가정했습니다.           
여기에서 일어난 상황을 설명하기 위해 이제 고양이 분류 예시에서            
사람 수준의 오차를 베이지안 최적 오차의 추정치라고 생각합시다.         
컴퓨터 비전 태스크에서 사람은 컴퓨터 비전에 능숙하기 때문에 이 추정은 매우 합리적입니다.         
즉 사람이 할 수 있는 일은 베이지안 오차와 큰 차이가 없다고 할 수 있죠.      
정의에 의해 사람 수준의 오차는 베이지안 오차보다 더 나쁩니다.       
왜냐하면 베이지안 오차보다 작은 오차는 존재하지 않으니까요.       
하지만 베이지안 오차와 사람 오차의 차이는 크지 않을 겁니다.       
여기에서 신기한 점은 사람 수준의 오차 또는 베이지안 오차의 추정치가 되겠죠.          
이 값에 따라 우리가 달성하고자 하는 목표가                                 
같은 학습 오차와 개발 오차에 대해 달라질 수 있다는 겁니다.           
이 두 예시에서는 편향을 줄이는 방법과 분산을 줄이는 방법에 초점을 뒀습니다.        
왼쪽의 예시에서는 8%라는 학습 오차가 너무 커서 1%로 오차를 줄이기 위해      
편향을 줄이는 방법을 쓸 수 있습니다.               
반면 오른쪽의 예시에서는 사람의 오차를 토대로  베이지안 오차가 7.5%로 추정해서        
베이지안 오차가 7.5%에 가깝다고 추정할 수 있겠죠.         
그러면 더 이상 학습오차를 줄일만한 공간이 충분치 않다는 걸 알게 됩니다.           
여러분은 7.5%보다 더 좋은 성능을 원치 않는 거죠.       
왜냐하면 그건 학습 세트에 과대적합으로만 달성할 수 있을 테니까요.        
대신 이 2%에 해당하는 차이의 측면에서 정규화나 더 많은 학습 데이터를 이용하여        
분산을 줄이는 기술을 활용해 알고리즘을 향상하는 걸 생각할 수 있습니다.            

# 회피 가능 편향을 줄여야할 때
여기에 이름을 지어줄텐데요. 널리 쓰이는 용어는 아닙니다.        
하지만 쓸모 있는 용어들이죠. 
베이지안 오차 또는 그 추정치와 학습 오차의 차이를 회피 가능 편향이라고 합니다.           
즉 베이지안 오차에 다다를 때까지 학습 성능을 향상시키길 원하겠지만         
베이지안 오차를 뛰어넘기를 원할 순 없습니다.          
아니 과대적합을 하지 않고서는 베이지안 오차를 넘어설 수 없죠.           
그리고 학습 오차와 개발 오차의 차이를 우리는 알고리즘의 분산이라고 부릅니다.          
회피 가능 편향이라는 용어는 더 이상 낮아질 수 없는 편향 또는 최소 오차가 있다는 걸 말하죠.         
예컨대 베이지안 오차가 7.5%라면 그 수준의 오차보다 더 낮아지는 걸 원치 않아야 합니다.     
즉 이 예시에서 편향으로 학습 오차 8%를 말하는 대신 회피 가능 편향이 0.5%라고 말하는 겁니다.       

# 분산을 줄여야할 때
![image](https://user-images.githubusercontent.com/50114210/68123431-3e590a00-ff50-11e9-9c50-31f7eec8f17c.png)        
반면 분산은 2%입니다 0.5%를 줄이는 것보다 2%를 줄이는 것이 더 여유 있죠.          
반대로 왼쪽의 예시에 대해서는 7%가 회피 가능 편향이고 2%는 분산값입니다.           
즉 왼쪽의 예시에서는 회피 가능 편향을 줄이는데 주력하는 것이 더 가능성 있는 거죠.         

# 아웃트로
이 예시에서 사람 수준의 오차와 베이지안 오차의 추정에 대한 이해를 토대로      
다양한 시나리오에서 편향 회피와 분산 회피라는 도구 중 무엇에 초점을 맞춰야 하는지 살펴봤습니다.       
