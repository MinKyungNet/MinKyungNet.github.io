---
layout: post
title : "얕은 신경망 네트워크 : 활성화 함수의 미분"
tags: [Neural Networks, Gradient Descent]
categories: [Neural Networks and Deep Learning]
---

블로그에 인터넷 강의의 자막을 그대로 따라치는 조악한 내용의 포스팅을 하는 이유는 강의에 대한 이해를 어느정도 높이고 싶어서이다.  
그런데 이번 강의는 내 수준에서 좀 벗어난 듯하여 이해가 되지 않았음에도, 일단 글을 올리겠다.
예전에 들었던 영상을 찾아봐도 행렬 미분을 어떻게 하는지 좀 처럼 감이 잡히지 않는다.
nx라는 말도 이전 강의에서 사용하지 않았는데... 왜 갑자기 튀어나온 것인지 잘 모르겠다.
나중에 더 실력이 쌓였을 때 이 글에 돌아와 좀 더 멋진 자세로 글을 써내려가겠다고 다짐한다.

---

# 학습 목표
* 경사 하강법 구현 방법을 알 수 있다.

# 핵심 키워드
* 경사 하강법

# 학습 내용
![image](https://user-images.githubusercontent.com/50114210/64251636-ef90f300-cf53-11e9-8b5e-9b3aa1cf5138.png)

# 인트로
좋습니다. 지금까지 흥미로운 내용이었죠. 이번 강의에서는 경사 하강법을 구현하는 방법을 알아보겠습니다.    
한 개의 은닉층을 가진 신경망에 대해서 말이죠. 이 강의에서는 여러분이 구현해야 할 식들을 드릴 것입니다.   
이것은 역전파를 계산하거나, 경사 하강법을 적용하기 위한 식들입니다.   
그리고 이 부분을 마친 후에는 저는 추가적인 설명을 해드릴 것인데요. 이 물리적인 식들이 왜 신경마에서 경사를 계산하는데 있어서 올바른 식인지 말이죠.

# 단일층 신경망의 경사 하강법
### 변수들
![image](https://user-images.githubusercontent.com/50114210/64251875-78a82a00-cf54-11e9-817a-104264a37810.png)

현재 여러분의 단일층 신경망은 w[1], b[1], w[2], b[2]의 변수를 가집니다. 그리고 기억하시는 것처럼
</br>
</br>
</br>

### 레이어
![image](https://user-images.githubusercontent.com/50114210/64251974-abeab900-cf54-11e9-92b8-285fa1f45ecb.png)

nx는 n[0]을 입력 특성으로 가지고   
n[1]은 은닉 유닛으로 가지며    
n[2]를 출력 유닛으로 가집니다.    
우리가 사용한 샘플에서는 n[2]는 1이었습니다.
</br>
</br>
</br>

### 벡터화된 변수들의 차원
![image](https://user-images.githubusercontent.com/50114210/64252036-d89ed080-cf54-11e9-8646-0fd4693f9448.png)

그러면 행렬 w[1]은 (n[1], n[0])차원을 가집니다. 그리고   
b[1]은 n[1]차원을 가지는 벡터가 됩니다. (n[1], 1)차원이라고 적을 수 있겠죠. 따라서 행 벡터가 될 겁니다.    
w[2]는 (n[2], n[1])차원이 되고    
b[2]는 (n[2], 1)차원이 됩니다.    
이것은 한 개의 은닉 유닛을 가진 n[2]가 1인 예시를 사용한 경우입니다.
</br>
</br>
</br>

### 로지스틱 회귀의 비용함수

![image](https://user-images.githubusercontent.com/50114210/64252326-890cd480-cf55-11e9-8353-96e8eb4bcbb4.png)

그리고 여기에 신경망의 비용 함수가 있습니다. 여기서는 여러분이 이진 분류를 하고 있다고 가정하겠습니다.   
이 경우에 각 변수들에 대한 비용은 다음과 같이 손실함수의 평균이 됩니다. 여기서 L은 신경망이 예측한 y^값에 대한 손실입니다.    
여기 부분은 실제로는 a[2]가 되겠죠. y는 참 값을 말합니다.   
여러분이 이진분류를 하는 경우에 손실 함수는 로지스틱 회귀에서 사용한 것과 같을 것입니다.   

이제 이 알고리즘에서 변수들을 훈련시키기 위해서 경사 하강법을 사용해야합니다.   
변수를 어떤 값으로 초기화 한 후 경사 하강법이 반복될 때마다 예측값을 계산합니다.   
다시 말해서 y^[i]를 계산하는 것이 됩니다. i는 1부터 m까지의 정수가 되죠

</br>
</br>
</br>

### 변수들의 도함수

![image](https://user-images.githubusercontent.com/50114210/64252464-fa4c8780-cf55-11e9-8ee1-8bc7e503f0d7.png)

그리고 이제 도함수 계산해야 합니다. dw[1]을 계산하는데 이 값은 변수 w[i]에 대한 비용 함수의 도함수를 의미합니다.    
또한 db[1]을 계산해야 하는데 이 값은 b[1]에 대한 비용 함수의 도함수가 되는 것이죠.    
w[2]와 b[2]에 대해서도 비슷하게 도함수를 계산합니다.    
결국 경사 하강법은 w[1]을 w[1] - 러닝레이트 * dw[1]으로 바꿉니다.   
b[1]은 b[1] - 러닝레이트 * db[1]으로 바꿉니다.   
w[2]와 b[2]에 대해서도 비슷하게 적용됩니다. 콜론이 붙은 등호 또는 일반 등호를 사용하는데 두 표기법 모두 괜찮습니다.   
이제 이것이 경사 하강법의 한 반복이 됩니다. 그리고 변수들이 수렴할 때까지 이것을 반복하게 되는 것이죠.

이전 강의에서 어떻게 예측값을 계산하는지 배웠습니다. 또 어떻게 벡터화된 방법으로 출력값을 계산하는지 배웠죠.   
여기서 중요한 것은 어떻게 이 편미분을 계산하는지 입니다.     
dw[1]과 db[1]그리고 dw[2], db[2]까지 말이죠.    
이제 하려고 하는 것은 이 도함수를 계산할 수 있도록 몇 가지 식을 드리는 것입니다.

</br>
</br>
</br>

# 정방향 전파

![image](https://user-images.githubusercontent.com/50114210/64252898-0c7af580-cf57-11e9-839d-201dc7a10f89.png)

정방향 전파에 대한 식들을 다시 정리해보겠습니다.
z[1]은 w[1] * X + b[1]이 됩니다. 각 층에서의 활성화 함수인 A[1]은 z[1]에 대한 함수이므로 이렇게 됩니다.    
z[2]는 w[2] * A[1] + b[2]가 되고 마지막으로 여기서의 모든 샘플들은 벡터화된 상태입니다.    
A[2]는 g[2] * z[2]가 됩니다.     
여기서는 이진 분류를 하고 있기 때문에 이 활성화 함수는 실제로는 시그모이드 함수가 될 것입니다.   
이것이 정방향 전파에 대한 식입니다. 또는 신경망에서 왼쪽에서 오른쪽으로의 진행과정이라고 할 수 있습니다. 

</br>
</br>
</br>

# 역방향 전파

![image](https://user-images.githubusercontent.com/50114210/64253084-75626d80-cf57-11e9-9d31-4bfc5ed7b6a9.png)

이제 도함수를 계산해보겠습니다. 이것은 역전파 단계에 관한 것이죠.   
먼저 dz[2]를 계산하는데 이것은 A[2]에서 참 값 Y를 뺀 것과 같습니다.     
여기선 모두 벡터화된 샘플을 사용하기 때문에 행렬 Y는 (1, m)행렬이 됩니다. m가지의 모든 샘플을 가로로 나열하는 것이죠.    

이제 dw[2]는 이런 형태의 식이 됩니다. 이 세개의 식은 로지스틱 회귀에서의 경사 하강법과 굉장히 비슷합니다.    
여기서 axis = 1이 되고 keemdims는 True가 됩니다. 이것에 대해서 약간 더 설명해드리자면,     
np.sum은 파이썬 numpy 명령어인데 행렬의 어떤 축 방향으로 덧셈을 계산할 때 사용합니다. 이 경우에는 가로로 더하는 것이죠.     
keepdims가 하는 역할은 파이썬이 잘못된 1차원 배열을 출력하지 않게 하는 것입니다. 여기에서는 (n, )의 차원을 가지게 됩니다.    
따라서 keepdims를 True로 설정하면 db[2]에 대한 파이썬의 출력값은 (n, 1)벡터가 될 것입니다. 정확히는 (n[2], 1)이 되겠죠.    
그리고 이 경우에는 (1, 1)이 되기 때문에 영향을 미치진 않습니다.   
하지만 나중에 이것이 중요한 경우가 있습니다. 지금까지 로지스틱 회귀와 굉장히 비슷한 것을 했습니다.    
역전파 계산을 계속 진행해보겠습니다.

dz[1]을 계산하면 (W[2]^T * dZ[2]) * (g'[1] * z[1])이 됩니다. 이 g'[1]은 은닉층에서 사용했던 활성화 함수의 도함수입니다.    
출력층에서는 시그모이드 함수를 이용해 이진 분류를 합니다. dz[2]는 이미 앞서 구했습니다. 가운데 곱셉은 요소별 곱셈입니다.
왼쪽 행렬은 (n[1], m)행렬이 되고 오른쪽 행렬 또한 (n[1], m)행렬이 됩니다. 따라서 이 곱셈은 두 행렬의 요소별 곱셈이 되어야하는 것이죠.   

그리고 마지막으로 dw[1]과 db[1]을 앞에서 해준 것처럼 구해줍니다.
keepdims변수를 사용하고 싶지 않다면 reshape함수를 호출해도 좋습니다. 이 함수는 출력값을 다시 재배열해서 db에 맞는 차원으로 만들 수 있습니다.


