---
layout: post
title: "머신러닝을 위한 기본 레시피"
tags: [Bias, Variance]
categories: [Improving deep neural networks]
---

# 학습 목표
머신러닝에서의 "기본 레시피"의 사용법을 배운다.

# 핵심 키워드
편향(Bias)
분산(Variance)

# 인트로
이전 비디오에서 훈련 오차와 개발 오차가 알고리즘이 편향 혹은 분산문제, 혹은 둘 다를 가지는지 진단하는데 어떻게 도움이 되는지 살펴보았습니다.    
이 정보는 저만의 머신러닝 레시피를 훨씬 더 체계적으로 사용할 수 있고, 알고리즘의 성능을 체계적으로 개선할 수 있도록 도와줍니다.    

# 머신러닝의 기본 레시피
![image](https://user-images.githubusercontent.com/50114210/64840732-80905a00-d637-11e9-885e-48c03f992284.png)      
신경망을 훈련할 때 사용하는 기본 레시피입니다.     
        
            
### 높은 편향
최초의 모델을 훈련하고 난 뒤 처음으로 질문하는 것은 알고리즘이 높은 편향을 가지는지 입니다.     
높은 편향을 평가하기 위해서는 훈련 세트 혹은 훈련 데이터의 성능을 봐야합니다.     
만약 높은 편향을 가져서 훈련 세트에도 잘 맞지 않는다면 시도할 수 있는 것은    
더 많은 은닉 층 혹은 은닉 유닛을 갖는 네트워크를 선택하는 것입니다.    
아니면 더 오랜 시간 훈련시키거나, 다른 발전된 최적화 알고리즘을 사용합니다.     
혹은, 이 문제에 더 잘 맞는 신경망 아키텍쳐를 찾을 수도 있습니다.      
여기서 큰 네트워크를 찾는 것은 대부분 도움이 되고,    
새로운 아키텍쳐를 찾는 것은 도움이 될 수도 안 될 수도 있지만 해가 되지는 않습니다.   
따라서 학습 알고리즘을 훈련시킬 때 최소한 편향 문제를 해결할 때까지 이 방법들을 사도합니다.    


보통 충분히 큰 네트워크라면 훈련 데이터는 잘 맞출 수 있습니다.    
물론 사람이 봐도 구분이 힘들정도라면 잘 안될수도 있지만요.     
즉, 베이즈 오차가 그렇게 높지 않다면 그것보다 더 크게 훈련하는 경우     
최소한 훈련 세트에 대해서는 잘 맞을 것입니다.    
          
          
          
### 높은 분산
편향을 수용 가능한 크기로 줄이게 되면 그 다음에 물어볼 것은 분산 문제가 있는지입니다.    
이를 평가하기 위해 개발 세트 성능을 보게 됩니다.    
꽤 좋은 훈련 세트 성능에서 꽤 좋은 개발 세트 성능을 일반화할 수 있는지 말입니다.     
높은 분산 문제가 있을 때 이를 해결하는 가장 좋은 방법은 데이터를 더 얻는 것입니다.    
그러나 가끔은 데이터를 더 얻지 못하는 경우가 있습니다.    
이럴 경우에는 과대 적합을 줄이기위해 정규화를 시도할 수 있습니다.    
그리고 다른 신경망 아키텍쳐를 찾는 것을 시도해볼 수도 있습니다.    
찾을 수 있다면 앞의 편향 문제처럼 분산을 줄일 수 있습니다.    
         
         
### 효율적인 시도를 위한 편향과 분산의 파악
그러나 이 방법을 완전히 체계화하기는 어렵습니다.   
이 방법들은 낮은 편향과 분산을 찾을 때까지 계속 시도하고 반복하게 됩니다.    
몇 가지 중요한 것은 첫번째로 높은 편향이냐 분산이냐에 따라 시도해 볼 수 있는 방법이 아주 달라질 수 있습니다.    
그래서 주로 훈련과 개발 세트를 편향이나 분산문제가 있는지 진단하는데 사용합니다.     
그 결과 시도해 볼 수 있는 방법을 적절하게 선택합니다.    
예를 들어 높은 편향 문제가 있다면 더 많은 훈련 데이터를 얻는 것은 크게 도움이 되지 않습니다.    
가장 효율적인 방법이 아니라는 것이죠.    
따라서 편햐과 분산, 혹은 둘 다의 문제가 얼마나 있는지를 명확히하는 것은     
가장 유용한 시도를 선택하는데 집중할 수 있도록 합니다.    

# 편향-분산 트레이드오프
초기 머신러닝의 시대에는 편향-분산 트레이드오프에 대한 많은 논의가 있었습니다.    
그 이유는 시도할 수 있는 많은 것들이 편향을 증가시키고 분산을 감소시키거나    
편향을 감소시키고 분산을 증가시키기 떄문입니다.     
딥러닝 이전 시대로 돌아가면 툴이 그렇게 많지는 않았습니다.    
서로를 나쁘게 하지 않고 편향만 감소시키거나 분산만 감소시키는 툴이 많이 없었습니다.    

### 현대
그러나 현대의 딥러닝 빅데이터 시대에는 더 큰 네트워크를 훈련시키고     
더 많은 데이터를 얻는 것이 둘 다의 경우에 항상 적용되는 것은 아니지만   
만약 그런 경우라면 더 큰 네트워크를 갖는 것이 대부분 분산을 해치지 않고     
편향만을 감소시킵니다. 정규화를 올바르게 했다면 말이죠.     
그리고 데이터를 더 얻는 것도 대부분 편향을 해치지 않고 분산을 감소시킵니다.    
따라서 더 큰 네트워크를 훈련시키거나 더 많은 데이터를 얻는 이 두 단계는     
서로에게 영향을 미치지 않고 편향만을 감소시키거나 분산만을 감소시키는 툴이됩니다.
이것디 지도 학습에 딥러닝이 매우 유용한 큰 이유 중 하나라고 생각합니다.    
편향과 분산의 균형을 신경써야 하는 트레이드오프가 훨씬 적기 때문입니다.    

그러나 가끔은 편향이나 분산을 줄이는데 어쩔 수 없이 다른 것을 증가시키는 선택을 하는 경우도 있습니다.    
사실 최종적으로 잘 정규화된 네트워크를 갖는 방법도 있습니다.      
더 큰 네트워크를 훈련시키는 것은 대부분 절대 해가 없습니다.    
너무 큰 신경망을 훈련시키는 주된 비용은 계산시간입니다. 정규화도 마찬가지구요.    
편향과 분산을 진단하기 위한 머신러닝 문제를 어떻게 체계화할지에 관한 기본 구조에 대한 감을 잡으셨기를 바랍니다.      



